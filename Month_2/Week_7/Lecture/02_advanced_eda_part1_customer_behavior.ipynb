{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "customer_behavior_title"
      },
      "source": [
        "# Week 7: Advanced EDA with Business Intelligence - Part 1: Customer Behavior Analysis and Segmentation\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this session, you will be able to:\n",
        "- Conduct comprehensive customer behavior analysis using advanced EDA techniques\n",
        "- Implement data-driven customer segmentation strategies\n",
        "- Apply RFM (Recency, Frequency, Monetary) analysis for customer insights\n",
        "- Create customer journey and lifecycle analysis\n",
        "- Build actionable customer personas based on behavioral data\n",
        "\n",
        "## Business Context\n",
        "Today we dive deep into **customer behavior analytics** using our live Olist e-commerce dataset. Understanding customer patterns is crucial for:\n",
        "- **Customer Retention**: Identifying at-risk customers\n",
        "- **Revenue Optimization**: Finding high-value customer segments\n",
        "- **Marketing Strategy**: Personalizing campaigns based on behavior\n",
        "- **Product Strategy**: Understanding purchase patterns and preferences\n",
        "\n",
        "**Key Business Questions:**\n",
        "- Who are our most valuable customers and what drives their behavior?\n",
        "- How can we segment customers for targeted marketing?\n",
        "- What patterns exist in customer purchase journeys?\n",
        "- Which customers are at risk of churning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section_customer"
      },
      "source": [
        "## 1. Environment Setup and Secure Data Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "secure_setup"
      },
      "outputs": [],
      "source": [
        "# Essential imports for customer behavior analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Advanced analytics libraries\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from scipy import stats\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "\n",
        "# Visualization enhancement\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.offline as pyo\n",
        "pyo.init_notebook_mode(connected=True)\n",
        "\n",
        "# Database connection (secure)\n",
        "import os\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# Display and plotting settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "\n",
        "print(\"‚úÖ Environment setup complete for customer behavior analysis!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "secure_database_connection"
      },
      "outputs": [],
      "source": [
        "# Secure Database Connection Using Environment Variables\n",
        "# NOTE: In production, always use environment variables for credentials\n",
        "\n",
        "# For Google Colab users: Set environment variables\n",
        "# You can set these in your Colab environment or use Colab secrets\n",
        "import os\n",
        "\n",
        "# Set up environment variables (replace with your actual credentials)\n",
        "# In production, these should be set at the system level\n",
        "if 'SUPABASE_DB_HOST' not in os.environ:\n",
        "    # Temporary setup for educational purposes only\n",
        "    # NEVER hardcode credentials in production code!\n",
        "    os.environ['SUPABASE_DB_HOST'] = 'aws-0-us-east-1.pooler.supabase.com'\n",
        "    os.environ['SUPABASE_DB_PORT'] = '6543'\n",
        "    os.environ['SUPABASE_DB_NAME'] = 'postgres'\n",
        "    os.environ['SUPABASE_DB_USER'] = 'postgres.pzykoxdiwsyclwfqfiii'\n",
        "    os.environ['SUPABASE_DB_PASSWORD'] = 'L3tMeQuery123!'\n",
        "\n",
        "# Construct database URL from environment variables\n",
        "DATABASE_URL = f\"postgresql://{os.environ['SUPABASE_DB_USER']}:{os.environ['SUPABASE_DB_PASSWORD']}@{os.environ['SUPABASE_DB_HOST']}:{os.environ['SUPABASE_DB_PORT']}/{os.environ['SUPABASE_DB_NAME']}\"\n",
        "\n",
        "# Create secure database engine\n",
        "engine = create_engine(DATABASE_URL)\n",
        "\n",
        "# Test connection\n",
        "try:\n",
        "    with engine.connect() as conn:\n",
        "        result = conn.execute(\"SELECT 1 as connection_test\")\n",
        "        print(\"‚úÖ Secure database connection established!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Connection failed: {e}\")\n",
        "\n",
        "print(\"\\nüîí Security Note: Database credentials loaded from environment variables\")\n",
        "print(\"   This is the secure way to handle sensitive connection information.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "customer_data_loading"
      },
      "source": [
        "## 2. Customer Data Loading and Preparation\n",
        "\n",
        "Let's load comprehensive customer data for behavioral analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_customer_data"
      },
      "outputs": [],
      "source": [
        "# Comprehensive Customer Behavior Dataset\n",
        "print(\"üîÑ Loading comprehensive customer behavior dataset...\")\n",
        "\n",
        "# Customer transaction and behavior query\n",
        "customer_behavior_query = \"\"\"\n",
        "WITH customer_orders AS (\n",
        "    SELECT \n",
        "        c.customer_unique_id,\n",
        "        c.customer_state,\n",
        "        c.customer_city,\n",
        "        o.order_id,\n",
        "        o.order_purchase_timestamp,\n",
        "        o.order_delivered_customer_date,\n",
        "        EXTRACT(YEAR FROM o.order_purchase_timestamp) as order_year,\n",
        "        EXTRACT(MONTH FROM o.order_purchase_timestamp) as order_month,\n",
        "        EXTRACT(DOW FROM o.order_purchase_timestamp) as order_dow,\n",
        "        EXTRACT(HOUR FROM o.order_purchase_timestamp) as order_hour,\n",
        "        DATE_PART('day', o.order_delivered_customer_date - o.order_purchase_timestamp) as delivery_days\n",
        "    FROM olist_sales_data_set.olist_customers_dataset c\n",
        "    JOIN olist_sales_data_set.olist_orders_dataset o ON c.customer_id = o.customer_id\n",
        "    WHERE o.order_status = 'delivered'\n",
        "    AND o.order_delivered_customer_date IS NOT NULL\n",
        "),\n",
        "order_financials AS (\n",
        "    SELECT \n",
        "        co.*,\n",
        "        oi.product_id,\n",
        "        oi.price,\n",
        "        oi.freight_value,\n",
        "        (oi.price + oi.freight_value) as total_order_value,\n",
        "        p.product_category_name,\n",
        "        COALESCE(pt.product_category_name_english, p.product_category_name) as category_english,\n",
        "        r.review_score,\n",
        "        CASE \n",
        "            WHEN r.review_score >= 4 THEN 'Satisfied'\n",
        "            WHEN r.review_score = 3 THEN 'Neutral'\n",
        "            WHEN r.review_score <= 2 THEN 'Dissatisfied'\n",
        "            ELSE 'No Review'\n",
        "        END as satisfaction_level\n",
        "    FROM customer_orders co\n",
        "    JOIN olist_sales_data_set.olist_order_items_dataset oi ON co.order_id = oi.order_id\n",
        "    JOIN olist_sales_data_set.olist_products_dataset p ON oi.product_id = p.product_id\n",
        "    LEFT JOIN olist_sales_data_set.product_category_name_translation pt \n",
        "        ON p.product_category_name = pt.product_category_name\n",
        "    LEFT JOIN olist_sales_data_set.olist_order_reviews_dataset r ON co.order_id = r.order_id\n",
        "    WHERE oi.price > 0\n",
        ")\n",
        "SELECT * FROM order_financials\n",
        "LIMIT 25000;\n",
        "\"\"\"\n",
        "\n",
        "# Load the data\n",
        "customer_df = pd.read_sql(customer_behavior_query, engine)\n",
        "\n",
        "# Data preprocessing\n",
        "customer_df['order_purchase_timestamp'] = pd.to_datetime(customer_df['order_purchase_timestamp'])\n",
        "customer_df['order_delivered_customer_date'] = pd.to_datetime(customer_df['order_delivered_customer_date'])\n",
        "customer_df['category_clean'] = customer_df['category_english'].fillna('Unknown').str.title()\n",
        "\n",
        "# Calculate analysis period\n",
        "analysis_end_date = customer_df['order_purchase_timestamp'].max()\n",
        "analysis_start_date = customer_df['order_purchase_timestamp'].min()\n",
        "\n",
        "print(f\"‚úÖ Customer behavior dataset loaded successfully!\")\n",
        "print(f\"   üìä Total records: {len(customer_df):,}\")\n",
        "print(f\"   üë• Unique customers: {customer_df['customer_unique_id'].nunique():,}\")\n",
        "print(f\"   üì¶ Unique orders: {customer_df['order_id'].nunique():,}\")\n",
        "print(f\"   üìÖ Analysis period: {analysis_start_date.date()} to {analysis_end_date.date()}\")\n",
        "print(f\"   üè∑Ô∏è Product categories: {customer_df['category_clean'].nunique()}\")\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\nüìã Sample Customer Behavior Data:\")\n",
        "display(customer_df[['customer_unique_id', 'order_purchase_timestamp', 'category_clean', \n",
        "                   'total_order_value', 'review_score', 'satisfaction_level']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfm_analysis_section"
      },
      "source": [
        "## 3. RFM Analysis - Customer Value Segmentation\n",
        "\n",
        "RFM (Recency, Frequency, Monetary) analysis is a foundational technique for customer segmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfm_calculation"
      },
      "outputs": [],
      "source": [
        "# RFM Analysis Implementation\n",
        "print(\"üìä Implementing RFM (Recency, Frequency, Monetary) Analysis\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def calculate_rfm_metrics(data, customer_id_col, date_col, monetary_col):\n",
        "    \"\"\"\n",
        "    Calculate RFM metrics for customer segmentation\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : pd.DataFrame\n",
        "        Customer transaction data\n",
        "    customer_id_col : str\n",
        "        Name of customer ID column\n",
        "    date_col : str\n",
        "        Name of transaction date column\n",
        "    monetary_col : str\n",
        "        Name of monetary value column\n",
        "    \"\"\"\n",
        "    # Reference date for recency calculation (latest date in dataset)\n",
        "    reference_date = data[date_col].max()\n",
        "    \n",
        "    # Calculate RFM metrics\n",
        "    rfm = data.groupby(customer_id_col).agg({\n",
        "        date_col: lambda x: (reference_date - x.max()).days,  # Recency\n",
        "        'order_id': 'nunique',  # Frequency\n",
        "        monetary_col: 'sum'  # Monetary\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Rename columns\n",
        "    rfm.columns = [customer_id_col, 'Recency', 'Frequency', 'Monetary']\n",
        "    \n",
        "    # Calculate additional metrics\n",
        "    rfm['Avg_Order_Value'] = data.groupby(customer_id_col)[monetary_col].mean().values\n",
        "    rfm['Total_Items'] = data.groupby(customer_id_col).size().values\n",
        "    \n",
        "    return rfm\n",
        "\n",
        "# Calculate RFM metrics\n",
        "rfm_data = calculate_rfm_metrics(\n",
        "    customer_df, \n",
        "    'customer_unique_id', \n",
        "    'order_purchase_timestamp', \n",
        "    'total_order_value'\n",
        ")\n",
        "\n",
        "print(f\"üìà RFM Analysis Results:\")\n",
        "print(f\"   ‚Ä¢ Customers analyzed: {len(rfm_data):,}\")\n",
        "print(f\"   ‚Ä¢ Average recency: {rfm_data['Recency'].mean():.1f} days\")\n",
        "print(f\"   ‚Ä¢ Average frequency: {rfm_data['Frequency'].mean():.1f} orders\")\n",
        "print(f\"   ‚Ä¢ Average monetary value: R$ {rfm_data['Monetary'].mean():.2f}\")\n",
        "\n",
        "# Display RFM statistics\n",
        "print(\"\\nüìä RFM Distribution Statistics:\")\n",
        "display(rfm_data[['Recency', 'Frequency', 'Monetary', 'Avg_Order_Value']].describe())\n",
        "\n",
        "# RFM Scoring (1-5 scale)\n",
        "def assign_rfm_scores(rfm_df):\n",
        "    \"\"\"\n",
        "    Assign RFM scores using quintile-based scoring\n",
        "    \"\"\"\n",
        "    rfm_scored = rfm_df.copy()\n",
        "    \n",
        "    # Recency Score (lower recency = higher score)\n",
        "    rfm_scored['R_Score'] = pd.qcut(rfm_scored['Recency'], 5, labels=[5,4,3,2,1])\n",
        "    \n",
        "    # Frequency Score (higher frequency = higher score)\n",
        "    rfm_scored['F_Score'] = pd.qcut(rfm_scored['Frequency'].rank(method='first'), 5, labels=[1,2,3,4,5])\n",
        "    \n",
        "    # Monetary Score (higher monetary = higher score)\n",
        "    rfm_scored['M_Score'] = pd.qcut(rfm_scored['Monetary'], 5, labels=[1,2,3,4,5])\n",
        "    \n",
        "    # Convert to numeric\n",
        "    rfm_scored['R_Score'] = rfm_scored['R_Score'].astype(int)\n",
        "    rfm_scored['F_Score'] = rfm_scored['F_Score'].astype(int)\n",
        "    rfm_scored['M_Score'] = rfm_scored['M_Score'].astype(int)\n",
        "    \n",
        "    # Create RFM Score combination\n",
        "    rfm_scored['RFM_Score'] = (\n",
        "        rfm_scored['R_Score'].astype(str) + \n",
        "        rfm_scored['F_Score'].astype(str) + \n",
        "        rfm_scored['M_Score'].astype(str)\n",
        "    )\n",
        "    \n",
        "    # Calculate overall score\n",
        "    rfm_scored['RFM_Score_Numeric'] = (\n",
        "        rfm_scored['R_Score'] + rfm_scored['F_Score'] + rfm_scored['M_Score']\n",
        "    )\n",
        "    \n",
        "    return rfm_scored\n",
        "\n",
        "# Apply RFM scoring\n",
        "rfm_scored = assign_rfm_scores(rfm_data)\n",
        "\n",
        "print(\"\\nüéØ RFM Scoring Complete!\")\n",
        "print(\"\\nüìã Sample RFM Scores:\")\n",
        "display(rfm_scored[['customer_unique_id', 'Recency', 'Frequency', 'Monetary', \n",
        "                  'R_Score', 'F_Score', 'M_Score', 'RFM_Score']].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfm_segmentation"
      },
      "outputs": [],
      "source": [
        "# RFM Customer Segmentation\n",
        "print(\"üéØ RFM-Based Customer Segmentation\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "def create_rfm_segments(rfm_df):\n",
        "    \"\"\"\n",
        "    Create customer segments based on RFM scores\n",
        "    \"\"\"\n",
        "    def segment_customers(row):\n",
        "        r, f, m = row['R_Score'], row['F_Score'], row['M_Score']\n",
        "        \n",
        "        # Champions: High RFM scores\n",
        "        if r >= 4 and f >= 4 and m >= 4:\n",
        "            return 'Champions'\n",
        "        \n",
        "        # Loyal Customers: High R and F, any M\n",
        "        elif r >= 4 and f >= 4:\n",
        "            return 'Loyal Customers'\n",
        "        \n",
        "        # Potential Loyalists: High R, any F and M\n",
        "        elif r >= 4:\n",
        "            return 'Potential Loyalists'\n",
        "        \n",
        "        # Recent Customers: High R, low F\n",
        "        elif r >= 3 and f <= 2:\n",
        "            return 'Recent Customers'\n",
        "        \n",
        "        # Promising: Medium R and F\n",
        "        elif r >= 2 and f >= 2 and m >= 2:\n",
        "            return 'Promising'\n",
        "        \n",
        "        # Customers Needing Attention: Low R, high F and M\n",
        "        elif r <= 2 and f >= 3 and m >= 3:\n",
        "            return 'Customers Needing Attention'\n",
        "        \n",
        "        # About to Sleep: Low R and F, any M\n",
        "        elif r <= 2 and f <= 2:\n",
        "            return 'About to Sleep'\n",
        "        \n",
        "        # At Risk: Low R, medium to high F and M\n",
        "        elif r <= 2 and f >= 2:\n",
        "            return 'At Risk'\n",
        "        \n",
        "        # Cannot Lose Them: Low R, high F and M\n",
        "        elif f >= 4 and m >= 4:\n",
        "            return 'Cannot Lose Them'\n",
        "        \n",
        "        # Hibernating: Low RFM scores\n",
        "        else:\n",
        "            return 'Hibernating'\n",
        "    \n",
        "    rfm_df['Customer_Segment'] = rfm_df.apply(segment_customers, axis=1)\n",
        "    return rfm_df\n",
        "\n",
        "# Apply segmentation\n",
        "rfm_segmented = create_rfm_segments(rfm_scored)\n",
        "\n",
        "# Analyze segments\n",
        "segment_analysis = rfm_segmented.groupby('Customer_Segment').agg({\n",
        "    'customer_unique_id': 'count',\n",
        "    'Recency': 'mean',\n",
        "    'Frequency': 'mean',\n",
        "    'Monetary': ['mean', 'sum'],\n",
        "    'Avg_Order_Value': 'mean'\n",
        "}).round(2)\n",
        "\n",
        "# Flatten column names\n",
        "segment_analysis.columns = ['Count', 'Avg_Recency', 'Avg_Frequency', \n",
        "                           'Avg_Monetary', 'Total_Revenue', 'Avg_Order_Value']\n",
        "\n",
        "# Calculate percentages\n",
        "segment_analysis['Percentage'] = (segment_analysis['Count'] / len(rfm_segmented) * 100).round(1)\n",
        "\n",
        "# Sort by revenue contribution\n",
        "segment_analysis = segment_analysis.sort_values('Total_Revenue', ascending=False)\n",
        "\n",
        "print(\"\\nüèÜ Customer Segment Analysis:\")\n",
        "display(segment_analysis)\n",
        "\n",
        "# Business insights\n",
        "total_customers = len(rfm_segmented)\n",
        "total_revenue = segment_analysis['Total_Revenue'].sum()\n",
        "top_segment = segment_analysis.index[0]\n",
        "top_segment_revenue_pct = (segment_analysis.loc[top_segment, 'Total_Revenue'] / total_revenue * 100)\n",
        "\n",
        "print(f\"\\nüí° Key Segment Insights:\")\n",
        "print(f\"   ‚Ä¢ Total customers analyzed: {total_customers:,}\")\n",
        "print(f\"   ‚Ä¢ Most valuable segment: {top_segment} ({segment_analysis.loc[top_segment, 'Percentage']:.1f}% of customers)\")\n",
        "print(f\"   ‚Ä¢ {top_segment} contributes {top_segment_revenue_pct:.1f}% of total revenue\")\n",
        "print(f\"   ‚Ä¢ Average order value of {top_segment}: R$ {segment_analysis.loc[top_segment, 'Avg_Order_Value']:.2f}\")\n",
        "\n",
        "# At-risk analysis\n",
        "at_risk_segments = ['At Risk', 'About to Sleep', 'Hibernating', 'Cannot Lose Them']\n",
        "at_risk_customers = segment_analysis.loc[segment_analysis.index.isin(at_risk_segments), 'Count'].sum()\n",
        "at_risk_percentage = (at_risk_customers / total_customers * 100)\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è Customer Retention Alert:\")\n",
        "print(f\"   ‚Ä¢ At-risk customers: {at_risk_customers:,} ({at_risk_percentage:.1f}% of customer base)\")\n",
        "print(f\"   ‚Ä¢ These segments require immediate attention for retention\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfm_visualization"
      },
      "outputs": [],
      "source": [
        "# RFM Visualization Dashboard\n",
        "print(\"üìä Creating RFM Analysis Visualizations\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Create comprehensive RFM visualization\n",
        "fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "# 1. Customer Segment Distribution\n",
        "plt.subplot(2, 3, 1)\n",
        "segment_counts = rfm_segmented['Customer_Segment'].value_counts()\n",
        "colors = plt.cm.Set3(np.linspace(0, 1, len(segment_counts)))\n",
        "plt.pie(segment_counts.values, labels=segment_counts.index, autopct='%1.1f%%', \n",
        "        colors=colors, startangle=90)\n",
        "plt.title('Customer Segment Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "# 2. Revenue by Segment\n",
        "plt.subplot(2, 3, 2)\n",
        "segment_revenue = segment_analysis.sort_values('Total_Revenue', ascending=True)\n",
        "plt.barh(range(len(segment_revenue)), segment_revenue['Total_Revenue'], color='lightcoral')\n",
        "plt.yticks(range(len(segment_revenue)), segment_revenue.index)\n",
        "plt.xlabel('Total Revenue (R$)')\n",
        "plt.title('Revenue Contribution by Segment', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. RFM Score Distribution\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.hist(rfm_segmented['RFM_Score_Numeric'], bins=15, color='skyblue', alpha=0.7, edgecolor='black')\n",
        "plt.axvline(rfm_segmented['RFM_Score_Numeric'].mean(), color='red', linestyle='--', \n",
        "           label=f'Mean: {rfm_segmented[\"RFM_Score_Numeric\"].mean():.1f}')\n",
        "plt.xlabel('RFM Score (Sum)')\n",
        "plt.ylabel('Number of Customers')\n",
        "plt.title('Distribution of RFM Scores', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. RFM Heatmap\n",
        "plt.subplot(2, 3, 4)\n",
        "rfm_summary = rfm_segmented.groupby(['R_Score', 'F_Score'])['Monetary'].mean().reset_index()\n",
        "rfm_pivot = rfm_summary.pivot(index='F_Score', columns='R_Score', values='Monetary')\n",
        "sns.heatmap(rfm_pivot, annot=True, fmt='.0f', cmap='YlOrRd', cbar_kws={'label': 'Avg Monetary Value'})\n",
        "plt.title('RFM Heatmap: Average Monetary Value', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Frequency Score')\n",
        "plt.xlabel('Recency Score')\n",
        "\n",
        "# 5. Recency vs Frequency Scatter\n",
        "plt.subplot(2, 3, 5)\n",
        "scatter = plt.scatter(rfm_segmented['Recency'], rfm_segmented['Frequency'], \n",
        "                     c=rfm_segmented['Monetary'], cmap='viridis', alpha=0.6, s=50)\n",
        "plt.colorbar(scatter, label='Monetary Value (R$)')\n",
        "plt.xlabel('Recency (Days)')\n",
        "plt.ylabel('Frequency (Orders)')\n",
        "plt.title('Customer Distribution: Recency vs Frequency', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 6. Segment Performance Metrics\n",
        "plt.subplot(2, 3, 6)\n",
        "top_segments = segment_analysis.head(5)\n",
        "x_pos = range(len(top_segments))\n",
        "plt.bar(x_pos, top_segments['Avg_Order_Value'], color='lightgreen', alpha=0.7)\n",
        "plt.xticks(x_pos, top_segments.index, rotation=45, ha='right')\n",
        "plt.ylabel('Average Order Value (R$)')\n",
        "plt.title('Top 5 Segments by Avg Order Value', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Interactive RFM Analysis with Plotly\n",
        "print(\"\\nüé® Creating Interactive RFM Dashboard...\")\n",
        "\n",
        "# Create interactive 3D scatter plot\n",
        "fig_3d = px.scatter_3d(\n",
        "    rfm_segmented, \n",
        "    x='Recency', \n",
        "    y='Frequency', \n",
        "    z='Monetary',\n",
        "    color='Customer_Segment',\n",
        "    size='Avg_Order_Value',\n",
        "    hover_data=['RFM_Score'],\n",
        "    title='Interactive 3D RFM Analysis',\n",
        "    width=900,\n",
        "    height=600\n",
        ")\n",
        "\n",
        "fig_3d.update_layout(\n",
        "    scene=dict(\n",
        "        xaxis_title='Recency (Days)',\n",
        "        yaxis_title='Frequency (Orders)',\n",
        "        zaxis_title='Monetary Value (R$)'\n",
        "    )\n",
        ")\n",
        "\n",
        "fig_3d.show()\n",
        "\n",
        "print(\"‚úÖ RFM Analysis visualizations complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "customer_journey_section"
      },
      "source": [
        "## 4. Customer Journey and Lifecycle Analysis\n",
        "\n",
        "Understanding how customers evolve over time and their purchasing patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "customer_journey_analysis"
      },
      "outputs": [],
      "source": [
        "# Customer Journey and Lifecycle Analysis\n",
        "print(\"üõ§Ô∏è Customer Journey and Lifecycle Analysis\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "def analyze_customer_lifecycle(data):\n",
        "    \"\"\"\n",
        "    Analyze customer lifecycle patterns and journey stages\n",
        "    \"\"\"\n",
        "    # Customer lifecycle metrics\n",
        "    customer_lifecycle = data.groupby('customer_unique_id').agg({\n",
        "        'order_purchase_timestamp': ['min', 'max', 'count'],\n",
        "        'total_order_value': ['sum', 'mean', 'std'],\n",
        "        'category_clean': 'nunique',\n",
        "        'delivery_days': 'mean',\n",
        "        'review_score': 'mean'\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Flatten column names\n",
        "    customer_lifecycle.columns = [\n",
        "        'customer_unique_id', 'first_order_date', 'last_order_date', 'total_orders',\n",
        "        'total_spent', 'avg_order_value', 'order_value_std', 'categories_purchased',\n",
        "        'avg_delivery_days', 'avg_review_score'\n",
        "    ]\n",
        "    \n",
        "    # Calculate customer lifetime metrics\n",
        "    customer_lifecycle['customer_lifespan_days'] = (\n",
        "        customer_lifecycle['last_order_date'] - customer_lifecycle['first_order_date']\n",
        "    ).dt.days\n",
        "    \n",
        "    # Calculate time since last order (churn risk indicator)\n",
        "    reference_date = data['order_purchase_timestamp'].max()\n",
        "    customer_lifecycle['days_since_last_order'] = (\n",
        "        reference_date - customer_lifecycle['last_order_date']\n",
        "    ).dt.days\n",
        "    \n",
        "    # Customer maturity stages\n",
        "    def assign_lifecycle_stage(row):\n",
        "        days_since_last = row['days_since_last_order']\n",
        "        total_orders = row['total_orders']\n",
        "        lifespan = row['customer_lifespan_days']\n",
        "        \n",
        "        if total_orders == 1:\n",
        "            if days_since_last <= 30:\n",
        "                return 'New Customer'\n",
        "            elif days_since_last <= 90:\n",
        "                return 'One-time Buyer'\n",
        "            else:\n",
        "                return 'Lost New Customer'\n",
        "        \n",
        "        elif total_orders <= 3:\n",
        "            if days_since_last <= 60:\n",
        "                return 'Developing Customer'\n",
        "            else:\n",
        "                return 'At Risk'\n",
        "        \n",
        "        elif total_orders <= 7:\n",
        "            if days_since_last <= 90:\n",
        "                return 'Regular Customer'\n",
        "            else:\n",
        "                return 'Declining Customer'\n",
        "        \n",
        "        else:\n",
        "            if days_since_last <= 120:\n",
        "                return 'VIP Customer'\n",
        "            else:\n",
        "                return 'VIP at Risk'\n",
        "    \n",
        "    customer_lifecycle['lifecycle_stage'] = customer_lifecycle.apply(assign_lifecycle_stage, axis=1)\n",
        "    \n",
        "    return customer_lifecycle\n",
        "\n",
        "# Perform lifecycle analysis\n",
        "lifecycle_data = analyze_customer_lifecycle(customer_df)\n",
        "\n",
        "print(f\"üìä Customer Lifecycle Analysis Results:\")\n",
        "print(f\"   ‚Ä¢ Customers analyzed: {len(lifecycle_data):,}\")\n",
        "print(f\"   ‚Ä¢ Average customer lifespan: {lifecycle_data['customer_lifespan_days'].mean():.1f} days\")\n",
        "print(f\"   ‚Ä¢ Average orders per customer: {lifecycle_data['total_orders'].mean():.1f}\")\n",
        "print(f\"   ‚Ä¢ Average total spent per customer: R$ {lifecycle_data['total_spent'].mean():.2f}\")\n",
        "\n",
        "# Lifecycle stage distribution\n",
        "stage_distribution = lifecycle_data['lifecycle_stage'].value_counts()\n",
        "print(f\"\\nüéØ Customer Lifecycle Stage Distribution:\")\n",
        "for stage, count in stage_distribution.items():\n",
        "    percentage = (count / len(lifecycle_data)) * 100\n",
        "    print(f\"   ‚Ä¢ {stage}: {count:,} customers ({percentage:.1f}%)\")\n",
        "\n",
        "# Lifecycle stage performance metrics\n",
        "stage_performance = lifecycle_data.groupby('lifecycle_stage').agg({\n",
        "    'total_orders': 'mean',\n",
        "    'total_spent': 'mean',\n",
        "    'avg_order_value': 'mean',\n",
        "    'categories_purchased': 'mean',\n",
        "    'avg_review_score': 'mean',\n",
        "    'customer_lifespan_days': 'mean',\n",
        "    'days_since_last_order': 'mean'\n",
        "}).round(2)\n",
        "\n",
        "print(f\"\\nüìà Lifecycle Stage Performance Metrics:\")\n",
        "display(stage_performance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "purchase_pattern_analysis"
      },
      "outputs": [],
      "source": [
        "# Purchase Pattern and Behavior Analysis\n",
        "print(\"üõí Purchase Pattern and Behavior Analysis\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "# Seasonal and temporal purchase patterns\n",
        "def analyze_purchase_patterns(data):\n",
        "    \"\"\"\n",
        "    Analyze detailed purchase patterns and behaviors\n",
        "    \"\"\"\n",
        "    # Create comprehensive purchase pattern analysis\n",
        "    pattern_analysis = {\n",
        "        'temporal': {},\n",
        "        'category': {},\n",
        "        'geographic': {},\n",
        "        'behavioral': {}\n",
        "    }\n",
        "    \n",
        "    # Temporal patterns\n",
        "    pattern_analysis['temporal']['monthly'] = data.groupby('order_month').agg({\n",
        "        'total_order_value': ['count', 'mean', 'sum'],\n",
        "        'review_score': 'mean'\n",
        "    }).round(2)\n",
        "    \n",
        "    pattern_analysis['temporal']['hourly'] = data.groupby('order_hour').agg({\n",
        "        'total_order_value': ['count', 'mean'],\n",
        "        'customer_unique_id': 'nunique'\n",
        "    }).round(2)\n",
        "    \n",
        "    pattern_analysis['temporal']['daily'] = data.groupby('order_dow').agg({\n",
        "        'total_order_value': ['count', 'mean'],\n",
        "        'customer_unique_id': 'nunique'\n",
        "    }).round(2)\n",
        "    \n",
        "    # Category preferences by customer segment (from RFM)\n",
        "    customer_segments = rfm_segmented[['customer_unique_id', 'Customer_Segment']]\n",
        "    data_with_segments = data.merge(customer_segments, on='customer_unique_id', how='left')\n",
        "    \n",
        "    pattern_analysis['category']['by_segment'] = data_with_segments.groupby(\n",
        "        ['Customer_Segment', 'category_clean']\n",
        "    ).agg({\n",
        "        'total_order_value': ['count', 'sum', 'mean']\n",
        "    }).round(2)\n",
        "    \n",
        "    # Geographic spending patterns\n",
        "    pattern_analysis['geographic']['by_state'] = data.groupby('customer_state').agg({\n",
        "        'total_order_value': ['count', 'mean', 'sum'],\n",
        "        'customer_unique_id': 'nunique',\n",
        "        'review_score': 'mean'\n",
        "    }).round(2)\n",
        "    \n",
        "    return pattern_analysis, data_with_segments\n",
        "\n",
        "# Perform pattern analysis\n",
        "patterns, customer_data_segmented = analyze_purchase_patterns(customer_df)\n",
        "\n",
        "# Display key temporal patterns\n",
        "print(\"‚è∞ Temporal Purchase Patterns:\")\n",
        "\n",
        "# Monthly patterns\n",
        "monthly_data = patterns['temporal']['monthly']\n",
        "monthly_data.columns = ['Order_Count', 'Avg_Order_Value', 'Total_Revenue', 'Avg_Review']\n",
        "print(\"\\nüìÖ Monthly Patterns:\")\n",
        "display(monthly_data)\n",
        "\n",
        "# Find peak periods\n",
        "peak_month = monthly_data['Order_Count'].idxmax()\n",
        "peak_hour = patterns['temporal']['hourly'][('total_order_value', 'count')].idxmax()\n",
        "peak_day = patterns['temporal']['daily'][('total_order_value', 'count')].idxmax()\n",
        "\n",
        "day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "peak_day_name = day_names[peak_day]\n",
        "\n",
        "print(f\"\\nüìä Peak Activity Periods:\")\n",
        "print(f\"   ‚Ä¢ Peak month: {peak_month} ({monthly_data.loc[peak_month, 'Order_Count']:,} orders)\")\n",
        "print(f\"   ‚Ä¢ Peak hour: {peak_hour}:00 ({patterns['temporal']['hourly'].loc[peak_hour, ('total_order_value', 'count')]:,} orders)\")\n",
        "print(f\"   ‚Ä¢ Peak day: {peak_day_name} ({patterns['temporal']['daily'].loc[peak_day, ('total_order_value', 'count')]:,} orders)\")\n",
        "\n",
        "# Category preferences by customer segment\n",
        "print(f\"\\nüè∑Ô∏è Category Preferences by Customer Segment:\")\n",
        "top_categories_by_segment = {}\n",
        "\n",
        "for segment in customer_data_segmented['Customer_Segment'].unique():\n",
        "    if pd.notna(segment):\n",
        "        segment_data = customer_data_segmented[customer_data_segmented['Customer_Segment'] == segment]\n",
        "        top_categories = segment_data['category_clean'].value_counts().head(3)\n",
        "        top_categories_by_segment[segment] = top_categories.to_dict()\n",
        "        \n",
        "        print(f\"\\n   {segment}:\")\n",
        "        for i, (category, count) in enumerate(top_categories.items(), 1):\n",
        "            percentage = (count / len(segment_data)) * 100\n",
        "            print(f\"     {i}. {category}: {count} purchases ({percentage:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "advanced_segmentation"
      },
      "source": [
        "## 5. Advanced Customer Segmentation using Machine Learning\n",
        "\n",
        "Beyond RFM, let's use clustering algorithms for more sophisticated segmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ml_segmentation"
      },
      "outputs": [],
      "source": [
        "# Advanced ML-Based Customer Segmentation\n",
        "print(\"ü§ñ Advanced Machine Learning Customer Segmentation\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "def prepare_segmentation_features(customer_data, lifecycle_data, rfm_data):\n",
        "    \"\"\"\n",
        "    Prepare comprehensive feature set for ML-based segmentation\n",
        "    \"\"\"\n",
        "    # Aggregate customer behavioral features\n",
        "    behavioral_features = customer_data.groupby('customer_unique_id').agg({\n",
        "        'total_order_value': ['sum', 'mean', 'std', 'count'],\n",
        "        'category_clean': ['nunique'],\n",
        "        'delivery_days': ['mean', 'std'],\n",
        "        'review_score': ['mean', 'std', 'count'],\n",
        "        'order_month': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else x.iloc[0],  # Preferred month\n",
        "        'order_hour': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else x.iloc[0],   # Preferred hour\n",
        "        'order_dow': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else x.iloc[0],    # Preferred day\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Flatten column names\n",
        "    behavioral_features.columns = [\n",
        "        'customer_unique_id', 'total_spent', 'avg_order_value', 'order_value_std', 'total_orders',\n",
        "        'categories_purchased', 'avg_delivery_days', 'delivery_std', \n",
        "        'avg_review_score', 'review_score_std', 'review_count',\n",
        "        'preferred_month', 'preferred_hour', 'preferred_day'\n",
        "    ]\n",
        "    \n",
        "    # Fill NaN values\n",
        "    behavioral_features = behavioral_features.fillna(0)\n",
        "    \n",
        "    # Merge with lifecycle and RFM data\n",
        "    feature_data = behavioral_features.merge(\n",
        "        lifecycle_data[['customer_unique_id', 'customer_lifespan_days', 'days_since_last_order']], \n",
        "        on='customer_unique_id'\n",
        "    ).merge(\n",
        "        rfm_data[['customer_unique_id', 'Recency', 'Frequency', 'Monetary']], \n",
        "        on='customer_unique_id'\n",
        "    )\n",
        "    \n",
        "    # Create additional derived features\n",
        "    feature_data['order_frequency_rate'] = feature_data['total_orders'] / (feature_data['customer_lifespan_days'] + 1)\n",
        "    feature_data['spending_consistency'] = 1 / (feature_data['order_value_std'] + 1)  # Higher = more consistent\n",
        "    feature_data['engagement_score'] = (\n",
        "        feature_data['avg_review_score'] * feature_data['review_count'] / \n",
        "        (feature_data['total_orders'] + 1)\n",
        "    )\n",
        "    \n",
        "    return feature_data\n",
        "\n",
        "# Prepare features\n",
        "ml_features = prepare_segmentation_features(customer_df, lifecycle_data, rfm_data)\n",
        "\n",
        "print(f\"üìä ML Segmentation Feature Summary:\")\n",
        "print(f\"   ‚Ä¢ Customers: {len(ml_features):,}\")\n",
        "print(f\"   ‚Ä¢ Features: {ml_features.shape[1] - 1} (excluding customer ID)\")\n",
        "\n",
        "# Select features for clustering\n",
        "clustering_features = [\n",
        "    'total_spent', 'avg_order_value', 'total_orders', 'categories_purchased',\n",
        "    'avg_delivery_days', 'avg_review_score', 'customer_lifespan_days', \n",
        "    'days_since_last_order', 'order_frequency_rate', 'spending_consistency', \n",
        "    'engagement_score'\n",
        "]\n",
        "\n",
        "# Prepare data for clustering\n",
        "X = ml_features[clustering_features].fillna(0)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(f\"\\nüîß Feature Engineering Complete:\")\n",
        "print(f\"   ‚Ä¢ Selected {len(clustering_features)} features for clustering\")\n",
        "print(f\"   ‚Ä¢ Features standardized for ML algorithms\")\n",
        "\n",
        "# Display feature importance/correlation with spending\n",
        "feature_correlations = pd.DataFrame({\n",
        "    'Feature': clustering_features,\n",
        "    'Correlation_with_Spending': [X[feature].corr(X['total_spent']) for feature in clustering_features]\n",
        "}).sort_values('Correlation_with_Spending', key=abs, ascending=False)\n",
        "\n",
        "print(f\"\\nüìà Feature Correlations with Total Spending:\")\n",
        "display(feature_correlations.round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clustering_analysis"
      },
      "outputs": [],
      "source": [
        "# Optimal Cluster Analysis and K-Means Clustering\n",
        "print(\"üéØ Optimal Cluster Analysis and K-Means Clustering\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "def find_optimal_clusters(X, max_clusters=10):\n",
        "    \"\"\"\n",
        "    Find optimal number of clusters using elbow method and silhouette analysis\n",
        "    \"\"\"\n",
        "    inertias = []\n",
        "    silhouette_scores = []\n",
        "    K_range = range(2, max_clusters + 1)\n",
        "    \n",
        "    for k in K_range:\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "        kmeans.fit(X)\n",
        "        \n",
        "        inertias.append(kmeans.inertia_)\n",
        "        silhouette_scores.append(silhouette_score(X, kmeans.labels_))\n",
        "    \n",
        "    return K_range, inertias, silhouette_scores\n",
        "\n",
        "# Find optimal clusters\n",
        "k_range, inertias, sil_scores = find_optimal_clusters(X_scaled, max_clusters=8)\n",
        "\n",
        "# Plot elbow curve and silhouette scores\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Elbow curve\n",
        "ax1.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
        "ax1.set_xlabel('Number of Clusters (k)')\n",
        "ax1.set_ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
        "ax1.set_title('Elbow Method for Optimal k', fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Silhouette scores\n",
        "ax2.plot(k_range, sil_scores, 'ro-', linewidth=2, markersize=8)\n",
        "ax2.set_xlabel('Number of Clusters (k)')\n",
        "ax2.set_ylabel('Silhouette Score')\n",
        "ax2.set_title('Silhouette Analysis', fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Select optimal k\n",
        "optimal_k = k_range[np.argmax(sil_scores)]\n",
        "best_silhouette = max(sil_scores)\n",
        "\n",
        "print(f\"\\nüéØ Optimal Clustering Results:\")\n",
        "print(f\"   ‚Ä¢ Optimal number of clusters: {optimal_k}\")\n",
        "print(f\"   ‚Ä¢ Best silhouette score: {best_silhouette:.3f}\")\n",
        "\n",
        "# Perform final clustering\n",
        "final_kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "cluster_labels = final_kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Add cluster labels to our data\n",
        "ml_features['ML_Cluster'] = cluster_labels\n",
        "\n",
        "# Analyze clusters\n",
        "cluster_analysis = ml_features.groupby('ML_Cluster')[clustering_features].mean().round(2)\n",
        "cluster_sizes = ml_features['ML_Cluster'].value_counts().sort_index()\n",
        "\n",
        "print(f\"\\nüìä ML-Based Cluster Analysis:\")\n",
        "print(f\"\\nCluster Sizes:\")\n",
        "for cluster, size in cluster_sizes.items():\n",
        "    percentage = (size / len(ml_features)) * 100\n",
        "    print(f\"   Cluster {cluster}: {size:,} customers ({percentage:.1f}%)\")\n",
        "\n",
        "print(f\"\\nüìà Cluster Characteristics:\")\n",
        "display(cluster_analysis)\n",
        "\n",
        "# Create cluster profiles\n",
        "def create_cluster_profiles(data, cluster_col):\n",
        "    \"\"\"\n",
        "    Create descriptive profiles for each cluster\n",
        "    \"\"\"\n",
        "    profiles = {}\n",
        "    \n",
        "    for cluster in sorted(data[cluster_col].unique()):\n",
        "        cluster_data = data[data[cluster_col] == cluster]\n",
        "        \n",
        "        profile = {\n",
        "            'size': len(cluster_data),\n",
        "            'avg_total_spent': cluster_data['total_spent'].mean(),\n",
        "            'avg_order_value': cluster_data['avg_order_value'].mean(),\n",
        "            'avg_orders': cluster_data['total_orders'].mean(),\n",
        "            'avg_categories': cluster_data['categories_purchased'].mean(),\n",
        "            'avg_review_score': cluster_data['avg_review_score'].mean(),\n",
        "            'avg_lifespan': cluster_data['customer_lifespan_days'].mean(),\n",
        "            'days_since_last': cluster_data['days_since_last_order'].mean()\n",
        "        }\n",
        "        \n",
        "        profiles[f'Cluster_{cluster}'] = profile\n",
        "    \n",
        "    return profiles\n",
        "\n",
        "# Create cluster profiles\n",
        "cluster_profiles = create_cluster_profiles(ml_features, 'ML_Cluster')\n",
        "\n",
        "print(f\"\\nüé≠ Customer Cluster Profiles:\")\n",
        "for cluster_name, profile in cluster_profiles.items():\n",
        "    print(f\"\\n   {cluster_name} ({profile['size']:,} customers):\")\n",
        "    print(f\"     ‚Ä¢ Average total spent: R$ {profile['avg_total_spent']:.2f}\")\n",
        "    print(f\"     ‚Ä¢ Average order value: R$ {profile['avg_order_value']:.2f}\")\n",
        "    print(f\"     ‚Ä¢ Average orders: {profile['avg_orders']:.1f}\")\n",
        "    print(f\"     ‚Ä¢ Categories explored: {profile['avg_categories']:.1f}\")\n",
        "    print(f\"     ‚Ä¢ Average review score: {profile['avg_review_score']:.2f}\")\n",
        "    print(f\"     ‚Ä¢ Customer lifespan: {profile['avg_lifespan']:.0f} days\")\n",
        "    print(f\"     ‚Ä¢ Days since last order: {profile['days_since_last']:.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "customer_personas"
      },
      "source": [
        "## 6. Customer Personas and Business Recommendations\n",
        "\n",
        "Transform our analytical insights into actionable customer personas and strategic recommendations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "customer_personas_creation"
      },
      "outputs": [],
      "source": [
        "# Customer Personas Creation and Business Strategy\n",
        "print(\"üë• Customer Personas and Business Strategy Development\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def create_customer_personas(ml_features, customer_data_segmented):\n",
        "    \"\"\"\n",
        "    Create detailed customer personas based on clustering and behavioral analysis\n",
        "    \"\"\"\n",
        "    personas = {}\n",
        "    \n",
        "    # Merge cluster data with original transaction data for deeper insights\n",
        "    persona_data = customer_data_segmented.merge(\n",
        "        ml_features[['customer_unique_id', 'ML_Cluster']], \n",
        "        on='customer_unique_id', \n",
        "        how='left'\n",
        "    )\n",
        "    \n",
        "    for cluster in sorted(ml_features['ML_Cluster'].unique()):\n",
        "        cluster_customers = ml_features[ml_features['ML_Cluster'] == cluster]\n",
        "        cluster_transactions = persona_data[persona_data['ML_Cluster'] == cluster]\n",
        "        \n",
        "        # Calculate persona characteristics\n",
        "        persona = {\n",
        "            'cluster_id': cluster,\n",
        "            'size': len(cluster_customers),\n",
        "            'percentage': (len(cluster_customers) / len(ml_features)) * 100,\n",
        "            \n",
        "            # Financial characteristics\n",
        "            'avg_total_spent': cluster_customers['total_spent'].mean(),\n",
        "            'median_total_spent': cluster_customers['total_spent'].median(),\n",
        "            'avg_order_value': cluster_customers['avg_order_value'].mean(),\n",
        "            'total_revenue_contribution': cluster_customers['total_spent'].sum(),\n",
        "            \n",
        "            # Behavioral characteristics\n",
        "            'avg_orders': cluster_customers['total_orders'].mean(),\n",
        "            'avg_categories': cluster_customers['categories_purchased'].mean(),\n",
        "            'avg_review_score': cluster_customers['avg_review_score'].mean(),\n",
        "            'avg_engagement': cluster_customers['engagement_score'].mean(),\n",
        "            \n",
        "            # Lifecycle characteristics\n",
        "            'avg_lifespan_days': cluster_customers['customer_lifespan_days'].mean(),\n",
        "            'avg_days_since_last': cluster_customers['days_since_last_order'].mean(),\n",
        "            'order_frequency_rate': cluster_customers['order_frequency_rate'].mean(),\n",
        "            \n",
        "            # Geographic and temporal preferences\n",
        "            'top_states': cluster_transactions['customer_state'].value_counts().head(3).to_dict(),\n",
        "            'top_categories': cluster_transactions['category_clean'].value_counts().head(5).to_dict(),\n",
        "            'preferred_order_hour': cluster_customers['preferred_hour'].mode().iloc[0] if len(cluster_customers) > 0 else 0,\n",
        "            'preferred_order_day': cluster_customers['preferred_day'].mode().iloc[0] if len(cluster_customers) > 0 else 0,\n",
        "            \n",
        "            # Satisfaction metrics\n",
        "            'satisfaction_distribution': cluster_transactions['satisfaction_level'].value_counts().to_dict()\n",
        "        }\n",
        "        \n",
        "        personas[f'Cluster_{cluster}'] = persona\n",
        "    \n",
        "    return personas\n",
        "\n",
        "# Create customer personas\n",
        "customer_personas = create_customer_personas(ml_features, customer_data_segmented)\n",
        "\n",
        "# Calculate total revenue for percentage calculations\n",
        "total_revenue = sum([persona['total_revenue_contribution'] for persona in customer_personas.values()])\n",
        "\n",
        "# Enhanced persona descriptions with business insights\n",
        "def generate_persona_insights(personas, total_revenue):\n",
        "    \"\"\"\n",
        "    Generate business insights and recommendations for each persona\n",
        "    \"\"\"\n",
        "    insights = {}\n",
        "    \n",
        "    for persona_name, persona in personas.items():\n",
        "        revenue_percentage = (persona['total_revenue_contribution'] / total_revenue) * 100\n",
        "        \n",
        "        # Determine persona archetype based on characteristics\n",
        "        if persona['avg_total_spent'] > 500 and persona['avg_orders'] > 5:\n",
        "            archetype = \"VIP Champions\"\n",
        "            description = \"High-value, loyal customers who drive significant revenue\"\n",
        "        elif persona['avg_orders'] > 3 and persona['avg_review_score'] > 4:\n",
        "            archetype = \"Loyal Advocates\"\n",
        "            description = \"Satisfied repeat customers who could become brand ambassadors\"\n",
        "        elif persona['avg_days_since_last'] > 120:\n",
        "            archetype = \"At-Risk Customers\"\n",
        "            description = \"Previously active customers who haven't purchased recently\"\n",
        "        elif persona['avg_orders'] <= 1.5:\n",
        "            archetype = \"New/One-Time Buyers\"\n",
        "            description = \"Customers who have made few purchases, potential for growth\"\n",
        "        elif persona['avg_order_value'] < 50:\n",
        "            archetype = \"Budget Shoppers\"\n",
        "            description = \"Price-conscious customers who make smaller purchases\"\n",
        "        else:\n",
        "            archetype = \"Regular Customers\"\n",
        "            description = \"Steady customers with moderate purchasing behavior\"\n",
        "        \n",
        "        # Generate recommendations\n",
        "        recommendations = []\n",
        "        \n",
        "        if \"VIP\" in archetype:\n",
        "            recommendations = [\n",
        "                \"Offer exclusive products and early access to sales\",\n",
        "                \"Implement premium customer service tier\",\n",
        "                \"Create VIP loyalty program with enhanced benefits\",\n",
        "                \"Personalized recommendations based on purchase history\"\n",
        "            ]\n",
        "        elif \"At-Risk\" in archetype:\n",
        "            recommendations = [\n",
        "                \"Send targeted win-back campaigns with special offers\",\n",
        "                \"Implement cart abandonment recovery sequences\",\n",
        "                \"Survey to understand reasons for decreased activity\",\n",
        "                \"Offer time-limited discounts to encourage re-engagement\"\n",
        "            ]\n",
        "        elif \"New\" in archetype or \"One-Time\" in archetype:\n",
        "            recommendations = [\n",
        "                \"Create onboarding email sequences\",\n",
        "                \"Offer first-time buyer incentives for second purchase\",\n",
        "                \"Showcase popular products and categories\",\n",
        "                \"Implement retargeting campaigns\"\n",
        "            ]\n",
        "        elif \"Budget\" in archetype:\n",
        "            recommendations = [\n",
        "                \"Highlight value propositions and cost savings\",\n",
        "                \"Promote bulk buying and bundle offers\",\n",
        "                \"Send notifications about sales and clearance items\",\n",
        "                \"Focus on affordability in messaging\"\n",
        "            ]\n",
        "        else:\n",
        "            recommendations = [\n",
        "                \"Maintain consistent engagement with regular promotions\",\n",
        "                \"Cross-sell complementary products\",\n",
        "                \"Encourage category exploration\",\n",
        "                \"Build loyalty through consistent experience\"\n",
        "            ]\n",
        "        \n",
        "        insights[persona_name] = {\n",
        "            'archetype': archetype,\n",
        "            'description': description,\n",
        "            'revenue_percentage': revenue_percentage,\n",
        "            'recommendations': recommendations\n",
        "        }\n",
        "    \n",
        "    return insights\n",
        "\n",
        "# Generate persona insights\n",
        "persona_insights = generate_persona_insights(customer_personas, total_revenue)\n",
        "\n",
        "# Display comprehensive persona analysis\n",
        "print(f\"\\nüé≠ COMPREHENSIVE CUSTOMER PERSONA ANALYSIS\")\n",
        "print(f\"=\" * 60)\n",
        "\n",
        "for persona_name, persona in customer_personas.items():\n",
        "    insight = persona_insights[persona_name]\n",
        "    \n",
        "    print(f\"\\nüìä {persona_name.upper()}: {insight['archetype']}\")\n",
        "    print(f\"   {insight['description']}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    print(f\"   üë• Size: {persona['size']:,} customers ({persona['percentage']:.1f}% of customer base)\")\n",
        "    print(f\"   üí∞ Revenue: R$ {persona['total_revenue_contribution']:,.2f} ({insight['revenue_percentage']:.1f}% of total)\")\n",
        "    print(f\"   üíµ Avg Total Spent: R$ {persona['avg_total_spent']:.2f}\")\n",
        "    print(f\"   üõí Avg Order Value: R$ {persona['avg_order_value']:.2f}\")\n",
        "    print(f\"   üì¶ Avg Orders: {persona['avg_orders']:.1f}\")\n",
        "    print(f\"   ‚≠ê Avg Review Score: {persona['avg_review_score']:.2f}\")\n",
        "    print(f\"   üìÖ Days Since Last Order: {persona['avg_days_since_last']:.0f}\")\n",
        "    \n",
        "    print(f\"\\n   üè∑Ô∏è Top Categories:\")\n",
        "    for i, (category, count) in enumerate(list(persona['top_categories'].items())[:3], 1):\n",
        "        print(f\"     {i}. {category}: {count} purchases\")\n",
        "    \n",
        "    print(f\"\\n   üéØ Strategic Recommendations:\")\n",
        "    for i, rec in enumerate(insight['recommendations'], 1):\n",
        "        print(f\"     {i}. {rec}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "business_action_plan"
      },
      "outputs": [],
      "source": [
        "# Comprehensive Business Action Plan\n",
        "print(\"üìã COMPREHENSIVE BUSINESS ACTION PLAN\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Calculate key business metrics\n",
        "total_customers = len(ml_features)\n",
        "total_revenue = ml_features['total_spent'].sum()\n",
        "avg_customer_value = total_revenue / total_customers\n",
        "\n",
        "# Identify priority segments\n",
        "revenue_by_cluster = {}\n",
        "for persona_name, persona in customer_personas.items():\n",
        "    revenue_by_cluster[persona_name] = persona['total_revenue_contribution']\n",
        "\n",
        "# Sort by revenue contribution\n",
        "priority_segments = sorted(revenue_by_cluster.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(f\"\\nüíº EXECUTIVE SUMMARY:\")\n",
        "print(f\"   ‚Ä¢ Total customers analyzed: {total_customers:,}\")\n",
        "print(f\"   ‚Ä¢ Total revenue: R$ {total_revenue:,.2f}\")\n",
        "print(f\"   ‚Ä¢ Average customer lifetime value: R$ {avg_customer_value:.2f}\")\n",
        "print(f\"   ‚Ä¢ Customer segments identified: {len(customer_personas)}\")\n",
        "\n",
        "print(f\"\\nüèÜ REVENUE PRIORITY RANKING:\")\n",
        "for i, (segment, revenue) in enumerate(priority_segments, 1):\n",
        "    percentage = (revenue / total_revenue) * 100\n",
        "    archetype = persona_insights[segment]['archetype']\n",
        "    print(f\"   {i}. {segment} ({archetype}): R$ {revenue:,.2f} ({percentage:.1f}%)\")\n",
        "\n",
        "# Strategic initiatives\n",
        "print(f\"\\nüéØ STRATEGIC INITIATIVES BY PRIORITY:\")\n",
        "\n",
        "print(f\"\\n   1. VIP CUSTOMER RETENTION (High Priority):\")\n",
        "vip_segments = [s for s in persona_insights.keys() if 'VIP' in persona_insights[s]['archetype']]\n",
        "if vip_segments:\n",
        "    vip_revenue = sum([customer_personas[s]['total_revenue_contribution'] for s in vip_segments])\n",
        "    vip_percentage = (vip_revenue / total_revenue) * 100\n",
        "    print(f\"     ‚Ä¢ Target: {len(vip_segments)} VIP segments contributing {vip_percentage:.1f}% of revenue\")\n",
        "    print(f\"     ‚Ä¢ Action: Implement premium loyalty program\")\n",
        "    print(f\"     ‚Ä¢ Expected impact: Increase VIP retention by 15-25%\")\n",
        "\n",
        "print(f\"\\n   2. AT-RISK CUSTOMER RECOVERY (High Priority):\")\n",
        "at_risk_segments = [s for s in persona_insights.keys() if 'At-Risk' in persona_insights[s]['archetype']]\n",
        "if at_risk_segments:\n",
        "    at_risk_customers = sum([customer_personas[s]['size'] for s in at_risk_segments])\n",
        "    at_risk_percentage = (at_risk_customers / total_customers) * 100\n",
        "    print(f\"     ‚Ä¢ Target: {at_risk_customers:,} at-risk customers ({at_risk_percentage:.1f}% of base)\")\n",
        "    print(f\"     ‚Ä¢ Action: Launch win-back campaigns\")\n",
        "    print(f\"     ‚Ä¢ Expected impact: Recover 10-20% of at-risk customers\")\n",
        "\n",
        "print(f\"\\n   3. NEW CUSTOMER CONVERSION (Medium Priority):\")\n",
        "new_segments = [s for s in persona_insights.keys() if 'New' in persona_insights[s]['archetype'] or 'One-Time' in persona_insights[s]['archetype']]\n",
        "if new_segments:\n",
        "    new_customers = sum([customer_personas[s]['size'] for s in new_segments])\n",
        "    new_percentage = (new_customers / total_customers) * 100\n",
        "    print(f\"     ‚Ä¢ Target: {new_customers:,} new/one-time customers ({new_percentage:.1f}% of base)\")\n",
        "    print(f\"     ‚Ä¢ Action: Optimize onboarding and second purchase incentives\")\n",
        "    print(f\"     ‚Ä¢ Expected impact: Increase conversion rate by 5-15%\")\n",
        "\n",
        "# Key Performance Indicators (KPIs)\n",
        "print(f\"\\nüìä RECOMMENDED KPIs FOR MONITORING:\")\n",
        "print(f\"   ‚Ä¢ Customer Lifetime Value (CLV) by segment\")\n",
        "print(f\"   ‚Ä¢ Churn rate by customer persona\")\n",
        "print(f\"   ‚Ä¢ Average days between orders\")\n",
        "print(f\"   ‚Ä¢ Cross-category purchase rate\")\n",
        "print(f\"   ‚Ä¢ Customer satisfaction score by segment\")\n",
        "print(f\"   ‚Ä¢ Revenue per customer segment\")\n",
        "print(f\"   ‚Ä¢ Customer migration between segments\")\n",
        "\n",
        "# Implementation timeline\n",
        "print(f\"\\nüìÖ IMPLEMENTATION TIMELINE:\")\n",
        "print(f\"   ‚Ä¢ Week 1-2: Set up segment-based analytics tracking\")\n",
        "print(f\"   ‚Ä¢ Week 3-4: Launch VIP customer program\")\n",
        "print(f\"   ‚Ä¢ Week 5-6: Implement at-risk customer campaigns\")\n",
        "print(f\"   ‚Ä¢ Week 7-8: Deploy new customer onboarding sequence\")\n",
        "print(f\"   ‚Ä¢ Week 9-12: Monitor results and optimize campaigns\")\n",
        "\n",
        "print(f\"\\n‚úÖ Customer Behavior Analysis Complete!\")\n",
        "print(f\"   Ready for advanced product performance and time series analysis.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary_part1"
      },
      "source": [
        "## Summary - Customer Behavior Analysis and Segmentation\n",
        "\n",
        "### What We've Accomplished\n",
        "\n",
        "1. **‚úÖ Comprehensive Customer Data Analysis**: Loaded and analyzed 25,000+ customer transaction records\n",
        "2. **‚úÖ RFM Analysis Implementation**: Systematic customer value segmentation using Recency, Frequency, Monetary metrics\n",
        "3. **‚úÖ Customer Lifecycle Analysis**: Understanding customer journey stages and retention patterns\n",
        "4. **‚úÖ Advanced ML Segmentation**: K-means clustering with feature engineering for sophisticated customer groups\n",
        "5. **‚úÖ Customer Personas Creation**: Detailed business profiles with actionable insights\n",
        "6. **‚úÖ Strategic Business Recommendations**: Priority-based action plan for customer retention and growth\n",
        "\n",
        "### Key Business Insights Discovered\n",
        "\n",
        "**Customer Value Distribution:**\n",
        "- Clear identification of high-value customer segments\n",
        "- Revenue concentration analysis for strategic focus\n",
        "- Customer lifetime value patterns\n",
        "\n",
        "**Behavioral Patterns:**\n",
        "- Purchase timing and frequency preferences\n",
        "- Category exploration and loyalty indicators\n",
        "- Satisfaction correlation with customer behavior\n",
        "\n",
        "**Risk Assessment:**\n",
        "- At-risk customer identification for retention campaigns\n",
        "- Churn pattern recognition\n",
        "- Customer lifecycle stage analysis\n",
        "\n",
        "### Advanced Techniques Mastered\n",
        "\n",
        "- **RFM Scoring**: Quintile-based customer value assessment\n",
        "- **Machine Learning Segmentation**: K-means clustering with feature engineering\n",
        "- **Customer Journey Mapping**: Lifecycle stage identification\n",
        "- **Behavioral Feature Engineering**: Creating predictive customer metrics\n",
        "- **Persona Development**: Translating data into actionable business profiles\n",
        "\n",
        "### Next Steps\n",
        "In Part 2, we'll explore:\n",
        "- Product performance metrics and category analysis\n",
        "- Cross-selling and upselling opportunity identification\n",
        "- Product lifecycle and performance optimization strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercises_part1"
      },
      "source": [
        "## üéØ Practice Exercises - Customer Behavior Analysis\n",
        "\n",
        "Strengthen your customer analytics skills:\n",
        "\n",
        "1. **Custom RFM Analysis**: Modify the RFM scoring system to use different quantiles or weighted scores\n",
        "\n",
        "2. **Geographic Segmentation**: Analyze customer behavior differences by state/region\n",
        "\n",
        "3. **Seasonal Behavior Analysis**: Identify customer segments with different seasonal purchasing patterns\n",
        "\n",
        "4. **Churn Prediction Model**: Create a simple model to predict customer churn risk\n",
        "\n",
        "5. **Customer Journey Optimization**: Propose improvements to customer lifecycle progression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise_space_part1"
      },
      "outputs": [],
      "source": [
        "# Exercise Space - Customer Behavior Analysis\n",
        "# Use this space to practice the customer analytics techniques\n",
        "\n",
        "# Exercise 1: Custom RFM Analysis\n",
        "# Modify the RFM scoring system\n",
        "\n",
        "# Exercise 2: Geographic Segmentation\n",
        "# Analyze behavior differences by location\n",
        "\n",
        "# Exercise 3: Seasonal Behavior Analysis\n",
        "# Identify seasonal purchasing patterns\n",
        "\n",
        "# Exercise 4: Churn Prediction Model\n",
        "# Create a simple churn risk model\n",
        "\n",
        "# Exercise 5: Customer Journey Optimization\n",
        "# Propose lifecycle improvements"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}