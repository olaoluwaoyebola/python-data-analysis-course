{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "regression_fundamentals_header"
   },
   "source": [
    "# Linear Regression Part 1: Fundamentals\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this session, you will be able to:\n",
    "- Understand linear regression theory and assumptions\n",
    "- Implement simple and multiple linear regression in Python\n",
    "- Interpret regression coefficients in business context\n",
    "- Validate regression assumptions and diagnose problems\n",
    "- Build predictive models for e-commerce business metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "environment_setup"
   },
   "source": [
    "## Environment Setup\n",
    "\n",
    "We'll set up our environment with regression-specific libraries and database connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "imports_setup"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete for regression analysis!\n"
     ]
    }
   ],
   "source": [
    "# Standard data analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical and machine learning libraries\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import linear_harvey_collier, het_breuschpagan\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Database connectivity\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Visualization styling\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Environment setup complete for regression analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "database_connection"
   },
   "source": [
    "## Database Connection Setup\n",
    "\n",
    "Establishing connection to our Supabase PostgreSQL database with the Olist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "db_config"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database connection successful!\n",
      "Test result: 1\n",
      "üîí Security Note: Database credentials loaded from .env file\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Database configuration - reading from environment variables\n",
    "DATABASE_CONFIG = {\n",
    "    'host': os.getenv('POSTGRES_HOST'),\n",
    "    'port': os.getenv('POSTGRES_PORT', '6543'),\n",
    "    'database': os.getenv('POSTGRES_DATABASE', 'postgres'),\n",
    "    'user': os.getenv('POSTGRES_USER'),\n",
    "    'password': os.getenv('POSTGRES_PASSWORD')\n",
    "}\n",
    "\n",
    "def create_database_connection():\n",
    "    \"\"\"\n",
    "    Create a SQLAlchemy engine for database connections.\n",
    "    \n",
    "    Returns:\n",
    "        sqlalchemy.engine.Engine: Database engine for executing queries\n",
    "    \"\"\"\n",
    "    # Check if all required credentials are available\n",
    "    required_fields = ['host', 'user', 'password']\n",
    "    missing_fields = [field for field in required_fields if not DATABASE_CONFIG[field]]\n",
    "    \n",
    "    if missing_fields:\n",
    "        raise ValueError(f\"Missing database credentials: {missing_fields}\")\n",
    "    \n",
    "    connection_string = f\"postgresql://{DATABASE_CONFIG['user']}:{DATABASE_CONFIG['password']}@{DATABASE_CONFIG['host']}:{DATABASE_CONFIG['port']}/{DATABASE_CONFIG['database']}\"\n",
    "    engine = create_engine(connection_string, pool_size=5, max_overflow=10)\n",
    "    return engine\n",
    "\n",
    "# Test database connection\n",
    "try:\n",
    "    engine = create_database_connection()\n",
    "    \n",
    "    # Use proper SQLAlchemy syntax for newer versions\n",
    "    from sqlalchemy import text\n",
    "    test_query = text(\"SELECT 1 as test\")\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        test_result = conn.execute(test_query)\n",
    "        result_value = test_result.scalar()\n",
    "        print(\"‚úÖ Database connection successful!\")\n",
    "        print(f\"Test result: {result_value}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Database connection failed: {str(e)}\")\n",
    "    print(\"Please check your .env file and database credentials.\")\n",
    "    \n",
    "    # Debug information\n",
    "    print(\"\\nDebug information:\")\n",
    "    print(f\"Host: {DATABASE_CONFIG['host']}\")\n",
    "    print(f\"Port: {DATABASE_CONFIG['port']}\")\n",
    "    print(f\"Database: {DATABASE_CONFIG['database']}\")\n",
    "    print(f\"User: {DATABASE_CONFIG['user']}\")\n",
    "    print(f\"Password: {'*' * len(DATABASE_CONFIG['password']) if DATABASE_CONFIG['password'] else 'None'}\")\n",
    "\n",
    "print(\"üîí Security Note: Database credentials loaded from .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "regression_theory"
   },
   "source": [
    "# Linear Regression Theory and Fundamentals\n",
    "\n",
    "## What is Linear Regression?\n",
    "\n",
    "Linear regression is a statistical method that models the relationship between a dependent variable (target) and one or more independent variables (features) using a linear equation.\n",
    "\n",
    "### Simple Linear Regression\n",
    "**Formula**: y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œµ\n",
    "\n",
    "Where:\n",
    "- y = dependent variable (what we're predicting)\n",
    "- x = independent variable (predictor)\n",
    "- Œ≤‚ÇÄ = intercept (y-value when x = 0)\n",
    "- Œ≤‚ÇÅ = slope (change in y for unit change in x)\n",
    "- Œµ = error term\n",
    "\n",
    "### Multiple Linear Regression\n",
    "**Formula**: y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô + Œµ\n",
    "\n",
    "## Key Assumptions\n",
    "1. **Linearity**: Relationship between variables is linear\n",
    "2. **Independence**: Observations are independent\n",
    "3. **Homoscedasticity**: Constant variance of residuals\n",
    "4. **Normality**: Residuals are normally distributed\n",
    "5. **No multicollinearity**: Independent variables aren't highly correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_preparation"
   },
   "source": [
    "# Data Preparation for Regression Analysis\n",
    "\n",
    "## Loading E-commerce Data for Regression\n",
    "\n",
    "We'll create a comprehensive dataset for predicting customer order values based on various business factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "regression_data_loading"
   },
   "outputs": [],
   "source": [
    "def load_regression_dataset():\n",
    "    \"\"\"\n",
    "    Load and prepare data for regression analysis.\n",
    "    Target: Predict order value based on customer and order characteristics.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Prepared regression dataset\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    WITH order_features AS (\n",
    "        SELECT \n",
    "            o.order_id,\n",
    "            o.customer_id,\n",
    "            c.customer_state,\n",
    "            \n",
    "            -- Target variable: total order value\n",
    "            SUM(oi.price + oi.freight_value) as order_value,\n",
    "            \n",
    "            -- Features: order characteristics\n",
    "            COUNT(oi.order_item_id) as item_count,\n",
    "            AVG(oi.price) as avg_item_price,\n",
    "            SUM(oi.freight_value) as total_freight,\n",
    "            \n",
    "            -- Features: product characteristics\n",
    "            COUNT(DISTINCT p.product_category_name_english) as category_count,\n",
    "            AVG(p.product_weight_g) as avg_product_weight,\n",
    "            AVG(p.product_length_cm * p.product_height_cm * p.product_width_cm) as avg_product_volume,\n",
    "            \n",
    "            -- Features: temporal\n",
    "            EXTRACT(MONTH FROM o.order_purchase_timestamp) as purchase_month,\n",
    "            EXTRACT(DOW FROM o.order_purchase_timestamp) as purchase_day_of_week,\n",
    "            EXTRACT(HOUR FROM o.order_purchase_timestamp) as purchase_hour,\n",
    "            \n",
    "            -- Features: delivery\n",
    "            EXTRACT(DAYS FROM (o.order_delivered_customer_date - o.order_purchase_timestamp)) as delivery_days,\n",
    "            \n",
    "            -- Features: customer satisfaction\n",
    "            r.review_score\n",
    "            \n",
    "        FROM \"olist_sales_data_set\".\"olist_orders_dataset\" o\n",
    "        INNER JOIN \"olist_sales_data_set\".\"olist_customers_dataset\" c \n",
    "            ON o.customer_id = c.customer_id\n",
    "        INNER JOIN \"olist_sales_data_set\".\"olist_order_items_dataset\" oi \n",
    "            ON o.order_id = oi.order_id\n",
    "        INNER JOIN \"olist_sales_data_set\".\"olist_products_dataset\" p \n",
    "            ON oi.product_id = p.product_id\n",
    "        LEFT JOIN \"olist_sales_data_set\".\"olist_order_reviews_dataset\" r \n",
    "            ON o.order_id = r.order_id\n",
    "        WHERE o.order_status = 'delivered'\n",
    "            AND o.order_delivered_customer_date IS NOT NULL\n",
    "            AND p.product_weight_g IS NOT NULL\n",
    "            AND p.product_length_cm IS NOT NULL\n",
    "            AND p.product_height_cm IS NOT NULL\n",
    "            AND p.product_width_cm IS NOT NULL\n",
    "        GROUP BY \n",
    "            o.order_id, o.customer_id, c.customer_state,\n",
    "            o.order_purchase_timestamp, o.order_delivered_customer_date,\n",
    "            r.review_score\n",
    "        HAVING SUM(oi.price + oi.freight_value) > 0\n",
    "            AND COUNT(oi.order_item_id) <= 20  -- Remove extreme outliers\n",
    "    ),\n",
    "    regional_mapping AS (\n",
    "        SELECT \n",
    "            *,\n",
    "            CASE \n",
    "                WHEN customer_state IN ('SP', 'RJ', 'ES', 'MG') THEN 'Southeast'\n",
    "                WHEN customer_state IN ('PR', 'SC', 'RS') THEN 'South'\n",
    "                WHEN customer_state IN ('GO', 'MT', 'MS', 'DF') THEN 'Center-West'\n",
    "                WHEN customer_state IN ('BA', 'SE', 'PE', 'AL', 'PB', 'RN', 'CE', 'PI', 'MA') THEN 'Northeast'\n",
    "                WHEN customer_state IN ('AM', 'RR', 'AP', 'PA', 'TO', 'RO', 'AC') THEN 'North'\n",
    "                ELSE 'Other'\n",
    "            END as region\n",
    "        FROM order_features\n",
    "    )\n",
    "    SELECT *\n",
    "    FROM regional_mapping\n",
    "    WHERE region != 'Other'\n",
    "        AND delivery_days BETWEEN 0 AND 100  -- Remove unrealistic delivery times\n",
    "        AND order_value < 2000  -- Remove extreme outliers for stable regression\n",
    "    ORDER BY RANDOM()\n",
    "    LIMIT 5000  -- Sample for demonstration\n",
    "    \"\"\"\n",
    "    \n",
    "    return pd.read_sql(query, engine)\n",
    "\n",
    "# Load regression dataset\n",
    "regression_df = load_regression_dataset()\n",
    "\n",
    "print(f\"Loaded {len(regression_df):,} orders for regression analysis\")\n",
    "print(f\"\\nDataset shape: {regression_df.shape}\")\n",
    "print(f\"\\nTarget variable (order_value) statistics:\")\n",
    "print(regression_df['order_value'].describe())\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = regression_df.isnull().sum()\n",
    "print(f\"\\nMissing values:\")\n",
    "print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exploratory_analysis"
   },
   "source": [
    "## Exploratory Data Analysis for Regression\n",
    "\n",
    "Before building regression models, we need to understand our data and relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eda_regression"
   },
   "outputs": [],
   "source": [
    "def perform_regression_eda(data):\n",
    "    \"\"\"\n",
    "    Perform exploratory data analysis for regression modeling.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Regression dataset\n",
    "    \"\"\"\n",
    "    # 1. Target variable distribution\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Original distribution\n",
    "    axes[0, 0].hist(data['order_value'], bins=50, alpha=0.7, color='skyblue')\n",
    "    axes[0, 0].set_title('Order Value Distribution')\n",
    "    axes[0, 0].set_xlabel('Order Value (R$)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Log transformation\n",
    "    log_order_value = np.log(data['order_value'])\n",
    "    axes[0, 1].hist(log_order_value, bins=50, alpha=0.7, color='lightgreen')\n",
    "    axes[0, 1].set_title('Log(Order Value) Distribution')\n",
    "    axes[0, 1].set_xlabel('Log(Order Value)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Q-Q plot for normality check\n",
    "    stats.probplot(data['order_value'], dist=\"norm\", plot=axes[1, 0])\n",
    "    axes[1, 0].set_title('Q-Q Plot: Order Value vs Normal Distribution')\n",
    "    \n",
    "    # Q-Q plot for log-transformed\n",
    "    stats.probplot(log_order_value, dist=\"norm\", plot=axes[1, 1])\n",
    "    axes[1, 1].set_title('Q-Q Plot: Log(Order Value) vs Normal Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Correlation analysis\n",
    "    print(\"\\nCorrelation Analysis:\")\n",
    "    print(\"=\" * 25)\n",
    "    \n",
    "    # Select numeric columns for correlation\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    correlation_matrix = data[numeric_cols].corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Correlations with target variable\n",
    "    target_correlations = correlation_matrix['order_value'].abs().sort_values(ascending=False)\n",
    "    print(\"\\nStrongest correlations with order value:\")\n",
    "    for feature, corr in target_correlations.head(10).items():\n",
    "        if feature != 'order_value':\n",
    "            print(f\"  {feature}: {corr:.3f}\")\n",
    "    \n",
    "    # 3. Feature relationships\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"KEY FEATURE RELATIONSHIPS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Top predictors for scatter plots\n",
    "    top_features = target_correlations.head(6).index.tolist()\n",
    "    top_features.remove('order_value')  # Remove target variable\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, feature in enumerate(top_features[:6]):\n",
    "        axes[i].scatter(data[feature], data['order_value'], alpha=0.5, s=20)\n",
    "        axes[i].set_xlabel(feature.replace('_', ' ').title())\n",
    "        axes[i].set_ylabel('Order Value (R$)')\n",
    "        axes[i].set_title(f'{feature.replace(\"_\", \" \").title()} vs Order Value')\n",
    "        \n",
    "        # Add trend line\n",
    "        z = np.polyfit(data[feature].dropna(), data.loc[data[feature].notna(), 'order_value'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        axes[i].plot(data[feature], p(data[feature]), \"r--\", alpha=0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return target_correlations\n",
    "\n",
    "# Perform EDA\n",
    "correlations = perform_regression_eda(regression_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "simple_linear_regression"
   },
   "source": [
    "# Simple Linear Regression\n",
    "\n",
    "## Building Our First Regression Model\n",
    "\n",
    "Let's start with a simple linear regression using the strongest predictor of order value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "simple_regression_implementation"
   },
   "outputs": [],
   "source": [
    "def build_simple_linear_regression(data, feature_col, target_col='order_value'):\n",
    "    \"\"\"\n",
    "    Build and analyze a simple linear regression model.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Dataset\n",
    "        feature_col (str): Feature column name\n",
    "        target_col (str): Target column name\n",
    "    \n",
    "    Returns:\n",
    "        dict: Regression results and diagnostics\n",
    "    \"\"\"\n",
    "    # Prepare data (remove missing values)\n",
    "    clean_data = data[[feature_col, target_col]].dropna()\n",
    "    X = clean_data[feature_col]\n",
    "    y = clean_data[target_col]\n",
    "    \n",
    "    print(f\"Simple Linear Regression: {target_col} ~ {feature_col}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Method 1: Using statsmodels (for detailed statistics)\n",
    "    X_sm = sm.add_constant(X)  # Add intercept\n",
    "    model_sm = sm.OLS(y, X_sm).fit()\n",
    "    \n",
    "    print(\"\\nStatsmodels Summary:\")\n",
    "    print(model_sm.summary())\n",
    "    \n",
    "    # Method 2: Using scikit-learn (for prediction focus)\n",
    "    X_sklearn = X.values.reshape(-1, 1)\n",
    "    model_sklearn = LinearRegression()\n",
    "    model_sklearn.fit(X_sklearn, y)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_sm = model_sm.predict(X_sm)\n",
    "    y_pred_sklearn = model_sklearn.predict(X_sklearn)\n",
    "    \n",
    "    # Model performance metrics\n",
    "    r2_sm = model_sm.rsquared\n",
    "    r2_sklearn = r2_score(y, y_pred_sklearn)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred_sklearn))\n",
    "    mae = mean_absolute_error(y, y_pred_sklearn)\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(\"MODEL PERFORMANCE METRICS:\")\n",
    "    print(f\"{'='*40}\")\n",
    "    print(f\"R-squared: {r2_sm:.4f}\")\n",
    "    print(f\"Root Mean Square Error (RMSE): R$ {rmse:.2f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): R$ {mae:.2f}\")\n",
    "    \n",
    "    # Business interpretation\n",
    "    intercept = model_sklearn.intercept_\n",
    "    slope = model_sklearn.coef_[0]\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(\"BUSINESS INTERPRETATION:\")\n",
    "    print(f\"{'='*40}\")\n",
    "    print(f\"Intercept (Œ≤‚ÇÄ): R$ {intercept:.2f}\")\n",
    "    print(f\"Slope (Œ≤‚ÇÅ): R$ {slope:.2f}\")\n",
    "    print(f\"\\nInterpretation:\")\n",
    "    print(f\"‚Ä¢ Base order value: R$ {intercept:.2f}\")\n",
    "    print(f\"‚Ä¢ For each unit increase in {feature_col}, order value increases by R$ {slope:.2f}\")\n",
    "    print(f\"‚Ä¢ The model explains {r2_sm*100:.1f}% of the variance in order value\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Scatter plot with regression line\n",
    "    axes[0, 0].scatter(X, y, alpha=0.5, s=20, label='Data points')\n",
    "    axes[0, 0].plot(X, y_pred_sklearn, color='red', linewidth=2, label='Regression line')\n",
    "    axes[0, 0].set_xlabel(feature_col.replace('_', ' ').title())\n",
    "    axes[0, 0].set_ylabel('Order Value (R$)')\n",
    "    axes[0, 0].set_title(f'Linear Regression: {feature_col} vs Order Value')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Residuals plot\n",
    "    residuals = y - y_pred_sklearn\n",
    "    axes[0, 1].scatter(y_pred_sklearn, residuals, alpha=0.5, s=20)\n",
    "    axes[0, 1].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[0, 1].set_xlabel('Predicted Values')\n",
    "    axes[0, 1].set_ylabel('Residuals')\n",
    "    axes[0, 1].set_title('Residuals vs Predicted Values')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Q-Q plot of residuals\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[1, 0])\n",
    "    axes[1, 0].set_title('Q-Q Plot: Residuals vs Normal Distribution')\n",
    "    \n",
    "    # 4. Histogram of residuals\n",
    "    axes[1, 1].hist(residuals, bins=30, alpha=0.7, color='skyblue')\n",
    "    axes[1, 1].set_xlabel('Residuals')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_title('Distribution of Residuals')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'model_sm': model_sm,\n",
    "        'model_sklearn': model_sklearn,\n",
    "        'r2': r2_sm,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'residuals': residuals,\n",
    "        'predictions': y_pred_sklearn\n",
    "    }\n",
    "\n",
    "# Build simple linear regression with strongest predictor\n",
    "# Get the strongest predictor (excluding order_value itself)\n",
    "strongest_predictor = correlations.drop('order_value').index[0]\n",
    "print(f\"Building simple linear regression with strongest predictor: {strongest_predictor}\\n\")\n",
    "\n",
    "simple_regression_results = build_simple_linear_regression(regression_df, strongest_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "regression_assumptions"
   },
   "source": [
    "## Regression Assumptions Testing\n",
    "\n",
    "Let's formally test the key assumptions of linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "assumptions_testing"
   },
   "outputs": [],
   "source": [
    "def test_regression_assumptions(model_sm, residuals, features_data, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Test the key assumptions of linear regression.\n",
    "    \n",
    "    Args:\n",
    "        model_sm (statsmodels regression): Fitted statsmodels regression\n",
    "        residuals (array): Model residuals\n",
    "        features_data (pd.DataFrame): Feature data\n",
    "        alpha (float): Significance level\n",
    "    \n",
    "    Returns:\n",
    "        dict: Test results for each assumption\n",
    "    \"\"\"\n",
    "    print(\"REGRESSION ASSUMPTIONS TESTING\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Linearity Test (Harvey-Collier test)\n",
    "    print(\"\\n1. LINEARITY TEST (Harvey-Collier)\")\n",
    "    print(\"-\" * 35)\n",
    "    try:\n",
    "        hc_statistic, hc_p_value = linear_harvey_collier(model_sm)\n",
    "        results['linearity'] = {\n",
    "            'statistic': hc_statistic,\n",
    "            'p_value': hc_p_value,\n",
    "            'assumption_met': hc_p_value > alpha\n",
    "        }\n",
    "        \n",
    "        print(f\"Harvey-Collier statistic: {hc_statistic:.4f}\")\n",
    "        print(f\"P-value: {hc_p_value:.6f}\")\n",
    "        \n",
    "        if hc_p_value > alpha:\n",
    "            print(f\"‚úÖ Linearity assumption MET (p > {alpha})\")\n",
    "        else:\n",
    "            print(f\"‚ùå Linearity assumption VIOLATED (p <= {alpha})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Could not perform linearity test: {e}\")\n",
    "        results['linearity'] = {'error': str(e)}\n",
    "    \n",
    "    # 2. Homoscedasticity Test (Breusch-Pagan test)\n",
    "    print(\"\\n2. HOMOSCEDASTICITY TEST (Breusch-Pagan)\")\n",
    "    print(\"-\" * 42)\n",
    "    try:\n",
    "        bp_statistic, bp_p_value, _, _ = het_breuschpagan(residuals, model_sm.model.exog)\n",
    "        results['homoscedasticity'] = {\n",
    "            'statistic': bp_statistic,\n",
    "            'p_value': bp_p_value,\n",
    "            'assumption_met': bp_p_value > alpha\n",
    "        }\n",
    "        \n",
    "        print(f\"Breusch-Pagan statistic: {bp_statistic:.4f}\")\n",
    "        print(f\"P-value: {bp_p_value:.6f}\")\n",
    "        \n",
    "        if bp_p_value > alpha:\n",
    "            print(f\"‚úÖ Homoscedasticity assumption MET (p > {alpha})\")\n",
    "        else:\n",
    "            print(f\"‚ùå Homoscedasticity assumption VIOLATED (p <= {alpha})\")\n",
    "            print(\"  ‚Üí Consider using robust standard errors or transforming variables\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Could not perform homoscedasticity test: {e}\")\n",
    "        results['homoscedasticity'] = {'error': str(e)}\n",
    "    \n",
    "    # 3. Normality of Residuals Test (Shapiro-Wilk)\n",
    "    print(\"\\n3. NORMALITY OF RESIDUALS TEST (Shapiro-Wilk)\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    # Use sample for large datasets (Shapiro-Wilk limited to n <= 5000)\n",
    "    residuals_sample = residuals[:min(5000, len(residuals))]\n",
    "    \n",
    "    sw_statistic, sw_p_value = stats.shapiro(residuals_sample)\n",
    "    results['normality'] = {\n",
    "        'statistic': sw_statistic,\n",
    "        'p_value': sw_p_value,\n",
    "        'assumption_met': sw_p_value > alpha\n",
    "    }\n",
    "    \n",
    "    print(f\"Shapiro-Wilk statistic: {sw_statistic:.4f}\")\n",
    "    print(f\"P-value: {sw_p_value:.6f}\")\n",
    "    print(f\"Sample size used: {len(residuals_sample)}\")\n",
    "    \n",
    "    if sw_p_value > alpha:\n",
    "        print(f\"‚úÖ Normality assumption MET (p > {alpha})\")\n",
    "    else:\n",
    "        print(f\"‚ùå Normality assumption VIOLATED (p <= {alpha})\")\n",
    "        print(\"  ‚Üí Consider transforming the target variable or using robust methods\")\n",
    "    \n",
    "    # 4. Independence (Durbin-Watson test)\n",
    "    print(\"\\n4. INDEPENDENCE TEST (Durbin-Watson)\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    from statsmodels.stats.diagnostic import durbin_watson\n",
    "    dw_statistic = durbin_watson(residuals)\n",
    "    results['independence'] = {\n",
    "        'statistic': dw_statistic,\n",
    "        'assumption_met': 1.5 <= dw_statistic <= 2.5  # Rule of thumb\n",
    "    }\n",
    "    \n",
    "    print(f\"Durbin-Watson statistic: {dw_statistic:.4f}\")\n",
    "    \n",
    "    if 1.5 <= dw_statistic <= 2.5:\n",
    "        print(\"‚úÖ Independence assumption likely MET (1.5 ‚â§ DW ‚â§ 2.5)\")\n",
    "    elif dw_statistic < 1.5:\n",
    "        print(\"‚ùå Positive autocorrelation detected (DW < 1.5)\")\n",
    "    else:\n",
    "        print(\"‚ùå Negative autocorrelation detected (DW > 2.5)\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ASSUMPTIONS SUMMARY:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    assumptions_met = 0\n",
    "    total_assumptions = 0\n",
    "    \n",
    "    for assumption, test_result in results.items():\n",
    "        if 'assumption_met' in test_result:\n",
    "            total_assumptions += 1\n",
    "            if test_result['assumption_met']:\n",
    "                assumptions_met += 1\n",
    "                print(f\"‚úÖ {assumption.title()}: PASSED\")\n",
    "            else:\n",
    "                print(f\"‚ùå {assumption.title()}: FAILED\")\n",
    "    \n",
    "    print(f\"\\nOverall: {assumptions_met}/{total_assumptions} assumptions met\")\n",
    "    \n",
    "    if assumptions_met == total_assumptions:\n",
    "        print(\"üéâ All assumptions satisfied - model is reliable!\")\n",
    "    elif assumptions_met >= total_assumptions * 0.75:\n",
    "        print(\"‚ö†Ô∏è  Most assumptions satisfied - model is reasonably reliable\")\n",
    "    else:\n",
    "        print(\"üö® Multiple assumptions violated - consider model improvements\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test assumptions for our simple regression\n",
    "assumption_results = test_regression_assumptions(\n",
    "    simple_regression_results['model_sm'],\n",
    "    simple_regression_results['residuals'],\n",
    "    regression_df[[strongest_predictor]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "multiple_linear_regression"
   },
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "## Building a Comprehensive Predictive Model\n",
    "\n",
    "Now let's build a multiple linear regression model using several predictors to better explain order value variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "multiple_regression_implementation"
   },
   "outputs": [],
   "source": [
    "def build_multiple_linear_regression(data, target_col='order_value', test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Build and evaluate a multiple linear regression model.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Dataset\n",
    "        target_col (str): Target variable column\n",
    "        test_size (float): Proportion of data for testing\n",
    "        random_state (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        dict: Model results and evaluation metrics\n",
    "    \"\"\"\n",
    "    print(\"MULTIPLE LINEAR REGRESSION MODEL\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Select features for the model\n",
    "    # Use quantitative features that make business sense\n",
    "    feature_columns = [\n",
    "        'item_count',\n",
    "        'avg_item_price', \n",
    "        'total_freight',\n",
    "        'category_count',\n",
    "        'avg_product_weight',\n",
    "        'avg_product_volume',\n",
    "        'delivery_days',\n",
    "        'purchase_month',\n",
    "        'purchase_hour'\n",
    "    ]\n",
    "    \n",
    "    # Handle missing values and prepare data\n",
    "    model_data = data[feature_columns + [target_col]].dropna()\n",
    "    \n",
    "    print(f\"Features selected: {len(feature_columns)}\")\n",
    "    print(f\"Sample size after cleaning: {len(model_data):,}\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = model_data[feature_columns]\n",
    "    y = model_data[target_col]\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set size: {len(X_train):,}\")\n",
    "    print(f\"Test set size: {len(X_test):,}\")\n",
    "    \n",
    "    # Build models using both statsmodels and scikit-learn\n",
    "    \n",
    "    # 1. Statsmodels for detailed statistics\n",
    "    X_train_sm = sm.add_constant(X_train)\n",
    "    X_test_sm = sm.add_constant(X_test)\n",
    "    \n",
    "    model_sm = sm.OLS(y_train, X_train_sm).fit()\n",
    "    \n",
    "    print(\"\\nStatsmodels Regression Summary:\")\n",
    "    print(\"=\" * 35)\n",
    "    print(model_sm.summary())\n",
    "    \n",
    "    # 2. Scikit-learn for predictions\n",
    "    model_sklearn = LinearRegression()\n",
    "    model_sklearn.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model_sklearn.predict(X_train)\n",
    "    y_test_pred = model_sklearn.predict(X_test)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    \n",
    "    # Print performance metrics\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"MODEL PERFORMANCE COMPARISON:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"{'Metric':<20} {'Training':<12} {'Testing':<12} {'Difference':<12}\")\n",
    "    print(\"-\" * 56)\n",
    "    print(f\"{'R-squared':<20} {train_r2:<12.4f} {test_r2:<12.4f} {abs(train_r2-test_r2):<12.4f}\")\n",
    "    print(f\"{'RMSE (R$)':<20} {train_rmse:<12.2f} {test_rmse:<12.2f} {abs(train_rmse-test_rmse):<12.2f}\")\n",
    "    print(f\"{'MAE (R$)':<20} {train_mae:<12.2f} {test_mae:<12.2f} {abs(train_mae-test_mae):<12.2f}\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    r2_diff = train_r2 - test_r2\n",
    "    if r2_diff < 0.05:\n",
    "        print(\"\\n‚úÖ Model shows good generalization (low overfitting)\")\n",
    "    elif r2_diff < 0.10:\n",
    "        print(\"\\n‚ö†Ô∏è  Model shows moderate overfitting\")\n",
    "    else:\n",
    "        print(\"\\nüö® Model shows significant overfitting\")\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(\"FEATURE IMPORTANCE ANALYSIS:\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    # Get coefficients and their significance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_columns,\n",
    "        'Coefficient': model_sklearn.coef_,\n",
    "        'Abs_Coefficient': np.abs(model_sklearn.coef_),\n",
    "        'P_Value': model_sm.pvalues[1:],  # Exclude intercept\n",
    "        'Significant': model_sm.pvalues[1:] < 0.05\n",
    "    }).sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    print(\"Feature Coefficients (sorted by magnitude):\")\n",
    "    print(\"-\" * 60)\n",
    "    for _, row in feature_importance.iterrows():\n",
    "        significance = \"***\" if row['P_Value'] < 0.001 else \"**\" if row['P_Value'] < 0.01 else \"*\" if row['P_Value'] < 0.05 else \"\"\n",
    "        print(f\"{row['Feature']:<20} {row['Coefficient']:<10.3f} {row['P_Value']:<10.6f} {significance}\")\n",
    "    \n",
    "    print(\"\\nSignificance levels: *** p<0.001, ** p<0.01, * p<0.05\")\n",
    "    \n",
    "    # Business interpretation of top features\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"BUSINESS INTERPRETATION:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    top_features = feature_importance.head(5)\n",
    "    for _, row in top_features.iterrows():\n",
    "        if row['Significant']:\n",
    "            direction = \"increases\" if row['Coefficient'] > 0 else \"decreases\"\n",
    "            print(f\"‚Ä¢ {row['Feature'].replace('_', ' ').title()}: \"\n",
    "                  f\"Each unit increase {direction} order value by R$ {abs(row['Coefficient']):.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'model_sm': model_sm,\n",
    "        'model_sklearn': model_sklearn,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'y_train_pred': y_train_pred,\n",
    "        'y_test_pred': y_test_pred,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'feature_importance': feature_importance,\n",
    "        'feature_columns': feature_columns\n",
    "    }\n",
    "\n",
    "# Build multiple linear regression model\n",
    "multiple_regression_results = build_multiple_linear_regression(regression_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "multicollinearity_check"
   },
   "source": [
    "## Multicollinearity Assessment\n",
    "\n",
    "Let's check for multicollinearity among our predictors using Variance Inflation Factor (VIF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vif_analysis"
   },
   "outputs": [],
   "source": [
    "def check_multicollinearity(X, feature_names, vif_threshold=5.0):\n",
    "    \"\"\"\n",
    "    Check for multicollinearity using Variance Inflation Factor (VIF).\n",
    "    \n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature matrix\n",
    "        feature_names (list): List of feature names\n",
    "        vif_threshold (float): VIF threshold for concern\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: VIF scores for each feature\n",
    "    \"\"\"\n",
    "    print(\"MULTICOLLINEARITY ASSESSMENT (VIF Analysis)\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Calculate VIF for each feature\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = feature_names\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(feature_names))]\n",
    "    \n",
    "    # Sort by VIF score\n",
    "    vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "    \n",
    "    print(f\"VIF Scores (threshold: {vif_threshold}):\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    high_vif_count = 0\n",
    "    for _, row in vif_data.iterrows():\n",
    "        vif_score = row['VIF']\n",
    "        feature = row['Feature']\n",
    "        \n",
    "        if vif_score > vif_threshold:\n",
    "            status = \"üö® HIGH\"\n",
    "            high_vif_count += 1\n",
    "        elif vif_score > 2.5:\n",
    "            status = \"‚ö†Ô∏è  MODERATE\"\n",
    "        else:\n",
    "            status = \"‚úÖ LOW\"\n",
    "        \n",
    "        print(f\"{feature:<20} {vif_score:<8.2f} {status}\")\n",
    "    \n",
    "    # Summary and recommendations\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"MULTICOLLINEARITY SUMMARY:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    if high_vif_count == 0:\n",
    "        print(\"‚úÖ No significant multicollinearity detected\")\n",
    "        print(\"   All features have VIF < 5.0\")\n",
    "    else:\n",
    "        print(f\"üö® {high_vif_count} feature(s) show high multicollinearity (VIF > {vif_threshold})\")\n",
    "        print(\"\\nRecommendations:\")\n",
    "        print(\"‚Ä¢ Consider removing highly correlated features\")\n",
    "        print(\"‚Ä¢ Use dimensionality reduction techniques (PCA)\")\n",
    "        print(\"‚Ä¢ Apply ridge regression for regularization\")\n",
    "    \n",
    "    print(\"\\nVIF Interpretation Guide:\")\n",
    "    print(\"‚Ä¢ VIF = 1: No correlation with other features\")\n",
    "    print(\"‚Ä¢ 1 < VIF < 5: Moderate correlation\")\n",
    "    print(\"‚Ä¢ VIF ‚â• 5: High correlation (multicollinearity concern)\")\n",
    "    print(\"‚Ä¢ VIF ‚â• 10: Severe multicollinearity\")\n",
    "    \n",
    "    return vif_data\n",
    "\n",
    "# Check multicollinearity\n",
    "vif_results = check_multicollinearity(\n",
    "    multiple_regression_results['X_train'],\n",
    "    multiple_regression_results['feature_columns']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "session_summary"
   },
   "source": [
    "# Session Summary\n",
    "\n",
    "## What We Accomplished\n",
    "\n",
    "In this comprehensive regression fundamentals session, we covered:\n",
    "\n",
    "### 1. Linear Regression Theory\n",
    "- Understanding simple and multiple linear regression formulas\n",
    "- Key assumptions of linear regression\n",
    "- Business interpretation of coefficients\n",
    "\n",
    "### 2. Data Preparation\n",
    "- Created a comprehensive e-commerce dataset for regression\n",
    "- Performed exploratory data analysis specific to regression\n",
    "- Identified strong predictors through correlation analysis\n",
    "\n",
    "### 3. Simple Linear Regression\n",
    "- Built and interpreted single-variable regression models\n",
    "- Learned to use both statsmodels and scikit-learn\n",
    "- Visualized relationships and residuals\n",
    "\n",
    "### 4. Regression Diagnostics\n",
    "- Tested linearity assumptions (Harvey-Collier test)\n",
    "- Assessed homoscedasticity (Breusch-Pagan test)\n",
    "- Checked normality of residuals (Shapiro-Wilk test)\n",
    "- Evaluated independence (Durbin-Watson test)\n",
    "\n",
    "### 5. Multiple Linear Regression\n",
    "- Built comprehensive multi-variable predictive models\n",
    "- Implemented train-test splitting for model validation\n",
    "- Analyzed feature importance and statistical significance\n",
    "- Interpreted business implications of coefficients\n",
    "\n",
    "### 6. Multicollinearity Assessment\n",
    "- Used Variance Inflation Factor (VIF) analysis\n",
    "- Identified problematic feature correlations\n",
    "- Provided recommendations for multicollinearity issues\n",
    "\n",
    "## Key Business Skills Developed\n",
    "- Predictive modeling for e-commerce metrics\n",
    "- Statistical model validation and diagnostics\n",
    "- Feature selection and importance analysis\n",
    "- Business interpretation of statistical results\n",
    "- Model performance evaluation\n",
    "\n",
    "## Next Session Preview\n",
    "Tomorrow we'll continue with **Business Modeling Applications**, where we'll:\n",
    "- Apply regression to specific business problems\n",
    "- Build customer lifetime value prediction models\n",
    "- Implement price optimization using regression\n",
    "- Develop demand forecasting models"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
