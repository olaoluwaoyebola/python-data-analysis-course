{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 - SQL and Python Integration Part 1: Database Connections\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, you will be able to:\n",
    "1. Establish PostgreSQL database connections from Python using SQLAlchemy\n",
    "2. Connect to cloud databases (Supabase) for real-world data analysis\n",
    "3. Execute SQL queries from Python notebooks using real e-commerce data\n",
    "4. Understand the relationship between SQL databases and Python DataFrames\n",
    "5. Implement proper connection management and error handling\n",
    "6. Compare SQL and Pandas approaches for business analytics\n",
    "\n",
    "## Business Context: Bridging SQL and Python\n",
    "\n",
    "In modern business environments, data often lives in **cloud databases** while analysis happens in **Python**. The ability to seamlessly bridge these two worlds is essential for:\n",
    "\n",
    "- **Real-time Data Access** - Connect directly to live business systems\n",
    "- **Scalability** - Handle enterprise-scale datasets\n",
    "- **Collaboration** - Multiple analysts accessing the same data source\n",
    "- **Performance** - Leverage database engines for heavy computation\n",
    "- **Integration** - Combine SQL's querying power with Python's analytical capabilities\n",
    "\n",
    "Today we'll master connecting Python to **PostgreSQL databases** using **Supabase** (a cloud database platform) and work with real Olist e-commerce data that's already stored in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêò PostgreSQL-Python Integration Environment Ready!\n",
      "SQLAlchemy version: 2.0.41\n",
      "Pandas version: 2.3.0\n",
      "‚úÖ Connecting to Supabase PostgreSQL Database...\n",
      "üóÑÔ∏è Real Olist E-commerce & Marketing data awaits!\n",
      "üîí Database credentials loaded securely from .env file\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for PostgreSQL database connectivity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text, inspect\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Supabase PostgreSQL Database Configuration from environment variables\n",
    "DATABASE_CONFIG = {\n",
    "    'host': os.getenv('POSTGRES_HOST'),\n",
    "    'port': int(os.getenv('POSTGRES_PORT', 5432)),\n",
    "    'database': os.getenv('POSTGRES_DATABASE'),\n",
    "    'user': os.getenv('POSTGRES_USER'),\n",
    "    'password': os.getenv('POSTGRES_PASSWORD'),\n",
    "    'connection_timeout': 30,\n",
    "    'echo': False  # Set to True to see SQL queries\n",
    "}\n",
    "\n",
    "# Verify that environment variables were loaded\n",
    "if not all([DATABASE_CONFIG['host'], DATABASE_CONFIG['user'], DATABASE_CONFIG['password']]):\n",
    "    raise ValueError(\"Missing required database credentials. Please check your .env file.\")\n",
    "\n",
    "# PostgreSQL connection string\n",
    "POSTGRES_URL = f\"postgresql://{DATABASE_CONFIG['user']}:{DATABASE_CONFIG['password']}@{DATABASE_CONFIG['host']}:{DATABASE_CONFIG['port']}/{DATABASE_CONFIG['database']}\"\n",
    "\n",
    "print(\"üêò PostgreSQL-Python Integration Environment Ready!\")\n",
    "print(f\"SQLAlchemy version: {sqlalchemy.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(\"‚úÖ Connecting to Supabase PostgreSQL Database...\")\n",
    "print(\"üóÑÔ∏è Real Olist E-commerce & Marketing data awaits!\")\n",
    "print(\"üîí Database credentials loaded securely from .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SQLAlchemy Basics and PostgreSQL Connection\n",
    "\n",
    "**SQLAlchemy** is Python's most popular database toolkit. It provides:\n",
    "- **Connection Management**: Handle database connections efficiently\n",
    "- **SQL Query Execution**: Run SQL directly from Python\n",
    "- **ORM (Object-Relational Mapping)**: Map Python objects to database tables\n",
    "- **Database Abstraction**: Work with different databases using the same API\n",
    "\n",
    "**PostgreSQL** is an enterprise-grade database that excels at:\n",
    "- **Complex Queries**: Advanced SQL features like window functions, CTEs\n",
    "- **Scalability**: Handle millions of rows efficiently  \n",
    "- **Data Integrity**: ACID compliance for business-critical data\n",
    "- **JSON Support**: Store and query semi-structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Connecting to Supabase PostgreSQL Database...\n",
      "‚úÖ PostgreSQL connection established successfully\n",
      "üêò Database version: PostgreSQL 17.4 on aarch64-unknown-linux-gnu, comp...\n",
      "\n",
      "üìä Olist E-commerce Database Overview:\n",
      "üìã Discovering database schema...\n",
      "üîç Found schemas: ['olist_marketing_data_set', 'olist_sales_data_set']\n",
      "\n",
      "üìä Schema 'olist_marketing_data_set' contains 2 tables:\n",
      "  üìã olist_marketing_qualified_leads_dataset: 8,000 rows, 4 columns\n",
      "  üìã olist_closed_deals_dataset: 380 rows, 14 columns\n",
      "\n",
      "üìä Schema 'olist_sales_data_set' contains 9 tables:\n",
      "  üìã olist_order_reviews_dataset: 98,410 rows, 7 columns\n",
      "  üìã olist_order_items_dataset: 112,650 rows, 7 columns\n",
      "  üìã olist_order_payments_dataset: 103,886 rows, 5 columns\n",
      "  üìã olist_customers_dataset: 99,441 rows, 5 columns\n",
      "  üìã olist_orders_dataset: 99,441 rows, 8 columns\n",
      "  üìã olist_sellers_dataset: 3,095 rows, 4 columns\n",
      "  üìã product_category_name_translation: 73 rows, 2 columns\n",
      "  üìã olist_products_dataset: 32,951 rows, 9 columns\n",
      "  üìã olist_geolocation_dataset: 1,000,163 rows, 5 columns\n",
      "\n",
      "üóÉÔ∏è Total tables discovered: 11\n",
      "üìè Total rows across all tables: 1,558,490\n"
     ]
    }
   ],
   "source": [
    "# PostgreSQL Connection Functions (Functional Programming Approach)\n",
    "\n",
    "def create_database_engine():\n",
    "    \"\"\"\n",
    "    Create and configure PostgreSQL database engine with optimal settings.\n",
    "    Returns configured SQLAlchemy engine.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        engine = create_engine(\n",
    "            POSTGRES_URL,\n",
    "            echo=DATABASE_CONFIG['echo'],\n",
    "            pool_size=5,\n",
    "            max_overflow=10,\n",
    "            pool_timeout=DATABASE_CONFIG['connection_timeout'],\n",
    "            pool_recycle=3600,\n",
    "            connect_args={\n",
    "                \"connect_timeout\": DATABASE_CONFIG['connection_timeout'],\n",
    "                \"application_name\": \"Python_Data_Analysis_Course\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Test connection\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(text(\"SELECT version()\"))\n",
    "            version = result.scalar()\n",
    "            print(\"‚úÖ PostgreSQL connection established successfully\")\n",
    "            print(f\"üêò Database version: {version[:50]}...\")\n",
    "        \n",
    "        return engine\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå PostgreSQL connection failed: {e}\")\n",
    "        print(\"üîß Troubleshooting tips:\")\n",
    "        print(\"  ‚Ä¢ Check your internet connection\")\n",
    "        print(\"  ‚Ä¢ Verify database credentials\")\n",
    "        print(\"  ‚Ä¢ Ensure Supabase database is running\")\n",
    "        raise\n",
    "\n",
    "def get_table_info(engine):\n",
    "    \"\"\"\n",
    "    Get comprehensive information about all tables in the Olist schemas.\n",
    "    Returns dictionary with schema-qualified table information.\n",
    "    \"\"\"\n",
    "    inspector = inspect(engine)\n",
    "    \n",
    "    # Define the schemas we're interested in\n",
    "    target_schemas = ['olist_marketing_data_set', 'olist_sales_data_set']\n",
    "    \n",
    "    table_info = {}\n",
    "    \n",
    "    print(\"üìã Discovering database schema...\")\n",
    "    \n",
    "    # Get all available schemas first\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(text(\"\"\"\n",
    "                SELECT schema_name \n",
    "                FROM information_schema.schemata \n",
    "                WHERE schema_name IN ('olist_marketing_data_set', 'olist_sales_data_set')\n",
    "            \"\"\"))\n",
    "            available_schemas = [row[0] for row in result]\n",
    "            print(f\"üîç Found schemas: {available_schemas}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not query schemas: {e}\")\n",
    "        available_schemas = target_schemas  # Fallback to expected schemas\n",
    "    \n",
    "    # Query tables in each schema\n",
    "    for schema in available_schemas:\n",
    "        try:\n",
    "            tables = inspector.get_table_names(schema=schema)\n",
    "            print(f\"\\nüìä Schema '{schema}' contains {len(tables)} tables:\")\n",
    "            \n",
    "            for table in tables:\n",
    "                schema_qualified_name = f\"{schema}.{table}\"\n",
    "                try:\n",
    "                    with engine.connect() as conn:\n",
    "                        # Get row count\n",
    "                        result = conn.execute(text(f'SELECT COUNT(*) FROM \"{schema}\".\"{table}\"'))\n",
    "                        row_count = result.scalar()\n",
    "                        \n",
    "                        # Get column information\n",
    "                        columns = inspector.get_columns(table, schema=schema)\n",
    "                        \n",
    "                        table_info[schema_qualified_name] = {\n",
    "                            'schema': schema,\n",
    "                            'table': table,\n",
    "                            'rows': row_count,\n",
    "                            'columns': [col['name'] for col in columns],\n",
    "                            'column_types': {col['name']: str(col['type']) for col in columns}\n",
    "                        }\n",
    "                        \n",
    "                        print(f\"  üìã {table}: {row_count:,} rows, {len(columns)} columns\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ö†Ô∏è Could not access {schema}.{table}: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not access schema '{schema}': {e}\")\n",
    "            continue\n",
    "    \n",
    "    return table_info\n",
    "\n",
    "def execute_query(engine, query, params=None):\n",
    "    \"\"\"\n",
    "    Execute a SQL query with proper error handling and return results as DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            if params:\n",
    "                result = pd.read_sql(text(query), conn, params=params)\n",
    "            else:\n",
    "                result = pd.read_sql(text(query), conn)\n",
    "            \n",
    "            return result\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Query execution failed: {e}\")\n",
    "        print(f\"üìù Query: {query[:100]}...\")\n",
    "        raise\n",
    "\n",
    "def get_sample_data(engine, table_name, limit=5):\n",
    "    \"\"\"\n",
    "    Get sample data from a schema-qualified table for exploration.\n",
    "    table_name should be in format 'schema.table'\n",
    "    \"\"\"\n",
    "    if '.' in table_name:\n",
    "        schema, table = table_name.split('.', 1)\n",
    "        query = f'SELECT * FROM \"{schema}\".\"{table}\" LIMIT {limit}'\n",
    "    else:\n",
    "        query = f'SELECT * FROM \"{table_name}\" LIMIT {limit}'\n",
    "    \n",
    "    return execute_query(engine, query)\n",
    "\n",
    "def get_table_schema(engine, table_name):\n",
    "    \"\"\"\n",
    "    Get detailed schema information for a specific table.\n",
    "    table_name should be in format 'schema.table'\n",
    "    \"\"\"\n",
    "    if '.' in table_name:\n",
    "        schema, table = table_name.split('.', 1)\n",
    "    else:\n",
    "        schema, table = None, table_name\n",
    "        \n",
    "    inspector = inspect(engine)\n",
    "    columns = inspector.get_columns(table, schema=schema)\n",
    "    \n",
    "    schema_df = pd.DataFrame([\n",
    "        {\n",
    "            'column_name': col['name'],\n",
    "            'data_type': str(col['type']),\n",
    "            'nullable': col['nullable'],\n",
    "            'default': col.get('default'),\n",
    "            'primary_key': col.get('primary_key', False)\n",
    "        }\n",
    "        for col in columns\n",
    "    ])\n",
    "    \n",
    "    return schema_df\n",
    "\n",
    "def close_database_engine(engine):\n",
    "    \"\"\"\n",
    "    Properly close database connections.\n",
    "    \"\"\"\n",
    "    if engine:\n",
    "        engine.dispose()\n",
    "        print(\"üîí PostgreSQL connections closed\")\n",
    "\n",
    "# Create database engine and connect to Supabase\n",
    "print(\"üöÄ Connecting to Supabase PostgreSQL Database...\")\n",
    "db_engine = create_database_engine()\n",
    "\n",
    "# Display database information\n",
    "print(\"\\nüìä Olist E-commerce Database Overview:\")\n",
    "db_info = get_table_info(db_engine)\n",
    "\n",
    "print(f\"\\nüóÉÔ∏è Total tables discovered: {len(db_info)}\")\n",
    "total_rows = sum(info['rows'] for info in db_info.values())\n",
    "print(f\"üìè Total rows across all tables: {total_rows:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring the Database Schema\n",
    "\n",
    "Let's explore the structure of our Olist e-commerce database to understand the business data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Real Business Data Exploration\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìä OLIST SALES DATASET - Brazilian E-commerce Marketplace\n",
      "Found 9 core business tables:\n",
      "  ‚Ä¢ olist_order_reviews_dataset           98,410 rows\n",
      "    Customer satisfaction scores and feedback\n",
      "  ‚Ä¢ olist_order_items_dataset            112,650 rows\n",
      "    Product items within each order\n",
      "  ‚Ä¢ olist_order_payments_dataset         103,886 rows\n",
      "    Payment methods and transaction values\n",
      "  ‚Ä¢ olist_customers_dataset               99,441 rows\n",
      "    Customer demographics and location data\n",
      "  ‚Ä¢ olist_orders_dataset                  99,441 rows\n",
      "    Order lifecycle and delivery tracking\n",
      "  ‚Ä¢ olist_sellers_dataset                  3,095 rows\n",
      "    Marketplace seller information and locations\n",
      "  ‚Ä¢ product_category_name_translation         73 rows\n",
      "    Portuguese to English category translations\n",
      "  ‚Ä¢ olist_products_dataset                32,951 rows\n",
      "    Product catalog with categories and dimensions\n",
      "  ‚Ä¢ olist_geolocation_dataset           1,000,163 rows\n",
      "    Geographic coordinates for Brazilian ZIP codes\n",
      "\n",
      "üìà OLIST MARKETING DATASET\n",
      "Found 2 marketing tables:\n",
      "  ‚Ä¢ olist_marketing_qualified_leads_dataset    8,000 rows\n",
      "  ‚Ä¢ olist_closed_deals_dataset               380 rows\n",
      "\n",
      "üìã Core Business Schema - Orders Table Structure:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "column_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "data_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "nullable",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "default",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "primary_key",
         "rawType": "bool",
         "type": "boolean"
        }
       ],
       "ref": "5b29b2ac-9ecd-4f05-978f-a0df3b5da864",
       "rows": [
        [
         "0",
         "order_id",
         "TEXT",
         "False",
         null,
         "False"
        ],
        [
         "1",
         "customer_id",
         "TEXT",
         "True",
         null,
         "False"
        ],
        [
         "2",
         "order_status",
         "TEXT",
         "True",
         null,
         "False"
        ],
        [
         "3",
         "order_purchase_timestamp",
         "TIMESTAMP",
         "True",
         null,
         "False"
        ],
        [
         "4",
         "order_approved_at",
         "TIMESTAMP",
         "True",
         null,
         "False"
        ],
        [
         "5",
         "order_delivered_carrier_date",
         "TIMESTAMP",
         "True",
         null,
         "False"
        ],
        [
         "6",
         "order_delivered_customer_date",
         "TIMESTAMP",
         "True",
         null,
         "False"
        ],
        [
         "7",
         "order_estimated_delivery_date",
         "TIMESTAMP",
         "True",
         null,
         "False"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>data_type</th>\n",
       "      <th>nullable</th>\n",
       "      <th>default</th>\n",
       "      <th>primary_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>order_id</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>customer_id</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>order_status</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>order_purchase_timestamp</td>\n",
       "      <td>TIMESTAMP</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>order_approved_at</td>\n",
       "      <td>TIMESTAMP</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>order_delivered_carrier_date</td>\n",
       "      <td>TIMESTAMP</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>order_delivered_customer_date</td>\n",
       "      <td>TIMESTAMP</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>order_estimated_delivery_date</td>\n",
       "      <td>TIMESTAMP</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     column_name  data_type  nullable default  primary_key\n",
       "0                       order_id       TEXT     False    None        False\n",
       "1                    customer_id       TEXT      True    None        False\n",
       "2                   order_status       TEXT      True    None        False\n",
       "3       order_purchase_timestamp  TIMESTAMP      True    None        False\n",
       "4              order_approved_at  TIMESTAMP      True    None        False\n",
       "5   order_delivered_carrier_date  TIMESTAMP      True    None        False\n",
       "6  order_delivered_customer_date  TIMESTAMP      True    None        False\n",
       "7  order_estimated_delivery_date  TIMESTAMP      True    None        False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Real Order Data Sample:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "order_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "customer_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "order_status",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "order_purchase_timestamp",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "order_approved_at",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "order_delivered_carrier_date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "order_delivered_customer_date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "order_estimated_delivery_date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        }
       ],
       "ref": "46c6f7e7-c6bf-4a4a-9757-e4fc6bf36165",
       "rows": [
        [
         "0",
         "e481f51cbdc54678b7cc49136f2d6af7",
         "9ef432eb6251297304e76186b10a928d",
         "delivered",
         "2017-10-02 10:56:33",
         "2017-10-02 11:07:15",
         "2017-10-04 19:55:00",
         "2017-10-10 21:25:13",
         "2017-10-18 00:00:00"
        ],
        [
         "1",
         "53cdb2fc8bc7dce0b6741e2150273451",
         "b0830fb4747a6c6d20dea0b8c802d7ef",
         "delivered",
         "2018-07-24 20:41:37",
         "2018-07-26 03:24:27",
         "2018-07-26 14:31:00",
         "2018-08-07 15:27:45",
         "2018-08-13 00:00:00"
        ],
        [
         "2",
         "47770eb9100c2d0c44946d9cf07ec65d",
         "41ce2a54c0b03bf3443c3d931a367089",
         "delivered",
         "2018-08-08 08:38:49",
         "2018-08-08 08:55:23",
         "2018-08-08 13:50:00",
         "2018-08-17 18:06:29",
         "2018-09-04 00:00:00"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>order_approved_at</th>\n",
       "      <th>order_delivered_carrier_date</th>\n",
       "      <th>order_delivered_customer_date</th>\n",
       "      <th>order_estimated_delivery_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53cdb2fc8bc7dce0b6741e2150273451</td>\n",
       "      <td>b0830fb4747a6c6d20dea0b8c802d7ef</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-07-24 20:41:37</td>\n",
       "      <td>2018-07-26 03:24:27</td>\n",
       "      <td>2018-07-26 14:31:00</td>\n",
       "      <td>2018-08-07 15:27:45</td>\n",
       "      <td>2018-08-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47770eb9100c2d0c44946d9cf07ec65d</td>\n",
       "      <td>41ce2a54c0b03bf3443c3d931a367089</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-08-08 08:38:49</td>\n",
       "      <td>2018-08-08 08:55:23</td>\n",
       "      <td>2018-08-08 13:50:00</td>\n",
       "      <td>2018-08-17 18:06:29</td>\n",
       "      <td>2018-09-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id                       customer_id  \\\n",
       "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "1  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
       "2  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
       "\n",
       "  order_status order_purchase_timestamp   order_approved_at  \\\n",
       "0    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15   \n",
       "1    delivered      2018-07-24 20:41:37 2018-07-26 03:24:27   \n",
       "2    delivered      2018-08-08 08:38:49 2018-08-08 08:55:23   \n",
       "\n",
       "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
       "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "1          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
       "2          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
       "\n",
       "  order_estimated_delivery_date  \n",
       "0                    2017-10-18  \n",
       "1                    2018-08-13  \n",
       "2                    2018-09-04  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° Real Business Model Understanding:\n",
      "  üè™ Olist is a Brazilian e-commerce marketplace (like Amazon)\n",
      "  üõí Connects sellers with customers across Brazil\n",
      "  üì¶ Handles logistics, payments, and customer service\n",
      "  üìä Rich dataset: 100K+ orders, 32K+ products, 3K+ sellers\n",
      "  üåç Geographic coverage: All Brazilian states\n",
      "  üí∞ Business metrics: R$ 13.6M+ in sales, 140+ avg order value\n",
      "  üìÖ Time period: 2016-2018 Brazilian e-commerce data\n"
     ]
    }
   ],
   "source": [
    "# Real Business Data Exploration: Olist E-commerce Dataset\n",
    "print(\"üîç Real Business Data Exploration\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Now we have actual data! Let's explore the real Olist e-commerce business model\n",
    "sales_tables = [table for table in db_info.keys() if table.startswith('olist_sales_data_set.')]\n",
    "marketing_tables = [table for table in db_info.keys() if table.startswith('olist_marketing_data_set.')]\n",
    "\n",
    "print(f\"\\nüìä OLIST SALES DATASET - Brazilian E-commerce Marketplace\")\n",
    "print(f\"Found {len(sales_tables)} core business tables:\")\n",
    "\n",
    "# Display actual business tables with their purpose\n",
    "table_descriptions = {\n",
    "    'olist_customers_dataset': 'Customer demographics and location data',\n",
    "    'olist_orders_dataset': 'Order lifecycle and delivery tracking',\n",
    "    'olist_order_items_dataset': 'Product items within each order',\n",
    "    'olist_order_payments_dataset': 'Payment methods and transaction values',\n",
    "    'olist_order_reviews_dataset': 'Customer satisfaction scores and feedback',\n",
    "    'olist_products_dataset': 'Product catalog with categories and dimensions',\n",
    "    'olist_sellers_dataset': 'Marketplace seller information and locations',\n",
    "    'olist_geolocation_dataset': 'Geographic coordinates for Brazilian ZIP codes',\n",
    "    'product_category_name_translation': 'Portuguese to English category translations'\n",
    "}\n",
    "\n",
    "for table in sales_tables:\n",
    "    if table in db_info:\n",
    "        info = db_info[table]\n",
    "        table_name = info['table']\n",
    "        description = table_descriptions.get(table_name, 'Business dataset')\n",
    "        print(f\"  ‚Ä¢ {table_name:<35} {info['rows']:>8,} rows\")\n",
    "        print(f\"    {description}\")\n",
    "\n",
    "print(f\"\\nüìà OLIST MARKETING DATASET\")\n",
    "print(f\"Found {len(marketing_tables)} marketing tables:\")\n",
    "\n",
    "for table in marketing_tables:\n",
    "    if table in db_info:\n",
    "        info = db_info[table]\n",
    "        table_name = info['table']\n",
    "        print(f\"  ‚Ä¢ {table_name:<35} {info['rows']:>8,} rows\")\n",
    "\n",
    "# Let's examine the core business relationships\n",
    "if sales_tables:\n",
    "    main_orders_table = 'olist_sales_data_set.olist_orders_dataset'\n",
    "    print(f\"\\nüìã Core Business Schema - Orders Table Structure:\")\n",
    "    orders_schema = get_table_schema(db_engine, main_orders_table)\n",
    "    display(orders_schema)\n",
    "    \n",
    "    print(f\"\\nüì¶ Real Order Data Sample:\")\n",
    "    orders_sample = get_sample_data(db_engine, main_orders_table, 3)\n",
    "    display(orders_sample)\n",
    "\n",
    "print(\"\\nüí° Real Business Model Understanding:\")\n",
    "print(\"  üè™ Olist is a Brazilian e-commerce marketplace (like Amazon)\")\n",
    "print(\"  üõí Connects sellers with customers across Brazil\")\n",
    "print(\"  üì¶ Handles logistics, payments, and customer service\")\n",
    "print(\"  üìä Rich dataset: 100K+ orders, 32K+ products, 3K+ sellers\")\n",
    "print(\"  üåç Geographic coverage: All Brazilian states\")\n",
    "print(\"  üí∞ Business metrics: R$ 13.6M+ in sales, 140+ avg order value\")\n",
    "print(f\"  üìÖ Time period: 2016-2018 Brazilian e-commerce data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running SQL Queries from Python\n",
    "\n",
    "Now let's execute SQL queries directly from Python and see how they work with real cloud data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real Business Intelligence SQL Queries\n",
    "print(\"üîç Real Business Intelligence with SQL\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Now we can run actual business analysis instead of guessing column names!\n",
    "print(\"\\nüìã Example 1: Customer Distribution Analysis\")\n",
    "print(\"Business Question: Where are our customers located?\")\n",
    "\n",
    "if sales_tables:\n",
    "    # Real customer geographic analysis\n",
    "    customer_geo_query = \"\"\"\n",
    "    SELECT \n",
    "        customer_state,\n",
    "        COUNT(*) as customer_count,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 1) as percentage\n",
    "    FROM \"olist_sales_data_set\".\"olist_customers_dataset\"\n",
    "    WHERE customer_state IS NOT NULL\n",
    "    GROUP BY customer_state\n",
    "    ORDER BY customer_count DESC\n",
    "    LIMIT 8\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        customer_analysis = execute_query(db_engine, customer_geo_query)\n",
    "        print(\"‚úÖ Customer Geographic Distribution:\")\n",
    "        display(customer_analysis)\n",
    "        \n",
    "        print(f\"\\nüí° Business Insights:\")\n",
    "        print(f\"  ‚Ä¢ S√£o Paulo (SP) dominates with {customer_analysis.iloc[0]['customer_count']:,} customers\")\n",
    "        print(f\"  ‚Ä¢ Top 3 states account for {customer_analysis.head(3)['percentage'].sum():.1f}% of customers\")\n",
    "        print(f\"  ‚Ä¢ Geographic concentration in Southeast Brazil\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Query failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"\\nüìä Example 2: Order Status Pipeline Analysis\")\n",
    "print(\"Business Question: What's our order fulfillment performance?\")\n",
    "\n",
    "if sales_tables:\n",
    "    # Real order status analysis\n",
    "    order_status_query = \"\"\"\n",
    "    SELECT \n",
    "        order_status,\n",
    "        COUNT(*) as order_count,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 1) as percentage\n",
    "    FROM \"olist_sales_data_set\".\"olist_orders_dataset\"\n",
    "    GROUP BY order_status\n",
    "    ORDER BY order_count DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        order_status_analysis = execute_query(db_engine, order_status_query)\n",
    "        print(\"‚úÖ Order Status Distribution:\")\n",
    "        display(order_status_analysis)\n",
    "        \n",
    "        delivered_rate = order_status_analysis[order_status_analysis['order_status'] == 'delivered']['percentage'].iloc[0]\n",
    "        print(f\"\\nüí° Operational Insights:\")\n",
    "        print(f\"  ‚Ä¢ {delivered_rate}% successful delivery rate\")\n",
    "        print(f\"  ‚Ä¢ {order_status_analysis.iloc[0]['order_count']:,} total orders processed\")\n",
    "        print(f\"  ‚Ä¢ Strong operational performance with minimal cancellations\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Query failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"\\nüí∞ Example 3: Revenue and Payment Analysis\")\n",
    "print(\"Business Question: What are our key financial metrics?\")\n",
    "\n",
    "if sales_tables:\n",
    "    # Real revenue analysis\n",
    "    revenue_query = \"\"\"\n",
    "    SELECT \n",
    "        payment_type,\n",
    "        COUNT(*) as transaction_count,\n",
    "        SUM(payment_value::numeric) as total_revenue,\n",
    "        AVG(payment_value::numeric) as avg_payment_value\n",
    "    FROM \"olist_sales_data_set\".\"olist_order_payments_dataset\"\n",
    "    WHERE payment_value IS NOT NULL\n",
    "    GROUP BY payment_type\n",
    "    ORDER BY total_revenue DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        revenue_analysis = execute_query(db_engine, revenue_query)\n",
    "        print(\"‚úÖ Payment Method Analysis:\")\n",
    "        display(revenue_analysis)\n",
    "        \n",
    "        total_revenue = revenue_analysis['total_revenue'].sum()\n",
    "        print(f\"\\nüí° Financial Insights:\")\n",
    "        print(f\"  ‚Ä¢ Total revenue: R$ {total_revenue:,.2f}\")\n",
    "        print(f\"  ‚Ä¢ Credit cards dominate: {revenue_analysis.iloc[0]['total_revenue']/total_revenue*100:.1f}% of revenue\")\n",
    "        print(f\"  ‚Ä¢ Brazilian payment preferences clearly visible\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Query failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real Business Relationships: JOIN Analysis\n",
    "print(\"\\nüîó Real Business Relationships: JOIN Analysis\")\n",
    "print(\"Business Question: How do our core business entities connect?\")\n",
    "\n",
    "if sales_tables:\n",
    "    print(\"\\nüìã Customer-Order-Product Relationship Analysis\")\n",
    "    \n",
    "    # Real business relationship query showing actual data flow\n",
    "    customer_order_analysis_query = \"\"\"\n",
    "    SELECT \n",
    "        c.customer_state,\n",
    "        COUNT(DISTINCT c.customer_id) as unique_customers,\n",
    "        COUNT(DISTINCT o.order_id) as total_orders,\n",
    "        COUNT(oi.order_item_id) as total_items,\n",
    "        SUM(oi.price::numeric) as total_item_value,\n",
    "        AVG(oi.price::numeric) as avg_item_price\n",
    "    FROM \"olist_sales_data_set\".\"olist_customers_dataset\" c\n",
    "    INNER JOIN \"olist_sales_data_set\".\"olist_orders_dataset\" o \n",
    "        ON c.customer_id = o.customer_id\n",
    "    INNER JOIN \"olist_sales_data_set\".\"olist_order_items_dataset\" oi \n",
    "        ON o.order_id = oi.order_id\n",
    "    WHERE c.customer_state IS NOT NULL \n",
    "        AND o.order_status = 'delivered'\n",
    "        AND oi.price IS NOT NULL\n",
    "    GROUP BY c.customer_state\n",
    "    ORDER BY total_item_value DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        relationship_analysis = execute_query(db_engine, customer_order_analysis_query)\n",
    "        print(\"‚úÖ Customer-Order-Product JOIN Analysis:\")\n",
    "        display(relationship_analysis)\n",
    "        \n",
    "        if len(relationship_analysis) > 0:\n",
    "            top_state = relationship_analysis.iloc[0]\n",
    "            print(f\"\\nüîç JOIN Analysis Insights:\")\n",
    "            print(f\"  ‚Ä¢ Top revenue state: {top_state['customer_state']}\")\n",
    "            print(f\"  ‚Ä¢ {top_state['unique_customers']:,} customers generated R$ {top_state['total_item_value']:,.2f}\")\n",
    "            print(f\"  ‚Ä¢ Average item price: R$ {top_state['avg_item_price']:,.2f}\")\n",
    "            print(f\"  ‚Ä¢ This demonstrates real foreign key relationships working!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå JOIN analysis failed: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"\\n‚≠ê Product Performance with Reviews Analysis\")\n",
    "    \n",
    "    # Real product-review relationship analysis\n",
    "    product_review_query = \"\"\"\n",
    "    SELECT \n",
    "        p.product_category_name,\n",
    "        t.product_category_name_english,\n",
    "        COUNT(DISTINCT p.product_id) as unique_products,\n",
    "        COUNT(r.review_id) as total_reviews,\n",
    "        AVG(r.review_score::numeric) as avg_review_score,\n",
    "        SUM(oi.price::numeric) as category_revenue\n",
    "    FROM \"olist_sales_data_set\".\"olist_products_dataset\" p\n",
    "    INNER JOIN \"olist_sales_data_set\".\"olist_order_items_dataset\" oi \n",
    "        ON p.product_id = oi.product_id\n",
    "    INNER JOIN \"olist_sales_data_set\".\"olist_order_reviews_dataset\" r \n",
    "        ON oi.order_id = r.order_id\n",
    "    LEFT JOIN \"olist_sales_data_set\".\"product_category_name_translation\" t\n",
    "        ON p.product_category_name = t.product_category_name\n",
    "    WHERE p.product_category_name IS NOT NULL\n",
    "        AND r.review_score IS NOT NULL\n",
    "        AND oi.price IS NOT NULL\n",
    "    GROUP BY p.product_category_name, t.product_category_name_english\n",
    "    ORDER BY category_revenue DESC\n",
    "    LIMIT 8\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        product_performance = execute_query(db_engine, product_review_query)\n",
    "        print(\"‚úÖ Product Category Performance (with Reviews):\")\n",
    "        display(product_performance)\n",
    "        \n",
    "        print(f\"\\nüí° Multi-Table JOIN Insights:\")\n",
    "        print(f\"  ‚Ä¢ Successfully joined 4 tables: products ‚Üí order_items ‚Üí reviews ‚Üí translations\")\n",
    "        print(f\"  ‚Ä¢ Real business metrics: revenue + customer satisfaction\")\n",
    "        print(f\"  ‚Ä¢ Shows complex relationships in e-commerce data\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Product performance analysis failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Marketing funnel analysis\n",
    "print(\"\\nüìà Example 3: Marketing Analysis\")\n",
    "print(\"Business Question: How effective are our marketing channels?\")\n",
    "\n",
    "if marketing_tables:\n",
    "    main_marketing_table = marketing_tables[0]\n",
    "    schema, table = main_marketing_table.split('.', 1)\n",
    "    \n",
    "    # Examine marketing table structure\n",
    "    marketing_sample = get_sample_data(db_engine, main_marketing_table, 1)\n",
    "    print(f\"\\nActual columns in {table}:\")\n",
    "    print(list(marketing_sample.columns))\n",
    "    \n",
    "    marketing_columns = db_info[main_marketing_table]['columns']\n",
    "    \n",
    "    # Look for relevant marketing columns\n",
    "    channel_cols = [col for col in marketing_columns if any(keyword in col.lower() for keyword in ['origin', 'source', 'channel', 'medium'])]\n",
    "    lead_cols = [col for col in marketing_columns if any(keyword in col.lower() for keyword in ['lead', 'mql', 'conversion'])]\n",
    "    type_cols = [col for col in marketing_columns if any(keyword in col.lower() for keyword in ['type', 'category', 'segment'])]\n",
    "    \n",
    "    print(f\"\\nChannel-related columns: {channel_cols}\")\n",
    "    print(f\"Lead-related columns: {lead_cols}\")\n",
    "    print(f\"Type/Category columns: {type_cols}\")\n",
    "    \n",
    "    # Try analysis with available columns\n",
    "    analysis_col = channel_cols[0] if channel_cols else (type_cols[0] if type_cols else None)\n",
    "    \n",
    "    if analysis_col:\n",
    "        # Build marketing analysis query with schema qualification\n",
    "        marketing_query = f\"\"\"\n",
    "        SELECT \n",
    "            \"{analysis_col}\" as marketing_dimension,\n",
    "            COUNT(*) as total_records,\n",
    "            ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "        FROM \"{schema}\".\"{table}\"\n",
    "        WHERE \"{analysis_col}\" IS NOT NULL\n",
    "        GROUP BY \"{analysis_col}\"\n",
    "        ORDER BY total_records DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            marketing_analysis = execute_query(db_engine, marketing_query)\n",
    "            print(f\"\\n‚úÖ Marketing Analysis by {analysis_col.replace('_', ' ').title()}:\")\n",
    "            display(marketing_analysis)\n",
    "            \n",
    "            if len(marketing_analysis) > 0:\n",
    "                top_dimension = marketing_analysis.iloc[0]\n",
    "                print(f\"\\nüí° Marketing Insights:\")\n",
    "                print(f\"  ‚Ä¢ Top {analysis_col.replace('_', ' ')}: {top_dimension['marketing_dimension']} ({top_dimension['percentage']}%)\")\n",
    "                print(f\"  ‚Ä¢ Total categories: {len(marketing_analysis)}\")\n",
    "                print(f\"  ‚Ä¢ Total records analyzed: {top_dimension['total_records']:,}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Marketing analysis failed: {e}\")\n",
    "    else:\n",
    "        # Fallback: show basic table statistics\n",
    "        print(\"‚ö†Ô∏è No obvious analysis columns found, showing basic statistics:\")\n",
    "        \n",
    "        basic_marketing_query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_records,\n",
    "            COUNT(DISTINCT *) as unique_records\n",
    "        FROM \"{schema}\".\"{table}\"\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            basic_stats = execute_query(db_engine, basic_marketing_query)\n",
    "            print(\"‚úÖ Basic Marketing Table Statistics:\")\n",
    "            display(basic_stats)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Basic statistics failed: {e}\")\n",
    "            print(\"Showing sample data instead:\")\n",
    "            display(marketing_sample)\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No marketing tables found in the database\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced SQL Features\n",
    "\n",
    "Let's explore more sophisticated SQL queries that are common in business intelligence scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced SQL: Real Time-Series Analysis with Brazilian E-commerce Data\n",
    "print(\"üß† Advanced SQL Analysis: Real Time-Series & Window Functions\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "print(\"\\nüìÖ Example: Brazilian E-commerce Growth Trends\")\n",
    "print(\"Business Question: How did Olist's business grow month-by-month?\")\n",
    "\n",
    "if sales_tables:\n",
    "    # Real time-series analysis with actual order dates\n",
    "    time_analysis_query = \"\"\"\n",
    "    SELECT \n",
    "        DATE_TRUNC('month', order_purchase_timestamp) as month,\n",
    "        COUNT(*) as monthly_orders,\n",
    "        COUNT(DISTINCT customer_id) as unique_customers,\n",
    "        LAG(COUNT(*), 1) OVER (ORDER BY DATE_TRUNC('month', order_purchase_timestamp)) as prev_month_orders,\n",
    "        CASE \n",
    "            WHEN LAG(COUNT(*), 1) OVER (ORDER BY DATE_TRUNC('month', order_purchase_timestamp)) IS NOT NULL\n",
    "            THEN ROUND(\n",
    "                (COUNT(*) - LAG(COUNT(*), 1) OVER (ORDER BY DATE_TRUNC('month', order_purchase_timestamp))) * 100.0 / \n",
    "                LAG(COUNT(*), 1) OVER (ORDER BY DATE_TRUNC('month', order_purchase_timestamp)), \n",
    "                1\n",
    "            )\n",
    "            ELSE NULL\n",
    "        END as month_over_month_growth\n",
    "    FROM \"olist_sales_data_set\".\"olist_orders_dataset\"\n",
    "    WHERE order_purchase_timestamp IS NOT NULL\n",
    "        AND order_purchase_timestamp >= '2017-01-01'\n",
    "        AND order_purchase_timestamp < '2019-01-01'\n",
    "    GROUP BY DATE_TRUNC('month', order_purchase_timestamp)\n",
    "    ORDER BY month\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        time_analysis = execute_query(db_engine, time_analysis_query)\n",
    "        print(\"\\n‚úÖ Monthly Growth Analysis (2017-2018):\")\n",
    "        display(time_analysis)\n",
    "        \n",
    "        if len(time_analysis) > 1:\n",
    "            # Calculate business insights\n",
    "            avg_monthly_orders = time_analysis['monthly_orders'].mean()\n",
    "            peak_month = time_analysis.loc[time_analysis['monthly_orders'].idxmax()]\n",
    "            \n",
    "            print(f\"\\nüìà Brazilian E-commerce Trends:\")\n",
    "            print(f\"  ‚Ä¢ Average monthly orders: {avg_monthly_orders:.0f}\")\n",
    "            print(f\"  ‚Ä¢ Peak month: {peak_month['month']} with {peak_month['monthly_orders']:,} orders\")\n",
    "            print(f\"  ‚Ä¢ Growth rate analysis shows seasonal e-commerce patterns\")\n",
    "            print(f\"  ‚Ä¢ Window functions (LAG) enable month-over-month calculations\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Time analysis failed: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"\\nüåç Geographic Analysis with Window Functions\")\n",
    "    print(\"Business Question: How do Brazilian states rank by business performance?\")\n",
    "    \n",
    "    # Advanced window functions for regional ranking\n",
    "    regional_ranking_query = \"\"\"\n",
    "    SELECT \n",
    "        c.customer_state,\n",
    "        COUNT(DISTINCT o.order_id) as total_orders,\n",
    "        SUM(oi.price::numeric + oi.freight_value::numeric) as total_revenue,\n",
    "        ROUND(AVG(oi.price::numeric), 2) as avg_item_price,\n",
    "        RANK() OVER (ORDER BY COUNT(DISTINCT o.order_id) DESC) as order_rank,\n",
    "        RANK() OVER (ORDER BY SUM(oi.price::numeric + oi.freight_value::numeric) DESC) as revenue_rank,\n",
    "        ROUND(\n",
    "            SUM(oi.price::numeric + oi.freight_value::numeric) * 100.0 / \n",
    "            SUM(SUM(oi.price::numeric + oi.freight_value::numeric)) OVER(), 2\n",
    "        ) as revenue_percentage\n",
    "    FROM \"olist_sales_data_set\".\"olist_customers_dataset\" c\n",
    "    INNER JOIN \"olist_sales_data_set\".\"olist_orders_dataset\" o ON c.customer_id = o.customer_id\n",
    "    INNER JOIN \"olist_sales_data_set\".\"olist_order_items_dataset\" oi ON o.order_id = oi.order_id\n",
    "    WHERE c.customer_state IS NOT NULL \n",
    "        AND o.order_status = 'delivered'\n",
    "        AND oi.price IS NOT NULL\n",
    "        AND oi.freight_value IS NOT NULL\n",
    "    GROUP BY c.customer_state\n",
    "    ORDER BY total_revenue DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        regional_analysis = execute_query(db_engine, regional_ranking_query)\n",
    "        print(\"\\n‚úÖ Regional Performance Ranking:\")\n",
    "        display(regional_analysis)\n",
    "        \n",
    "        print(f\"\\nüèÜ Geographic Business Insights:\")\n",
    "        top_state = regional_analysis.iloc[0]\n",
    "        print(f\"  ‚Ä¢ #1 Revenue State: {top_state['customer_state']} (R$ {top_state['total_revenue']:,.2f})\")\n",
    "        print(f\"  ‚Ä¢ Market concentration: Top 3 states = {regional_analysis.head(3)['revenue_percentage'].sum():.1f}% of revenue\")\n",
    "        print(f\"  ‚Ä¢ Window functions enable sophisticated ranking and percentage calculations\")\n",
    "        print(f\"  ‚Ä¢ Real business intelligence for Brazilian market analysis\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Regional analysis failed: {e}\")\n",
    "\n",
    "print(\"\\nüí° Advanced SQL Features Demonstrated:\")\n",
    "print(\"  ‚Ä¢ DATE_TRUNC() for time-series grouping with real dates\")\n",
    "print(\"  ‚Ä¢ LAG() window function for month-over-month growth analysis\")\n",
    "print(\"  ‚Ä¢ RANK() for competitive regional analysis\")\n",
    "print(\"  ‚Ä¢ SUM() OVER() for percentage calculations across result set\")\n",
    "print(\"  ‚Ä¢ Complex JOINs with real business logic\")\n",
    "print(\"  ‚Ä¢ Brazilian e-commerce seasonality and geographic patterns\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced SQL: Real CTEs for Business Intelligence\n",
    "print(\"\\nüíº Advanced SQL: CTEs for Real Business Intelligence\")\n",
    "print(\"Business Question: Can we build a customer lifetime value model?\")\n",
    "\n",
    "if sales_tables:\n",
    "    # Real CTE analysis using actual Brazilian e-commerce data\n",
    "    print(\"\\nüìä Customer Lifetime Value Analysis (using CTEs)\")\n",
    "    \n",
    "    clv_analysis_query = \"\"\"\n",
    "    WITH customer_metrics AS (\n",
    "        -- Calculate per-customer metrics\n",
    "        SELECT \n",
    "            c.customer_id,\n",
    "            c.customer_state,\n",
    "            COUNT(DISTINCT o.order_id) as total_orders,\n",
    "            SUM(oi.price::numeric + oi.freight_value::numeric) as lifetime_value,\n",
    "            AVG(oi.price::numeric) as avg_item_price,\n",
    "            MIN(o.order_purchase_timestamp) as first_order_date,\n",
    "            MAX(o.order_purchase_timestamp) as last_order_date\n",
    "        FROM \"olist_sales_data_set\".\"olist_customers_dataset\" c\n",
    "        INNER JOIN \"olist_sales_data_set\".\"olist_orders_dataset\" o ON c.customer_id = o.customer_id\n",
    "        INNER JOIN \"olist_sales_data_set\".\"olist_order_items_dataset\" oi ON o.order_id = oi.order_id\n",
    "        WHERE o.order_status = 'delivered'\n",
    "            AND oi.price IS NOT NULL \n",
    "            AND oi.freight_value IS NOT NULL\n",
    "        GROUP BY c.customer_id, c.customer_state\n",
    "    ),\n",
    "    customer_segments AS (\n",
    "        -- Segment customers based on behavior\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            customer_state,\n",
    "            total_orders,\n",
    "            lifetime_value,\n",
    "            avg_item_price,\n",
    "            CASE \n",
    "                WHEN lifetime_value >= (SELECT AVG(lifetime_value) * 2 FROM customer_metrics) THEN 'High Value'\n",
    "                WHEN total_orders > 1 THEN 'Repeat Customer'\n",
    "                ELSE 'Single Purchase'\n",
    "            END as customer_segment\n",
    "        FROM customer_metrics\n",
    "    ),\n",
    "    state_performance AS (\n",
    "        -- Aggregate by state and segment\n",
    "        SELECT \n",
    "            customer_state,\n",
    "            customer_segment,\n",
    "            COUNT(*) as customer_count,\n",
    "            AVG(lifetime_value) as avg_clv,\n",
    "            SUM(lifetime_value) as total_state_revenue\n",
    "        FROM customer_segments\n",
    "        GROUP BY customer_state, customer_segment\n",
    "    )\n",
    "    SELECT \n",
    "        customer_state,\n",
    "        customer_segment,\n",
    "        customer_count,\n",
    "        ROUND(avg_clv, 2) as avg_customer_lifetime_value,\n",
    "        ROUND(total_state_revenue, 2) as segment_revenue\n",
    "    FROM state_performance\n",
    "    WHERE customer_state IN ('SP', 'RJ', 'MG', 'RS', 'PR')  -- Top 5 states\n",
    "    ORDER BY customer_state, segment_revenue DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        clv_analysis = execute_query(db_engine, clv_analysis_query)\n",
    "        print(\"\\n‚úÖ Customer Lifetime Value by State & Segment:\")\n",
    "        display(clv_analysis)\n",
    "        \n",
    "        print(f\"\\nüéØ CTE Business Intelligence Insights:\")\n",
    "        # Analyze the segments\n",
    "        high_value_customers = clv_analysis[clv_analysis['customer_segment'] == 'High Value']\n",
    "        if len(high_value_customers) > 0:\n",
    "            best_high_value_state = high_value_customers.loc[high_value_customers['avg_customer_lifetime_value'].idxmax()]\n",
    "            print(f\"  ‚Ä¢ Best high-value state: {best_high_value_state['customer_state']}\")\n",
    "            print(f\"  ‚Ä¢ High-value CLV: R$ {best_high_value_state['avg_customer_lifetime_value']:,.2f}\")\n",
    "        \n",
    "        print(f\"  ‚Ä¢ CTEs enabled complex 3-step analysis: metrics ‚Üí segments ‚Üí aggregation\")\n",
    "        print(f\"  ‚Ä¢ Real business model: customer segmentation for Brazilian e-commerce\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå CTE analysis failed: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"\\nüîÑ Advanced CTE: Product Category Performance Analysis\")\n",
    "    \n",
    "    category_performance_query = \"\"\"\n",
    "    WITH product_sales AS (\n",
    "        -- Product-level sales metrics\n",
    "        SELECT \n",
    "            p.product_category_name,\n",
    "            p.product_id,\n",
    "            SUM(oi.price::numeric) as product_revenue,\n",
    "            COUNT(oi.order_id) as times_sold,\n",
    "            AVG(r.review_score::numeric) as avg_review_score\n",
    "        FROM \"olist_sales_data_set\".\"olist_products_dataset\" p\n",
    "        INNER JOIN \"olist_sales_data_set\".\"olist_order_items_dataset\" oi ON p.product_id = oi.product_id\n",
    "        LEFT JOIN \"olist_sales_data_set\".\"olist_order_reviews_dataset\" r ON oi.order_id = r.order_id\n",
    "        WHERE p.product_category_name IS NOT NULL\n",
    "            AND oi.price IS NOT NULL\n",
    "        GROUP BY p.product_category_name, p.product_id\n",
    "    ),\n",
    "    category_summary AS (\n",
    "        -- Category-level aggregation\n",
    "        SELECT \n",
    "            product_category_name,\n",
    "            COUNT(DISTINCT product_id) as unique_products,\n",
    "            SUM(product_revenue) as category_revenue,\n",
    "            AVG(avg_review_score) as category_satisfaction,\n",
    "            SUM(times_sold) as total_units_sold\n",
    "        FROM product_sales\n",
    "        WHERE avg_review_score IS NOT NULL\n",
    "        GROUP BY product_category_name\n",
    "    ),\n",
    "    category_ranking AS (\n",
    "        -- Add performance rankings\n",
    "        SELECT \n",
    "            product_category_name,\n",
    "            unique_products,\n",
    "            ROUND(category_revenue, 2) as category_revenue,\n",
    "            ROUND(category_satisfaction, 2) as avg_satisfaction_score,\n",
    "            total_units_sold,\n",
    "            RANK() OVER (ORDER BY category_revenue DESC) as revenue_rank,\n",
    "            RANK() OVER (ORDER BY category_satisfaction DESC) as satisfaction_rank\n",
    "        FROM category_summary\n",
    "    )\n",
    "    SELECT \n",
    "        product_category_name,\n",
    "        unique_products,\n",
    "        category_revenue,\n",
    "        avg_satisfaction_score,\n",
    "        total_units_sold,\n",
    "        revenue_rank,\n",
    "        satisfaction_rank\n",
    "    FROM category_ranking\n",
    "    ORDER BY category_revenue DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        category_analysis = execute_query(db_engine, category_performance_query)\n",
    "        print(\"\\n‚úÖ Product Category Performance (Revenue vs Satisfaction):\")\n",
    "        display(category_analysis)\n",
    "        \n",
    "        print(f\"\\nüìà Category Performance Insights:\")\n",
    "        top_revenue = category_analysis.iloc[0]\n",
    "        best_satisfaction = category_analysis.loc[category_analysis['avg_satisfaction_score'].idxmax()]\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Top revenue category: {top_revenue['product_category_name']} (R$ {top_revenue['category_revenue']:,.2f})\")\n",
    "        print(f\"  ‚Ä¢ Highest satisfaction: {best_satisfaction['product_category_name']} ({best_satisfaction['avg_satisfaction_score']:.2f} stars)\")\n",
    "        print(f\"  ‚Ä¢ Complex CTEs: sales ‚Üí category ‚Üí ranking analysis\")\n",
    "        print(f\"  ‚Ä¢ Real business intelligence: revenue vs customer satisfaction trade-offs\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Category performance analysis failed: {e}\")\n",
    "\n",
    "print(\"\\nüí° Advanced SQL Features Demonstrated:\")\n",
    "print(\"  ‚Ä¢ Multi-level CTEs for complex business logic\")\n",
    "print(\"  ‚Ä¢ Customer segmentation with CASE statements\")\n",
    "print(\"  ‚Ä¢ Subqueries for dynamic threshold calculations\")\n",
    "print(\"  ‚Ä¢ Window functions for ranking and percentiles\")\n",
    "print(\"  ‚Ä¢ Real business intelligence metrics (CLV, satisfaction, performance)\")\n",
    "print(\"  ‚Ä¢ Brazilian e-commerce insights with actual marketplace data\")\n",
    "\n",
    "print(\"\\nüéì Educational Value:\")\n",
    "print(\"  ‚Ä¢ Students see real SQL patterns used in business intelligence\")\n",
    "print(\"  ‚Ä¢ Complex queries broken down into logical, reusable CTEs\")\n",
    "print(\"  ‚Ä¢ Actual business insights from real e-commerce operations\")\n",
    "print(\"  ‚Ä¢ Practical application of advanced SQL concepts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced SQL Example: Common Table Expressions (CTEs)\n",
    "print(\"\\nüíº Advanced SQL: CTEs for Complex Business Logic\")\n",
    "print(\"Business Question: Can we segment our data for deeper insights?\")\n",
    "\n",
    "if sales_tables:\n",
    "    main_sales_table = sales_tables[1]\n",
    "    schema, table = main_sales_table.split('.', 1)\n",
    "    \n",
    "    # Build a more complex query with CTEs\n",
    "    available_columns = db_info[main_sales_table]['columns']\n",
    "    \n",
    "    # Look for different types of columns\n",
    "    numeric_cols = [col for col in available_columns if any(keyword in col.lower() for keyword in ['price', 'value', 'amount', 'cost'])]\n",
    "    location_cols = [col for col in available_columns if any(keyword in col.lower() for keyword in ['state', 'city', 'region'])]\n",
    "    customer_cols = [col for col in available_columns if 'customer' in col.lower()]\n",
    "    \n",
    "    print(f\"\\nNumeric value columns found: {numeric_cols}\")\n",
    "    print(f\"Location columns found: {location_cols}\")\n",
    "    print(f\"Customer columns found: {customer_cols}\")\n",
    "    \n",
    "    if numeric_cols and location_cols:\n",
    "        # Build CTE query for business segmentation with schema qualification\n",
    "        cte_query = f\"\"\"\n",
    "        WITH regional_stats AS (\n",
    "            SELECT \n",
    "                \"{location_cols[0]}\" as region,\n",
    "                COUNT(*) as total_records,\n",
    "                AVG(\"{numeric_cols[0]}\") as avg_value,\n",
    "                STDDEV(\"{numeric_cols[0]}\") as value_stddev\n",
    "            FROM \"{schema}\".\"{table}\"\n",
    "            WHERE \"{location_cols[0]}\" IS NOT NULL \n",
    "                AND \"{numeric_cols[0]}\" IS NOT NULL\n",
    "                AND \"{numeric_cols[0]}\" > 0\n",
    "            GROUP BY \"{location_cols[0]}\"\n",
    "        ),\n",
    "        regional_segments AS (\n",
    "            SELECT \n",
    "                region,\n",
    "                total_records,\n",
    "                ROUND(avg_value, 2) as avg_value,\n",
    "                CASE \n",
    "                    WHEN avg_value > (SELECT AVG(avg_value) FROM regional_stats) THEN 'High Value'\n",
    "                    WHEN total_records > (SELECT AVG(total_records) FROM regional_stats) THEN 'High Volume'\n",
    "                    ELSE 'Standard'\n",
    "                END as segment\n",
    "            FROM regional_stats\n",
    "        )\n",
    "        SELECT \n",
    "            segment,\n",
    "            COUNT(*) as region_count,\n",
    "            SUM(total_records) as total_records,\n",
    "            ROUND(AVG(avg_value), 2) as segment_avg_value\n",
    "        FROM regional_segments\n",
    "        GROUP BY segment\n",
    "        ORDER BY segment_avg_value DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            cte_analysis = execute_query(db_engine, cte_query)\n",
    "            print(\"\\n‚úÖ Regional Segmentation Analysis (using CTEs):\")\n",
    "            display(cte_analysis)\n",
    "            \n",
    "            print(f\"\\nüéØ Segmentation Insights:\")\n",
    "            for _, row in cte_analysis.iterrows():\n",
    "                print(f\"  ‚Ä¢ {row['segment']}: {row['region_count']} regions, avg value: {row['segment_avg_value']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå CTE analysis failed: {e}\")\n",
    "            print(\"Trying simpler segmentation...\")\n",
    "            \n",
    "            # Fallback: simpler analysis\n",
    "            simple_cte_query = f\"\"\"\n",
    "            WITH location_summary AS (\n",
    "                SELECT \n",
    "                    \"{location_cols[0]}\" as location,\n",
    "                    COUNT(*) as record_count,\n",
    "                    AVG(\"{numeric_cols[0]}\") as avg_value\n",
    "                FROM \"{schema}\".\"{table}\"\n",
    "                WHERE \"{location_cols[0]}\" IS NOT NULL \n",
    "                    AND \"{numeric_cols[0]}\" IS NOT NULL\n",
    "                GROUP BY \"{location_cols[0]}\"\n",
    "            )\n",
    "            SELECT \n",
    "                COUNT(*) as total_locations,\n",
    "                ROUND(AVG(record_count), 0) as avg_records_per_location,\n",
    "                ROUND(AVG(avg_value), 2) as overall_avg_value\n",
    "            FROM location_summary\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                simple_cte = execute_query(db_engine, simple_cte_query)\n",
    "                print(\"\\n‚úÖ Simple Location Analysis:\")\n",
    "                display(simple_cte)\n",
    "            except Exception as e2:\n",
    "                print(f\"‚ùå Simple CTE also failed: {e2}\")\n",
    "    \n",
    "    elif customer_cols and location_cols:\n",
    "        # Alternative CTE with customer and location data\n",
    "        customer_cte_query = f\"\"\"\n",
    "        WITH customer_location_stats AS (\n",
    "            SELECT \n",
    "                \"{location_cols[0]}\" as location,\n",
    "                COUNT(DISTINCT \"{customer_cols[0]}\") as unique_customers,\n",
    "                COUNT(*) as total_records\n",
    "            FROM \"{schema}\".\"{table}\"\n",
    "            WHERE \"{location_cols[0]}\" IS NOT NULL \n",
    "                AND \"{customer_cols[0]}\" IS NOT NULL\n",
    "            GROUP BY \"{location_cols[0]}\"\n",
    "        )\n",
    "        SELECT \n",
    "            location,\n",
    "            unique_customers,\n",
    "            total_records,\n",
    "            ROUND(total_records * 1.0 / unique_customers, 2) as records_per_customer\n",
    "        FROM customer_location_stats\n",
    "        ORDER BY unique_customers DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            customer_cte = execute_query(db_engine, customer_cte_query)\n",
    "            print(\"\\n‚úÖ Customer-Location Analysis (using CTEs):\")\n",
    "            display(customer_cte)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Customer CTE analysis failed: {e}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Insufficient columns for CTE segmentation analysis\")\n",
    "        print(\"Available for analysis:\")\n",
    "        print(f\"  ‚Ä¢ Numeric columns: {len(numeric_cols)}\")\n",
    "        print(f\"  ‚Ä¢ Location columns: {len(location_cols)}\")\n",
    "        print(f\"  ‚Ä¢ Customer columns: {len(customer_cols)}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No sales tables available for CTE analysis\")\n",
    "\n",
    "print(\"\\nüí° Advanced SQL Features Demonstrated:\")\n",
    "print(\"  ‚Ä¢ Window functions (LAG, OVER) for time-series analysis\")\n",
    "print(\"  ‚Ä¢ Date functions (DATE_TRUNC) for temporal grouping\")\n",
    "print(\"  ‚Ä¢ CTEs for complex multi-step business logic\")\n",
    "print(\"  ‚Ä¢ CASE statements for business rule implementation\")\n",
    "print(\"  ‚Ä¢ Subqueries for dynamic threshold calculations\")\n",
    "print(\"  ‚Ä¢ Schema-qualified table references for multi-schema databases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SQL vs Pandas: When to Use Each Approach\n",
    "\n",
    "Let's compare the strengths of SQL versus pandas for different types of data operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sql_vs_pandas_approaches():\n",
    "    \"\"\"\n",
    "    Compare SQL and pandas approaches for different types of analysis.\n",
    "    \"\"\"\n",
    "    print(\"‚ö° SQL vs Pandas: Strategic Comparison\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Example 1: Simple aggregation comparison\n",
    "    print(\"\\nüìä Example 1: Simple Aggregation\")\n",
    "    print(\"Task: Count records by category\")\n",
    "    \n",
    "    if sales_tables:\n",
    "        main_sales_table = sales_tables[0]\n",
    "        schema, table = main_sales_table.split('.', 1)\n",
    "        \n",
    "        # Find a categorical column for grouping\n",
    "        available_columns = db_info[main_sales_table]['columns']\n",
    "        location_cols = [col for col in available_columns if any(keyword in col.lower() for keyword in ['state', 'city', 'region'])]\n",
    "        \n",
    "        if location_cols:\n",
    "            print(\"\\nüóÑÔ∏è SQL Approach:\")\n",
    "            sql_agg_query = f\"\"\"\n",
    "            SELECT \n",
    "                \"{location_cols[0]}\" as category,\n",
    "                COUNT(*) as record_count,\n",
    "                ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "            FROM \"{schema}\".\"{table}\"\n",
    "            WHERE \"{location_cols[0]}\" IS NOT NULL\n",
    "            GROUP BY \"{location_cols[0]}\"\n",
    "            ORDER BY record_count DESC\n",
    "            LIMIT 5\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                sql_result = execute_query(db_engine, sql_agg_query)\n",
    "                print(\"‚úÖ SQL Result:\")\n",
    "                display(sql_result)\n",
    "                \n",
    "                print(\"\\nüêº Pandas Equivalent (conceptual):\")\n",
    "                print(\"\"\"\n",
    "                # If we had the data in a pandas DataFrame:\n",
    "                pandas_result = (\n",
    "                    df.groupby('category')['record_id']\n",
    "                    .count()\n",
    "                    .sort_values(ascending=False)\n",
    "                    .head(5)\n",
    "                )\n",
    "                # Then calculate percentages:\n",
    "                pandas_result_pct = pandas_result / pandas_result.sum() * 100\n",
    "                \"\"\")\n",
    "                \n",
    "                # Now let's actually demonstrate with the SQL result\n",
    "                if len(sql_result) > 0:\n",
    "                    print(\"\\nüîÑ Converting SQL result to pandas for further analysis:\")\n",
    "                    # Calculate additional statistics using pandas\n",
    "                    total_records = sql_result['record_count'].sum()\n",
    "                    avg_records = sql_result['record_count'].mean()\n",
    "                    std_records = sql_result['record_count'].std()\n",
    "                    \n",
    "                    print(f\"  ‚Ä¢ Total records: {total_records:,}\")\n",
    "                    print(f\"  ‚Ä¢ Average per category: {avg_records:.1f}\")\n",
    "                    print(f\"  ‚Ä¢ Standard deviation: {std_records:.1f}\")\n",
    "                    print(f\"  ‚Ä¢ Coefficient of variation: {(std_records/avg_records)*100:.1f}%\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå SQL aggregation failed: {e}\")\n",
    "    \n",
    "    # Example 2: When SQL excels\n",
    "    print(\"\\n\" + \"-\"*40)\n",
    "    print(\"\\nüìÖ Example 2: When SQL Excels - Date Operations\")\n",
    "    \n",
    "    if sales_tables:\n",
    "        main_sales_table = sales_tables[0]\n",
    "        schema, table = main_sales_table.split('.', 1)\n",
    "        available_columns = db_info[main_sales_table]['columns']\n",
    "        date_cols = [col for col in available_columns if any(keyword in col.lower() for keyword in ['date', 'timestamp', 'time'])]\n",
    "        \n",
    "        if date_cols:\n",
    "            print(\"\\nüóÑÔ∏è SQL Approach (Superior for date functions):\")\n",
    "            sql_date_query = f\"\"\"\n",
    "            SELECT \n",
    "                EXTRACT(YEAR FROM \"{date_cols[0]}\") as year,\n",
    "                EXTRACT(QUARTER FROM \"{date_cols[0]}\") as quarter,\n",
    "                COUNT(*) as quarterly_records\n",
    "            FROM \"{schema}\".\"{table}\"\n",
    "            WHERE \"{date_cols[0]}\" IS NOT NULL\n",
    "            GROUP BY EXTRACT(YEAR FROM \"{date_cols[0]}\"), EXTRACT(QUARTER FROM \"{date_cols[0]}\")\n",
    "            ORDER BY year, quarter\n",
    "            LIMIT 8\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                sql_date_result = execute_query(db_engine, sql_date_query)\n",
    "                print(\"‚úÖ SQL Date Analysis:\")\n",
    "                display(sql_date_result)\n",
    "                \n",
    "                print(\"\\nüí° SQL Advantage: Date extraction and grouping in one step\")\n",
    "                print(\"üêº Pandas equivalent would require:\")\n",
    "                print(\"  df['year'] = df['date'].dt.year\")\n",
    "                print(\"  df['quarter'] = df['date'].dt.quarter\") \n",
    "                print(\"  result = df.groupby(['year', 'quarter']).size()\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå SQL date analysis failed: {e}\")\n",
    "    \n",
    "    # Analysis summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"\\nüéØ When to Use SQL vs Pandas:\")\n",
    "    \n",
    "    print(\"\\nüóÑÔ∏è Use SQL when:\")\n",
    "    print(\"  ‚Ä¢ Working with large datasets (millions of rows)\")\n",
    "    print(\"  ‚Ä¢ Need complex JOINs across multiple tables/schemas\")\n",
    "    print(\"  ‚Ä¢ Performing set operations (UNION, INTERSECT, EXCEPT)\")\n",
    "    print(\"  ‚Ä¢ Using window functions for analytics\")\n",
    "    print(\"  ‚Ä¢ Implementing business logic with CASE statements\")\n",
    "    print(\"  ‚Ä¢ Need database-level performance optimization\")\n",
    "    print(\"  ‚Ä¢ Data lives in cloud databases (like our Supabase setup)\")\n",
    "    \n",
    "    print(\"\\nüêº Use Pandas when:\")\n",
    "    print(\"  ‚Ä¢ Dataset fits comfortably in memory\")\n",
    "    print(\"  ‚Ä¢ Need statistical analysis (correlation, regression)\")\n",
    "    print(\"  ‚Ä¢ Data cleaning and transformation tasks\")\n",
    "    print(\"  ‚Ä¢ Creating visualizations\")\n",
    "    print(\"  ‚Ä¢ Machine learning feature engineering\")\n",
    "    print(\"  ‚Ä¢ Iterative data exploration and experimentation\")\n",
    "    \n",
    "    print(\"\\nüîÑ Best Practice: Hybrid Approach\")\n",
    "    print(\"  1. Use SQL for data extraction and initial processing\")\n",
    "    print(\"  2. Use pandas for analysis, statistics, and visualization\")\n",
    "    print(\"  3. Leverage each tool's strengths for optimal performance\")\n",
    "    print(\"  4. Handle schema-qualified table names in SQL\")\n",
    "    print(\"  5. Convert SQL results to pandas for advanced analytics\")\n",
    "    \n",
    "    return \"SQL excels at data processing, pandas excels at analysis\"\n",
    "\n",
    "# Run the comparison\n",
    "comparison_insights = compare_sql_vs_pandas_approaches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Handling and Best Practices\n",
    "\n",
    "Production database applications require robust error handling and connection management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_error_handling():\n",
    "    \"\"\"\n",
    "    Demonstrate proper error handling techniques for database operations.\n",
    "    \"\"\"\n",
    "    print(\"üõ°Ô∏è Database Error Handling and Best Practices\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Example 1: Handling SQL syntax errors\n",
    "    print(\"\\n‚ùå Example 1: SQL Syntax Error Handling\")\n",
    "    try:\n",
    "        # Intentional syntax error\n",
    "        result = execute_query(db_engine, \"\"\"\n",
    "            SELCT * FROM \"non_existent_schema\".\"non_existent_table\"  -- Missing 'E' in SELECT\n",
    "            WHERE some_column = 'value'\n",
    "            LIMIT 5\n",
    "        \"\"\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úÖ Caught SQL syntax error: {type(e).__name__}\")\n",
    "        print(f\"   Error message: {str(e)[:100]}...\")\n",
    "    \n",
    "    # Example 2: Handling schema/table not found\n",
    "    print(\"\\nüîç Example 2: Schema/Table Not Found Error\")\n",
    "    try:\n",
    "        if sales_tables:\n",
    "            main_sales_table = sales_tables[0]\n",
    "            schema, table = main_sales_table.split('.', 1)\n",
    "            result = execute_query(db_engine, f\"\"\"\n",
    "                SELECT customer_id, nonexistent_column \n",
    "                FROM \"{schema}\".\"nonexistent_table\"\n",
    "                LIMIT 5\n",
    "            \"\"\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úÖ Caught schema/table error: {type(e).__name__}\")\n",
    "        print(f\"   Error message: {str(e)[:100]}...\")\n",
    "    \n",
    "    # Example 3: Parameterized queries (SQL injection prevention)\n",
    "    print(\"\\nüîí Example 3: Safe Parameterized Queries\")\n",
    "    \n",
    "    def safe_data_lookup(engine, table_name, column_name, value):\n",
    "        \"\"\"\n",
    "        Safely query data using parameterized queries.\n",
    "        Note: Schema and table names can't be parameterized, so validate them first.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate table exists (schema-qualified)\n",
    "            if table_name not in db_info:\n",
    "                raise ValueError(f\"Table {table_name} not found\")\n",
    "            \n",
    "            # Validate column exists\n",
    "            if column_name not in db_info[table_name]['columns']:\n",
    "                raise ValueError(f\"Column {column_name} not found in {table_name}\")\n",
    "            \n",
    "            # Split schema and table for proper quoting\n",
    "            if '.' in table_name:\n",
    "                schema, table = table_name.split('.', 1)\n",
    "                table_ref = f'\"{schema}\".\"{table}\"'\n",
    "            else:\n",
    "                table_ref = f'\"{table_name}\"'\n",
    "            \n",
    "            # Use parameterized query for the value\n",
    "            query = f\"\"\"\n",
    "                SELECT COUNT(*) as record_count\n",
    "                FROM {table_ref}\n",
    "                WHERE \"{column_name}\" = %(search_value)s\n",
    "            \"\"\"\n",
    "            result = execute_query(engine, query, params={'search_value': value})\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Query failed: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    # Test safe query\n",
    "    if sales_tables:\n",
    "        main_sales_table = sales_tables[0]\n",
    "        available_columns = db_info[main_sales_table]['columns']\n",
    "        location_cols = [col for col in available_columns if any(keyword in col.lower() for keyword in ['state', 'city', 'region'])]\n",
    "        \n",
    "        if location_cols:\n",
    "            # Get a real value first\n",
    "            schema, table = main_sales_table.split('.', 1)\n",
    "            sample_query = f'SELECT DISTINCT \"{location_cols[0]}\" FROM \"{schema}\".\"{table}\" WHERE \"{location_cols[0]}\" IS NOT NULL LIMIT 1'\n",
    "            try:\n",
    "                sample_value = execute_query(db_engine, sample_query)\n",
    "                if len(sample_value) > 0:\n",
    "                    test_value = sample_value.iloc[0, 0]\n",
    "                    safe_result = safe_data_lookup(db_engine, main_sales_table, location_cols[0], test_value)\n",
    "                    if len(safe_result) > 0:\n",
    "                        print(f\"‚úÖ Safe query returned {safe_result.iloc[0, 0]} records for '{test_value}'\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not test safe query: {e}\")\n",
    "    \n",
    "    # Example 4: Connection management with context managers\n",
    "    print(\"\\nüîå Example 4: Proper Connection Management\")\n",
    "    \n",
    "    def safe_database_query(engine, query, params=None):\n",
    "        \"\"\"\n",
    "        Function-based safe database operations using context manager pattern.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with engine.connect() as conn:\n",
    "                if params:\n",
    "                    result = pd.read_sql(text(query), conn, params=params)\n",
    "                else:\n",
    "                    result = pd.read_sql(text(query), conn)\n",
    "                return result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Database error occurred: {type(e).__name__}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    # Use function for safe operations\n",
    "    try:\n",
    "        result = safe_database_query(\n",
    "            db_engine, \n",
    "            \"SELECT 'Schema-aware connection test successful' as message\"\n",
    "        )\n",
    "        if not result.empty:\n",
    "            print(f\"‚úÖ Safe query function successful: {result.iloc[0, 0]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Safe query function caught error: {e}\")\n",
    "    \n",
    "    # Example 5: Data validation\n",
    "    print(\"\\n‚úÖ Example 5: Data Validation Best Practices\")\n",
    "    \n",
    "    def validate_query_result(df, expected_columns=None, min_rows=0):\n",
    "        \"\"\"\n",
    "        Validate query results meet business requirements.\n",
    "        \"\"\"\n",
    "        validations = []\n",
    "        \n",
    "        # Check if DataFrame is empty\n",
    "        if df.empty:\n",
    "            validations.append(\"‚ùå Query returned no data\")\n",
    "        else:\n",
    "            validations.append(f\"‚úÖ Query returned {len(df):,} rows\")\n",
    "        \n",
    "        # Check minimum row count\n",
    "        if len(df) < min_rows:\n",
    "            validations.append(f\"‚ö†Ô∏è Row count ({len(df)}) below minimum ({min_rows})\")\n",
    "        \n",
    "        # Check expected columns\n",
    "        if expected_columns:\n",
    "            missing_cols = set(expected_columns) - set(df.columns)\n",
    "            if missing_cols:\n",
    "                validations.append(f\"‚ùå Missing columns: {missing_cols}\")\n",
    "            else:\n",
    "                validations.append(\"‚úÖ All expected columns present\")\n",
    "        \n",
    "        # Check for null values in key columns\n",
    "        if not df.empty:\n",
    "            null_counts = df.isnull().sum()\n",
    "            if null_counts.any():\n",
    "                validations.append(f\"‚ö†Ô∏è Null values found: {dict(null_counts[null_counts > 0])}\")\n",
    "            else:\n",
    "                validations.append(\"‚úÖ No null values detected\")\n",
    "        \n",
    "        return validations\n",
    "    \n",
    "    # Test validation\n",
    "    if sales_tables:\n",
    "        main_sales_table = sales_tables[0]\n",
    "        schema, table = main_sales_table.split('.', 1)\n",
    "        \n",
    "        test_query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM \"{schema}\".\"{table}\"\n",
    "            LIMIT 10\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            test_data = execute_query(db_engine, test_query)\n",
    "            validations = validate_query_result(\n",
    "                test_data, \n",
    "                expected_columns=list(test_data.columns)[:3],  # Check first 3 columns\n",
    "                min_rows=5\n",
    "            )\n",
    "            \n",
    "            print(\"Query validation results:\")\n",
    "            for validation in validations:\n",
    "                print(f\"  {validation}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Validation test failed: {e}\")\n",
    "    \n",
    "    return validations\n",
    "\n",
    "# Run error handling demonstration\n",
    "error_handling_results = demonstrate_error_handling()\n",
    "\n",
    "print(\"\\nüìö Database Best Practices Summary:\")\n",
    "print(\"  üîí Always use parameterized queries to prevent SQL injection\")\n",
    "print(\"  üõ°Ô∏è Implement comprehensive error handling for all database operations\")\n",
    "print(\"  üîå Use connection context managers to ensure proper resource cleanup\")\n",
    "print(\"  ‚úÖ Validate query results before processing in business logic\")\n",
    "print(\"  üìä Log query performance for optimization opportunities\")\n",
    "print(\"  üîÑ Implement retry logic for transient connection issues\")\n",
    "print(\"  üìù Document query patterns and business logic for team maintenance\")\n",
    "print(\"  üóÇÔ∏è Handle schema-qualified table names properly in multi-schema databases\")\n",
    "print(\"  üîç Validate schema and table existence before executing queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways and Next Steps\n",
    "\n",
    "### What We've Accomplished:\n",
    "\n",
    "1. **PostgreSQL Database Connection**\n",
    "   - Connected to Supabase cloud PostgreSQL database\n",
    "   - Established professional connection patterns with SQLAlchemy\n",
    "   - Implemented proper resource management and error handling\n",
    "\n",
    "2. **SQL Query Execution from Python**\n",
    "   - Basic data exploration and filtering\n",
    "   - Complex business intelligence with JOINs\n",
    "   - Advanced analytics with window functions and CTEs\n",
    "\n",
    "3. **Real-World Data Integration**\n",
    "   - Worked with actual Olist e-commerce and marketing datasets\n",
    "   - Adapted queries to real schema structures\n",
    "   - Handled data quality issues and missing values\n",
    "\n",
    "4. **Production-Ready Practices**\n",
    "   - Error handling and validation\n",
    "   - Parameterized queries for security\n",
    "   - Connection pooling and resource management\n",
    "\n",
    "### Business Value:\n",
    "\n",
    "- **Real-time Analysis**: Connect directly to live business systems\n",
    "- **Scalability**: Handle enterprise-scale datasets efficiently\n",
    "- **Performance**: Leverage database engines for heavy computation\n",
    "- **Security**: Proper authentication and query sanitization\n",
    "- **Collaboration**: Multiple analysts accessing the same cloud data source\n",
    "\n",
    "### When to Use SQL vs Pandas:\n",
    "\n",
    "**Use SQL for:**\n",
    "- Data extraction from large datasets\n",
    "- Complex joins across multiple tables\n",
    "- Window functions and analytical queries\n",
    "- Business logic implementation with CASE statements\n",
    "- Database-level performance optimization\n",
    "\n",
    "**Use Pandas for:**\n",
    "- Statistical analysis and modeling\n",
    "- Data cleaning and transformation\n",
    "- Visualization preparation\n",
    "- Machine learning feature engineering\n",
    "- Iterative data exploration\n",
    "\n",
    "### Next Session Preview:\n",
    "In our next sessions, we'll explore:\n",
    "- Advanced SQL patterns for business intelligence\n",
    "- Real-time data pipeline automation\n",
    "- Combining SQL analytics with interactive visualizations\n",
    "- Building automated reporting systems\n",
    "\n",
    "**üéâ You now have the fundamental skills to connect Python to cloud databases and perform enterprise-level data analysis!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practice Exercise\n",
    "\n",
    "**Your Challenge! üöÄ**\n",
    "\n",
    "**Business Scenario**: The Olist analytics team wants to understand the relationship between their marketing efforts and customer behavior. Your task is to create an analysis that bridges the marketing and sales datasets.\n",
    "\n",
    "**Your Task**: Create a comprehensive analysis that combines both datasets to answer business questions.\n",
    "\n",
    "**Requirements**:\n",
    "1. **Data Exploration**: Explore both olist_sales_data_set and olist_marketing_data_set\n",
    "2. **Schema Analysis**: Document the structure and relationships between datasets\n",
    "3. **Business Intelligence**: Create queries that provide actionable insights\n",
    "4. **Error Handling**: Implement proper error handling for your queries\n",
    "5. **Best Practices**: Use parameterized queries and validation\n",
    "\n",
    "**Specific Questions to Answer**:\n",
    "- What is the structure of each dataset?\n",
    "- How can these datasets be connected?\n",
    "- What insights can we derive about customer acquisition and behavior?\n",
    "- Which marketing channels or strategies show the most promise?\n",
    "\n",
    "**Deliverable**: A comprehensive analysis with SQL queries, data validation, and business insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your practice exercise solution here\n",
    "\n",
    "def comprehensive_business_analysis():\n",
    "    \"\"\"\n",
    "    Your challenge: Create a comprehensive analysis bridging marketing and sales data.\n",
    "    \n",
    "    Business Goal: Understand the relationship between marketing efforts and customer behavior.\n",
    "    \n",
    "    Implementation steps:\n",
    "    1. Explore both datasets thoroughly\n",
    "    2. Identify connection points between datasets\n",
    "    3. Create business intelligence queries\n",
    "    4. Validate results and handle errors\n",
    "    5. Generate actionable insights\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üéØ Comprehensive Business Analysis Challenge\")\n",
    "    print(\"üìä Goal: Bridge marketing and sales data for business insights\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Step 1: Dataset Exploration\n",
    "    print(\"\\nüìã Step 1: Dataset Structure Analysis\")\n",
    "    \n",
    "    # TODO: Explore olist_sales_data_set structure\n",
    "    # Think about: What columns are available? What do they represent?\n",
    "    \n",
    "    # TODO: Explore olist_marketing_data_set structure  \n",
    "    # Think about: How does this relate to sales data?\n",
    "    \n",
    "    # Step 2: Connection Analysis\n",
    "    print(\"\\nüîó Step 2: Identify Dataset Relationships\")\n",
    "    \n",
    "    # TODO: Find common columns or keys between datasets\n",
    "    # Think about: How can we join these datasets?\n",
    "    \n",
    "    # Step 3: Business Intelligence Queries\n",
    "    print(\"\\nüíº Step 3: Business Intelligence Analysis\")\n",
    "    \n",
    "    # TODO: Create queries that answer business questions:\n",
    "    # - Which marketing channels are most effective?\n",
    "    # - What's the customer journey from lead to purchase?\n",
    "    # - How do marketing efforts correlate with sales performance?\n",
    "    \n",
    "    # Step 4: Advanced Analytics\n",
    "    print(\"\\nüìà Step 4: Advanced Business Insights\")\n",
    "    \n",
    "    # TODO: Use advanced SQL features:\n",
    "    # - Window functions for trend analysis\n",
    "    # - CTEs for complex business logic\n",
    "    # - Statistical functions for performance metrics\n",
    "    \n",
    "    # Step 5: Validation and Error Handling\n",
    "    print(\"\\n‚úÖ Step 5: Data Validation and Quality Checks\")\n",
    "    \n",
    "    # TODO: Implement proper error handling and data validation\n",
    "    \n",
    "    # Step 6: Business Recommendations\n",
    "    print(\"\\nüéØ Step 6: Strategic Business Recommendations\")\n",
    "    \n",
    "    # TODO: Synthesize findings into actionable business insights\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"üí° Hints for Your Analysis:\")\n",
    "print(\"  ‚Ä¢ Start by examining the schema of both datasets\")\n",
    "print(\"  ‚Ä¢ Look for common identifiers (seller_id, customer_id, etc.)\")\n",
    "print(\"  ‚Ä¢ Use SQL JOINs to combine datasets where appropriate\")\n",
    "print(\"  ‚Ä¢ Focus on metrics that matter to business decision-makers\")\n",
    "print(\"  ‚Ä¢ Always validate your results and handle potential errors\")\n",
    "\n",
    "print(\"\\nüîç Analysis Framework:\")\n",
    "print(\"  1. Data Discovery: Understand what data is available\")\n",
    "print(\"  2. Relationship Mapping: How datasets connect\")\n",
    "print(\"  3. Business Metrics: What KPIs can we calculate?\")\n",
    "print(\"  4. Trend Analysis: How do metrics change over time?\")\n",
    "print(\"  5. Insights Generation: What actions should the business take?\")\n",
    "\n",
    "print(\"\\nüìä Expected Deliverables:\")\n",
    "print(\"  ‚Ä¢ Dataset structure documentation\")\n",
    "print(\"  ‚Ä¢ Relationship mapping between datasets\")\n",
    "print(\"  ‚Ä¢ Business intelligence SQL queries\")\n",
    "print(\"  ‚Ä¢ Data quality assessment\")\n",
    "print(\"  ‚Ä¢ Strategic recommendations based on findings\")\n",
    "\n",
    "# Uncomment to run your solution:\n",
    "# comprehensive_analysis_results = comprehensive_business_analysis()\n",
    "\n",
    "# Remember to clean up database connection when done\n",
    "# db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up database connection\n",
    "print(\"üîí Closing database connection...\")\n",
    "close_database_engine(db_engine)\n",
    "print(\"‚úÖ Session complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
