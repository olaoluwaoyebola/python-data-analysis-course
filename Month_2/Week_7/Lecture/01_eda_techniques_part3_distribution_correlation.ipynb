{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "distribution_correlation_title"
      },
      "source": [
        "# Week 7: EDA Techniques - Part 3: Distribution Analysis and Correlation Exploration\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this session, you will be able to:\n",
        "- Perform advanced distribution analysis using statistical tests\n",
        "- Create sophisticated correlation visualizations and interpretations\n",
        "- Apply probability distributions to business scenarios\n",
        "- Use statistical tests to validate business hypotheses\n",
        "- Build interactive visualizations for deeper data exploration\n",
        "\n",
        "## Business Context\n",
        "In this final part of our EDA techniques series, we focus on **advanced statistical analysis** to uncover deeper patterns in our Olist e-commerce data. This session emphasizes rigorous statistical validation and sophisticated visualization techniques.\n",
        "\n",
        "**Key Business Questions:**\n",
        "- What statistical distributions best model our business metrics?\n",
        "- Which variables have the strongest predictive relationships?\n",
        "- How can we validate our business assumptions statistically?\n",
        "- What advanced patterns exist in our customer behavior data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section_part3"
      },
      "source": [
        "## 1. Advanced Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "advanced_imports"
      },
      "outputs": [],
      "source": [
        "# Standard data analysis imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Advanced statistical analysis\n",
        "from scipy import stats\n",
        "from scipy.stats import (\n",
        "    normaltest, jarque_bera, shapiro, anderson,\n",
        "    kstest, mannwhitneyu, kruskal, spearmanr, kendalltau,\n",
        "    chi2_contingency, pearsonr\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "\n",
        "# Interactive plotting\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.offline as pyo\n",
        "pyo.init_notebook_mode(connected=True)\n",
        "\n",
        "# Database connection\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# Warnings and display settings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.float_format', '{:.4f}'.format)\n",
        "\n",
        "# Enhanced plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "print(\"âœ… Advanced environment setup complete for distribution and correlation analysis!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_comprehensive_data"
      },
      "outputs": [],
      "source": [
        "# Load comprehensive dataset for advanced analysis\n",
        "DATABASE_URL = \"postgresql://postgres.pzykoxdiwsyclwfqfiii:L3tMeQuery123!@aws-0-us-east-1.pooler.supabase.com:6543/postgres\"\n",
        "engine = create_engine(DATABASE_URL)\n",
        "\n",
        "print(\"ðŸ”„ Loading comprehensive dataset for advanced analysis...\")\n",
        "\n",
        "# Comprehensive business query with advanced metrics\n",
        "comprehensive_query = \"\"\"\n",
        "WITH order_metrics AS (\n",
        "    SELECT \n",
        "        o.order_id,\n",
        "        o.customer_id,\n",
        "        o.order_purchase_timestamp,\n",
        "        EXTRACT(YEAR FROM o.order_purchase_timestamp) as order_year,\n",
        "        EXTRACT(MONTH FROM o.order_purchase_timestamp) as order_month,\n",
        "        EXTRACT(DOW FROM o.order_purchase_timestamp) as order_dow,\n",
        "        EXTRACT(HOUR FROM o.order_purchase_timestamp) as order_hour,\n",
        "        DATE_PART('day', o.order_delivered_customer_date - o.order_purchase_timestamp) as delivery_days,\n",
        "        c.customer_state,\n",
        "        c.customer_city\n",
        "    FROM olist_sales_data_set.olist_orders_dataset o\n",
        "    JOIN olist_sales_data_set.olist_customers_dataset c ON o.customer_id = c.customer_id\n",
        "    WHERE o.order_status = 'delivered'\n",
        "    AND o.order_delivered_customer_date IS NOT NULL\n",
        ")\n",
        "SELECT \n",
        "    om.*,\n",
        "    oi.product_id,\n",
        "    oi.price,\n",
        "    oi.freight_value,\n",
        "    (oi.price + oi.freight_value) as total_item_value,\n",
        "    oi.freight_value / NULLIF(oi.price, 0) as freight_ratio,\n",
        "    (oi.price - oi.freight_value) / NULLIF(oi.price, 0) as profit_margin,\n",
        "    p.product_weight_g,\n",
        "    p.product_length_cm,\n",
        "    p.product_height_cm,\n",
        "    p.product_width_cm,\n",
        "    (p.product_length_cm * p.product_height_cm * p.product_width_cm) / 1000.0 as product_volume_liters,\n",
        "    COALESCE(pt.product_category_name_english, p.product_category_name) as category_english,\n",
        "    r.review_score\n",
        "FROM order_metrics om\n",
        "JOIN olist_sales_data_set.olist_order_items_dataset oi ON om.order_id = oi.order_id\n",
        "JOIN olist_sales_data_set.olist_products_dataset p ON oi.product_id = p.product_id\n",
        "LEFT JOIN olist_sales_data_set.product_category_name_translation pt \n",
        "    ON p.product_category_name = pt.product_category_name\n",
        "LEFT JOIN olist_sales_data_set.olist_order_reviews_dataset r ON om.order_id = r.order_id\n",
        "WHERE oi.price > 0 AND oi.freight_value >= 0\n",
        "LIMIT 20000\n",
        "\"\"\"\n",
        "\n",
        "# Load data\n",
        "df = pd.read_sql(comprehensive_query, engine)\n",
        "\n",
        "# Data preprocessing\n",
        "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
        "df['category_clean'] = df['category_english'].fillna('Unknown').str.title()\n",
        "df['price_log'] = np.log1p(df['price'])\n",
        "df['volume_log'] = np.log1p(df['product_volume_liters'].fillna(0))\n",
        "\n",
        "# Remove extreme outliers for better analysis\n",
        "price_q99 = df['price'].quantile(0.99)\n",
        "df = df[df['price'] <= price_q99].copy()\n",
        "\n",
        "print(f\"âœ… Comprehensive dataset loaded:\")\n",
        "print(f\"   ðŸ“Š Records: {len(df):,}\")\n",
        "print(f\"   ðŸ“‹ Features: {df.shape[1]}\")\n",
        "print(f\"   ðŸ·ï¸ Categories: {df['category_clean'].nunique()}\")\n",
        "print(f\"   ðŸ“… Date range: {df['order_purchase_timestamp'].min().date()} to {df['order_purchase_timestamp'].max().date()}\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\nðŸ“‹ Sample Data:\")\n",
        "display(df[['order_id', 'category_clean', 'price', 'delivery_days', 'review_score']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "distribution_analysis_section"
      },
      "source": [
        "## 2. Advanced Distribution Analysis\n",
        "\n",
        "Let's perform comprehensive distribution analysis to understand the statistical properties of our business metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "normality_testing"
      },
      "outputs": [],
      "source": [
        "# Comprehensive Normality Testing Suite\n",
        "def comprehensive_normality_tests(data, variable_name, sample_size=5000):\n",
        "    \"\"\"\n",
        "    Perform comprehensive normality tests on a variable\n",
        "    \"\"\"\n",
        "    print(f\"\\nðŸ§ª Normality Tests for {variable_name}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Sample data if too large\n",
        "    if len(data) > sample_size:\n",
        "        test_data = data.sample(sample_size, random_state=42)\n",
        "        print(f\"Note: Using random sample of {sample_size:,} observations\")\n",
        "    else:\n",
        "        test_data = data\n",
        "    \n",
        "    # Remove NaN values\n",
        "    test_data = test_data.dropna()\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    # Shapiro-Wilk Test (best for small samples)\n",
        "    if len(test_data) <= 5000:\n",
        "        sw_stat, sw_p = shapiro(test_data)\n",
        "        results['Shapiro-Wilk'] = {'statistic': sw_stat, 'p_value': sw_p}\n",
        "    \n",
        "    # D'Agostino-Pearson Test\n",
        "    dp_stat, dp_p = normaltest(test_data)\n",
        "    results['D\\'Agostino-Pearson'] = {'statistic': dp_stat, 'p_value': dp_p}\n",
        "    \n",
        "    # Jarque-Bera Test\n",
        "    jb_stat, jb_p = jarque_bera(test_data)\n",
        "    results['Jarque-Bera'] = {'statistic': jb_stat, 'p_value': jb_p}\n",
        "    \n",
        "    # Anderson-Darling Test\n",
        "    ad_result = anderson(test_data, dist='norm')\n",
        "    results['Anderson-Darling'] = {\n",
        "        'statistic': ad_result.statistic,\n",
        "        'critical_values': ad_result.critical_values,\n",
        "        'significance_levels': ad_result.significance_levels\n",
        "    }\n",
        "    \n",
        "    # Kolmogorov-Smirnov Test\n",
        "    # Standardize data for comparison with standard normal\n",
        "    standardized_data = (test_data - test_data.mean()) / test_data.std()\n",
        "    ks_stat, ks_p = kstest(standardized_data, 'norm')\n",
        "    results['Kolmogorov-Smirnov'] = {'statistic': ks_stat, 'p_value': ks_p}\n",
        "    \n",
        "    # Display results\n",
        "    print(\"\\nðŸ“Š Test Results (H0: Data is normally distributed):\")\n",
        "    alpha = 0.05\n",
        "    \n",
        "    for test_name, result in results.items():\n",
        "        if test_name == 'Anderson-Darling':\n",
        "            # Special handling for Anderson-Darling\n",
        "            stat = result['statistic']\n",
        "            critical_5pct = result['critical_values'][2]  # 5% significance level\n",
        "            reject = stat > critical_5pct\n",
        "            print(f\"   {test_name:20}: statistic = {stat:.4f}, critical(5%) = {critical_5pct:.4f}\")\n",
        "            print(f\"   {'':20}  {'Reject H0' if reject else 'Fail to reject H0'} (not normal)\" if reject else f\"   {'':20}  {'Fail to reject H0'} (possibly normal)\")\n",
        "        else:\n",
        "            stat = result['statistic']\n",
        "            p_val = result['p_value']\n",
        "            reject = p_val < alpha\n",
        "            print(f\"   {test_name:20}: statistic = {stat:.4f}, p-value = {p_val:.2e}\")\n",
        "            print(f\"   {'':20}  {'Reject H0' if reject else 'Fail to reject H0'} ({'not normal' if reject else 'possibly normal'})\")\n",
        "    \n",
        "    # Summary\n",
        "    normality_tests = ['Shapiro-Wilk', 'D\\'Agostino-Pearson', 'Jarque-Bera', 'Kolmogorov-Smirnov']\n",
        "    reject_count = sum(1 for test in normality_tests if test in results and results[test]['p_value'] < alpha)\n",
        "    \n",
        "    # Add Anderson-Darling result\n",
        "    if 'Anderson-Darling' in results:\n",
        "        ad_reject = results['Anderson-Darling']['statistic'] > results['Anderson-Darling']['critical_values'][2]\n",
        "        if ad_reject:\n",
        "            reject_count += 1\n",
        "    \n",
        "    total_tests = len(results)\n",
        "    \n",
        "    print(f\"\\nðŸ’¡ Summary: {reject_count}/{total_tests} tests reject normality\")\n",
        "    if reject_count >= total_tests * 0.5:\n",
        "        print(f\"   â†’ Strong evidence that {variable_name} is NOT normally distributed\")\n",
        "    else:\n",
        "        print(f\"   â†’ Weak evidence against normality for {variable_name}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Test key business variables\n",
        "key_variables = ['price', 'freight_value', 'delivery_days', 'product_weight_g']\n",
        "\n",
        "normality_results = {}\n",
        "for var in key_variables:\n",
        "    if var in df.columns and df[var].notna().sum() > 100:\n",
        "        normality_results[var] = comprehensive_normality_tests(df[var], var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "distribution_fitting"
      },
      "outputs": [],
      "source": [
        "# Distribution Fitting Analysis\n",
        "print(\"ðŸ“ˆ Distribution Fitting Analysis\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "def fit_distributions(data, distributions=None):\n",
        "    \"\"\"\n",
        "    Fit multiple probability distributions and find the best fit\n",
        "    \"\"\"\n",
        "    if distributions is None:\n",
        "        distributions = [\n",
        "            stats.norm, stats.expon, stats.gamma, stats.lognorm, \n",
        "            stats.beta, stats.uniform, stats.pareto\n",
        "        ]\n",
        "    \n",
        "    # Remove NaN values\n",
        "    clean_data = data.dropna()\n",
        "    \n",
        "    # Sample if data is too large\n",
        "    if len(clean_data) > 10000:\n",
        "        clean_data = clean_data.sample(10000, random_state=42)\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for dist in distributions:\n",
        "        try:\n",
        "            # Fit distribution\n",
        "            params = dist.fit(clean_data)\n",
        "            \n",
        "            # Calculate goodness of fit using Kolmogorov-Smirnov test\n",
        "            ks_stat, ks_p = kstest(clean_data, lambda x: dist.cdf(x, *params))\n",
        "            \n",
        "            # Calculate AIC (Akaike Information Criterion)\n",
        "            log_likelihood = np.sum(dist.logpdf(clean_data, *params))\n",
        "            aic = 2 * len(params) - 2 * log_likelihood\n",
        "            \n",
        "            results.append({\n",
        "                'distribution': dist.name,\n",
        "                'parameters': params,\n",
        "                'ks_statistic': ks_stat,\n",
        "                'ks_p_value': ks_p,\n",
        "                'aic': aic,\n",
        "                'log_likelihood': log_likelihood\n",
        "            })\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   Warning: Could not fit {dist.name}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Sort by AIC (lower is better)\n",
        "    results = sorted(results, key=lambda x: x['aic'])\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Fit distributions for price data\n",
        "print(\"\\nðŸ’° Price Distribution Fitting:\")\n",
        "price_fits = fit_distributions(df['price'])\n",
        "\n",
        "print(\"\\nðŸ† Top 5 Best-Fitting Distributions (by AIC):\")\n",
        "for i, result in enumerate(price_fits[:5]):\n",
        "    print(f\"{i+1}. {result['distribution'].title():12} - AIC: {result['aic']:.2f}, KS p-value: {result['ks_p_value']:.4f}\")\n",
        "\n",
        "# Visualize distribution fits\n",
        "def plot_distribution_comparison(data, fit_results, title, top_n=3):\n",
        "    \"\"\"\n",
        "    Plot original data against fitted distributions\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle(f'{title} - Distribution Fitting Comparison', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Clean data\n",
        "    clean_data = data.dropna()\n",
        "    if len(clean_data) > 5000:\n",
        "        clean_data = clean_data.sample(5000, random_state=42)\n",
        "    \n",
        "    # Original histogram\n",
        "    axes[0, 0].hist(clean_data, bins=50, density=True, alpha=0.7, color='lightblue', label='Original Data')\n",
        "    axes[0, 0].set_title('Original Data Distribution')\n",
        "    axes[0, 0].set_xlabel('Value')\n",
        "    axes[0, 0].set_ylabel('Density')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Fitted distributions\n",
        "    x = np.linspace(clean_data.min(), clean_data.max(), 1000)\n",
        "    \n",
        "    for i, result in enumerate(fit_results[:top_n]):\n",
        "        dist_name = result['distribution']\n",
        "        params = result['parameters']\n",
        "        \n",
        "        try:\n",
        "            # Get distribution object\n",
        "            dist = getattr(stats, dist_name)\n",
        "            \n",
        "            # Calculate PDF\n",
        "            pdf = dist.pdf(x, *params)\n",
        "            \n",
        "            # Plot on different subplots\n",
        "            if i == 0:\n",
        "                ax = axes[0, 1]\n",
        "            elif i == 1:\n",
        "                ax = axes[1, 0]\n",
        "            else:\n",
        "                ax = axes[1, 1]\n",
        "            \n",
        "            ax.hist(clean_data, bins=50, density=True, alpha=0.5, color='lightgray', label='Data')\n",
        "            ax.plot(x, pdf, 'r-', linewidth=2, label=f'{dist_name.title()} Fit')\n",
        "            ax.set_title(f'Best Fit #{i+1}: {dist_name.title()}\\nAIC: {result[\"aic\"]:.2f}')\n",
        "            ax.set_xlabel('Value')\n",
        "            ax.set_ylabel('Density')\n",
        "            ax.legend()\n",
        "            ax.grid(True, alpha=0.3)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Could not plot {dist_name}: {e}\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot price distribution fits\n",
        "plot_distribution_comparison(df['price'], price_fits, 'Price', top_n=3)\n",
        "\n",
        "# Business interpretation\n",
        "best_fit = price_fits[0]\n",
        "print(f\"\\nðŸ’¡ Business Insights - Price Distribution:\")\n",
        "print(f\"   â€¢ Best fitting distribution: {best_fit['distribution'].title()}\")\n",
        "print(f\"   â€¢ This suggests price data follows a {best_fit['distribution']} pattern\")\n",
        "\n",
        "if best_fit['distribution'] == 'lognorm':\n",
        "    print(f\"   â€¢ Log-normal distribution indicates multiplicative processes (common in pricing)\")\n",
        "elif best_fit['distribution'] == 'gamma':\n",
        "    print(f\"   â€¢ Gamma distribution suggests skewed data with a lower bound (typical for prices)\")\n",
        "elif best_fit['distribution'] == 'expon':\n",
        "    print(f\"   â€¢ Exponential distribution indicates most items are low-priced with few expensive items\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "transformation_analysis"
      },
      "source": [
        "## 3. Data Transformation Analysis\n",
        "\n",
        "Let's explore how different transformations can improve the normality of our business metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "transformation_comparison"
      },
      "outputs": [],
      "source": [
        "# Data Transformation Analysis\n",
        "print(\"ðŸ”„ Data Transformation Analysis\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "def compare_transformations(data, variable_name):\n",
        "    \"\"\"\n",
        "    Compare different transformations for normality improvement\n",
        "    \"\"\"\n",
        "    # Remove negative values and zeros for log transformations\n",
        "    positive_data = data[data > 0].dropna()\n",
        "    \n",
        "    if len(positive_data) < 100:\n",
        "        print(f\"Insufficient positive data for {variable_name}\")\n",
        "        return\n",
        "    \n",
        "    # Sample if too large\n",
        "    if len(positive_data) > 5000:\n",
        "        positive_data = positive_data.sample(5000, random_state=42)\n",
        "    \n",
        "    transformations = {\n",
        "        'Original': positive_data,\n",
        "        'Log': np.log(positive_data),\n",
        "        'Log1p': np.log1p(positive_data),\n",
        "        'Square Root': np.sqrt(positive_data),\n",
        "        'Box-Cox': None,  # Will be computed below\n",
        "        'Yeo-Johnson': None  # Will be computed below\n",
        "    }\n",
        "    \n",
        "    # Box-Cox transformation (requires positive data)\n",
        "    try:\n",
        "        transformed_boxcox, lambda_boxcox = stats.boxcox(positive_data)\n",
        "        transformations['Box-Cox'] = transformed_boxcox\n",
        "    except:\n",
        "        print(f\"   Warning: Box-Cox transformation failed for {variable_name}\")\n",
        "    \n",
        "    # Yeo-Johnson transformation (works with negative data)\n",
        "    try:\n",
        "        pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
        "        transformed_yj = pt.fit_transform(positive_data.values.reshape(-1, 1)).flatten()\n",
        "        transformations['Yeo-Johnson'] = transformed_yj\n",
        "    except:\n",
        "        print(f\"   Warning: Yeo-Johnson transformation failed for {variable_name}\")\n",
        "    \n",
        "    # Calculate normality statistics for each transformation\n",
        "    results = []\n",
        "    \n",
        "    for trans_name, trans_data in transformations.items():\n",
        "        if trans_data is None:\n",
        "            continue\n",
        "            \n",
        "        try:\n",
        "            # Basic statistics\n",
        "            skewness = stats.skew(trans_data)\n",
        "            kurtosis = stats.kurtosis(trans_data)\n",
        "            \n",
        "            # Shapiro-Wilk test (if sample size allows)\n",
        "            if len(trans_data) <= 5000:\n",
        "                sw_stat, sw_p = shapiro(trans_data)\n",
        "            else:\n",
        "                sw_stat, sw_p = np.nan, np.nan\n",
        "            \n",
        "            # Jarque-Bera test\n",
        "            jb_stat, jb_p = jarque_bera(trans_data)\n",
        "            \n",
        "            results.append({\n",
        "                'Transformation': trans_name,\n",
        "                'Skewness': skewness,\n",
        "                'Kurtosis': kurtosis,\n",
        "                'Shapiro_W': sw_stat,\n",
        "                'Shapiro_p': sw_p,\n",
        "                'JB_stat': jb_stat,\n",
        "                'JB_p': jb_p\n",
        "            })\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   Warning: Error processing {trans_name}: {e}\")\n",
        "    \n",
        "    # Convert to DataFrame and display\n",
        "    results_df = pd.DataFrame(results)\n",
        "    \n",
        "    print(f\"\\nðŸ“Š Transformation Comparison for {variable_name}:\")\n",
        "    display(results_df.round(4))\n",
        "    \n",
        "    # Recommend best transformation\n",
        "    # Score based on: lower absolute skewness + higher normality test p-values\n",
        "    results_df['normality_score'] = (\n",
        "        -np.abs(results_df['Skewness']) + \n",
        "        results_df['JB_p'].fillna(0) + \n",
        "        results_df['Shapiro_p'].fillna(0)\n",
        "    )\n",
        "    \n",
        "    best_transformation = results_df.loc[results_df['normality_score'].idxmax(), 'Transformation']\n",
        "    print(f\"\\nðŸ† Recommended transformation: {best_transformation}\")\n",
        "    \n",
        "    # Visualize transformations\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle(f'{variable_name} - Transformation Comparison', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, (trans_name, trans_data) in enumerate(transformations.items()):\n",
        "        if trans_data is None or i >= len(axes):\n",
        "            continue\n",
        "            \n",
        "        # Histogram with normal overlay\n",
        "        axes[i].hist(trans_data, bins=50, density=True, alpha=0.7, color='lightblue')\n",
        "        \n",
        "        # Overlay normal distribution\n",
        "        x = np.linspace(trans_data.min(), trans_data.max(), 100)\n",
        "        normal_overlay = stats.norm.pdf(x, trans_data.mean(), trans_data.std())\n",
        "        axes[i].plot(x, normal_overlay, 'r-', linewidth=2, label='Normal')\n",
        "        \n",
        "        axes[i].set_title(f'{trans_name}\\nSkew: {stats.skew(trans_data):.2f}')\n",
        "        axes[i].set_xlabel('Value')\n",
        "        axes[i].set_ylabel('Density')\n",
        "        axes[i].legend()\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for j in range(i+1, len(axes)):\n",
        "        axes[j].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return results_df, best_transformation\n",
        "\n",
        "# Apply transformation analysis to key variables\n",
        "transformation_results = {}\n",
        "\n",
        "for var in ['price', 'freight_value', 'delivery_days']:\n",
        "    if var in df.columns and df[var].notna().sum() > 100:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        results_df, best_trans = compare_transformations(df[var], var)\n",
        "        transformation_results[var] = {'results': results_df, 'best': best_trans}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "advanced_correlation_section"
      },
      "source": [
        "## 4. Advanced Correlation Analysis\n",
        "\n",
        "Let's explore sophisticated correlation techniques beyond basic Pearson correlation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "multiple_correlation_methods"
      },
      "outputs": [],
      "source": [
        "# Multiple Correlation Methods Comparison\n",
        "print(\"ðŸ”— Advanced Correlation Analysis\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "def comprehensive_correlation_analysis(data, numeric_cols=None):\n",
        "    \"\"\"\n",
        "    Perform comprehensive correlation analysis using multiple methods\n",
        "    \"\"\"\n",
        "    if numeric_cols is None:\n",
        "        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    \n",
        "    # Remove columns with too many missing values\n",
        "    valid_cols = []\n",
        "    for col in numeric_cols:\n",
        "        if data[col].notna().sum() > len(data) * 0.5:  # At least 50% non-null\n",
        "            valid_cols.append(col)\n",
        "    \n",
        "    # Create clean dataset\n",
        "    clean_data = data[valid_cols].dropna()\n",
        "    \n",
        "    if len(clean_data) < 100:\n",
        "        print(\"Insufficient data for correlation analysis\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\nðŸ“Š Analyzing correlations for {len(clean_data):,} complete observations\")\n",
        "    print(f\"Variables: {', '.join(valid_cols)}\")\n",
        "    \n",
        "    # Calculate different correlation methods\n",
        "    correlations = {\n",
        "        'Pearson': clean_data.corr(method='pearson'),\n",
        "        'Spearman': clean_data.corr(method='spearman'),\n",
        "        'Kendall': clean_data.corr(method='kendall')\n",
        "    }\n",
        "    \n",
        "    # Mutual Information (for non-linear relationships)\n",
        "    try:\n",
        "        mi_matrix = pd.DataFrame(index=valid_cols, columns=valid_cols, dtype=float)\n",
        "        \n",
        "        for i, col1 in enumerate(valid_cols):\n",
        "            for j, col2 in enumerate(valid_cols):\n",
        "                if i == j:\n",
        "                    mi_matrix.loc[col1, col2] = 1.0\n",
        "                elif i < j:\n",
        "                    # Calculate mutual information\n",
        "                    mi_score = mutual_info_regression(\n",
        "                        clean_data[[col1]], clean_data[col2], \n",
        "                        random_state=42\n",
        "                    )[0]\n",
        "                    # Normalize by entropy\n",
        "                    mi_normalized = mi_score / max(np.var(clean_data[col1]), np.var(clean_data[col2]))\n",
        "                    mi_matrix.loc[col1, col2] = mi_normalized\n",
        "                    mi_matrix.loc[col2, col1] = mi_normalized\n",
        "        \n",
        "        correlations['Mutual Information'] = mi_matrix.astype(float)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not calculate mutual information: {e}\")\n",
        "    \n",
        "    # Visualize correlation matrices\n",
        "    n_methods = len(correlations)\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
        "    fig.suptitle('Correlation Methods Comparison', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, (method, corr_matrix) in enumerate(correlations.items()):\n",
        "        if i >= len(axes):\n",
        "            break\n",
        "            \n",
        "        # Create mask for upper triangle\n",
        "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "        \n",
        "        # Plot heatmap\n",
        "        sns.heatmap(\n",
        "            corr_matrix, \n",
        "            mask=mask,\n",
        "            annot=True, \n",
        "            cmap='RdBu_r', \n",
        "            center=0,\n",
        "            square=True, \n",
        "            fmt='.3f', \n",
        "            cbar_kws={\"shrink\": .8},\n",
        "            ax=axes[i]\n",
        "        )\n",
        "        axes[i].set_title(f'{method} Correlation')\n",
        "    \n",
        "    # Hide unused subplot\n",
        "    if len(correlations) < len(axes):\n",
        "        for j in range(len(correlations), len(axes)):\n",
        "            axes[j].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Find strongest correlations for each method\n",
        "    print(f\"\\nðŸ† Strongest Correlations by Method:\")\n",
        "    \n",
        "    for method, corr_matrix in correlations.items():\n",
        "        # Create correlation pairs\n",
        "        correlation_pairs = []\n",
        "        \n",
        "        for i in range(len(corr_matrix.columns)):\n",
        "            for j in range(i+1, len(corr_matrix.columns)):\n",
        "                var1 = corr_matrix.columns[i]\n",
        "                var2 = corr_matrix.columns[j]\n",
        "                corr_val = corr_matrix.iloc[i, j]\n",
        "                \n",
        "                if not np.isnan(corr_val):\n",
        "                    correlation_pairs.append({\n",
        "                        'Variable 1': var1,\n",
        "                        'Variable 2': var2,\n",
        "                        'Correlation': corr_val\n",
        "                    })\n",
        "        \n",
        "        # Sort by absolute correlation\n",
        "        correlation_pairs.sort(key=lambda x: abs(x['Correlation']), reverse=True)\n",
        "        \n",
        "        print(f\"\\n   {method}:\")\n",
        "        for i, pair in enumerate(correlation_pairs[:3]):\n",
        "            print(f\"     {i+1}. {pair['Variable 1']} â†” {pair['Variable 2']}: {pair['Correlation']:.3f}\")\n",
        "    \n",
        "    return correlations, clean_data\n",
        "\n",
        "# Select key business variables for correlation analysis\n",
        "correlation_variables = [\n",
        "    'price', 'freight_value', 'total_item_value', 'delivery_days',\n",
        "    'product_weight_g', 'product_volume_liters', 'review_score'\n",
        "]\n",
        "\n",
        "# Filter to available variables\n",
        "available_vars = [var for var in correlation_variables if var in df.columns]\n",
        "\n",
        "correlation_results, clean_correlation_data = comprehensive_correlation_analysis(df, available_vars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "partial_correlation"
      },
      "outputs": [],
      "source": [
        "# Partial Correlation Analysis\n",
        "print(\"ðŸŽ¯ Partial Correlation Analysis\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "def calculate_partial_correlation(data, x, y, control_vars):\n",
        "    \"\"\"\n",
        "    Calculate partial correlation between x and y, controlling for control_vars\n",
        "    \"\"\"\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    \n",
        "    # Clean data\n",
        "    all_vars = [x, y] + control_vars\n",
        "    clean_data = data[all_vars].dropna()\n",
        "    \n",
        "    if len(clean_data) < 50:\n",
        "        return np.nan, np.nan\n",
        "    \n",
        "    # Regress x on control variables\n",
        "    reg_x = LinearRegression()\n",
        "    reg_x.fit(clean_data[control_vars], clean_data[x])\n",
        "    residuals_x = clean_data[x] - reg_x.predict(clean_data[control_vars])\n",
        "    \n",
        "    # Regress y on control variables\n",
        "    reg_y = LinearRegression()\n",
        "    reg_y.fit(clean_data[control_vars], clean_data[y])\n",
        "    residuals_y = clean_data[y] - reg_y.predict(clean_data[control_vars])\n",
        "    \n",
        "    # Calculate correlation of residuals\n",
        "    partial_corr, p_value = pearsonr(residuals_x, residuals_y)\n",
        "    \n",
        "    return partial_corr, p_value\n",
        "\n",
        "# Example: Partial correlation between price and review_score, controlling for other factors\n",
        "if all(var in clean_correlation_data.columns for var in ['price', 'review_score', 'delivery_days', 'freight_value']):\n",
        "    \n",
        "    print(\"\\nðŸ“Š Example: Price vs Review Score Relationship\")\n",
        "    \n",
        "    # Simple correlation\n",
        "    simple_corr, simple_p = pearsonr(\n",
        "        clean_correlation_data['price'], \n",
        "        clean_correlation_data['review_score']\n",
        "    )\n",
        "    \n",
        "    # Partial correlation controlling for delivery and freight\n",
        "    partial_corr, partial_p = calculate_partial_correlation(\n",
        "        clean_correlation_data, \n",
        "        'price', 'review_score', \n",
        "        ['delivery_days', 'freight_value']\n",
        "    )\n",
        "    \n",
        "    print(f\"   Simple correlation: {simple_corr:.4f} (p = {simple_p:.4f})\")\n",
        "    print(f\"   Partial correlation: {partial_corr:.4f} (p = {partial_p:.4f})\")\n",
        "    print(f\"   (controlling for delivery_days and freight_value)\")\n",
        "    \n",
        "    difference = abs(simple_corr) - abs(partial_corr)\n",
        "    if difference > 0.05:\n",
        "        print(f\"   ðŸ’¡ The control variables explain part of the relationship (difference: {difference:.3f})\")\n",
        "    else:\n",
        "        print(f\"   ðŸ’¡ The relationship is mostly direct (difference: {difference:.3f})\")\n",
        "\n",
        "# Correlation significance testing\n",
        "print(f\"\\nðŸ§ª Correlation Significance Testing\")\n",
        "print(f\"=\" * 40)\n",
        "\n",
        "def correlation_significance_test(data, var1, var2, method='pearson'):\n",
        "    \"\"\"\n",
        "    Test statistical significance of correlation\n",
        "    \"\"\"\n",
        "    clean_data = data[[var1, var2]].dropna()\n",
        "    \n",
        "    if len(clean_data) < 10:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    \n",
        "    if method == 'pearson':\n",
        "        corr, p_value = pearsonr(clean_data[var1], clean_data[var2])\n",
        "    elif method == 'spearman':\n",
        "        corr, p_value = spearmanr(clean_data[var1], clean_data[var2])\n",
        "    elif method == 'kendall':\n",
        "        corr, p_value = kendalltau(clean_data[var1], clean_data[var2])\n",
        "    \n",
        "    # Calculate confidence interval for Pearson correlation\n",
        "    if method == 'pearson' and len(clean_data) > 3:\n",
        "        # Fisher z-transformation\n",
        "        z = np.arctanh(corr)\n",
        "        se = 1 / np.sqrt(len(clean_data) - 3)\n",
        "        z_crit = stats.norm.ppf(0.975)  # 95% CI\n",
        "        \n",
        "        z_low = z - z_crit * se\n",
        "        z_high = z + z_crit * se\n",
        "        \n",
        "        ci_low = np.tanh(z_low)\n",
        "        ci_high = np.tanh(z_high)\n",
        "        \n",
        "        return corr, p_value, (ci_low, ci_high)\n",
        "    \n",
        "    return corr, p_value, None\n",
        "\n",
        "# Test significance of key correlations\n",
        "if len(available_vars) >= 2:\n",
        "    key_pairs = [\n",
        "        ('price', 'freight_value'),\n",
        "        ('price', 'review_score'),\n",
        "        ('delivery_days', 'review_score'),\n",
        "        ('product_weight_g', 'freight_value')\n",
        "    ]\n",
        "    \n",
        "    print(\"\\nðŸ“Š Correlation Significance Results:\")\n",
        "    \n",
        "    for var1, var2 in key_pairs:\n",
        "        if var1 in available_vars and var2 in available_vars:\n",
        "            corr, p_val, ci = correlation_significance_test(clean_correlation_data, var1, var2)\n",
        "            \n",
        "            significance = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"ns\"\n",
        "            \n",
        "            print(f\"\\n   {var1} â†” {var2}:\")\n",
        "            print(f\"     Correlation: {corr:.4f} {significance}\")\n",
        "            print(f\"     p-value: {p_val:.4f}\")\n",
        "            if ci:\n",
        "                print(f\"     95% CI: [{ci[0]:.4f}, {ci[1]:.4f}]\")\n",
        "    \n",
        "    print(f\"\\n   Legend: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "interactive_visualizations"
      },
      "source": [
        "## 5. Interactive Visualizations\n",
        "\n",
        "Let's create interactive visualizations for deeper exploration of distributions and correlations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "interactive_correlation_plot"
      },
      "outputs": [],
      "source": [
        "# Interactive Correlation Heatmap\n",
        "print(\"ðŸŽ¨ Creating Interactive Visualizations\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Prepare data for interactive plots\n",
        "plot_data = clean_correlation_data.sample(min(2000, len(clean_correlation_data)), random_state=42)\n",
        "\n",
        "# Interactive correlation heatmap\n",
        "correlation_matrix = plot_data.corr()\n",
        "\n",
        "fig_heatmap = px.imshow(\n",
        "    correlation_matrix,\n",
        "    text_auto=True,\n",
        "    color_continuous_scale='RdBu',\n",
        "    color_continuous_midpoint=0,\n",
        "    title='Interactive Correlation Matrix - Hover for Details',\n",
        "    width=800,\n",
        "    height=600\n",
        ")\n",
        "\n",
        "fig_heatmap.update_layout(\n",
        "    title_font_size=16,\n",
        "    title_x=0.5\n",
        ")\n",
        "\n",
        "fig_heatmap.show()\n",
        "\n",
        "# Interactive scatter plot matrix\n",
        "# Select top variables for scatter matrix\n",
        "scatter_vars = ['price', 'freight_value', 'delivery_days', 'review_score']\n",
        "scatter_vars = [var for var in scatter_vars if var in plot_data.columns]\n",
        "\n",
        "if len(scatter_vars) >= 3:\n",
        "    fig_scatter = px.scatter_matrix(\n",
        "        plot_data,\n",
        "        dimensions=scatter_vars,\n",
        "        title='Interactive Scatter Plot Matrix - Select Variables',\n",
        "        width=900,\n",
        "        height=700\n",
        "    )\n",
        "    \n",
        "    fig_scatter.update_layout(\n",
        "        title_font_size=16,\n",
        "        title_x=0.5\n",
        "    )\n",
        "    \n",
        "    fig_scatter.show()\n",
        "\n",
        "# Interactive distribution comparison\n",
        "if 'category_clean' in df.columns:\n",
        "    # Get top categories by count\n",
        "    top_categories = df['category_clean'].value_counts().head(5).index.tolist()\n",
        "    category_data = df[df['category_clean'].isin(top_categories)]\n",
        "    \n",
        "    # Sample data for performance\n",
        "    if len(category_data) > 5000:\n",
        "        category_data = category_data.sample(5000, random_state=42)\n",
        "    \n",
        "    # Interactive box plot for price by category\n",
        "    fig_box = px.box(\n",
        "        category_data,\n",
        "        x='category_clean',\n",
        "        y='price',\n",
        "        title='Price Distribution by Product Category (Interactive)',\n",
        "        width=1000,\n",
        "        height=500\n",
        "    )\n",
        "    \n",
        "    fig_box.update_layout(\n",
        "        title_font_size=16,\n",
        "        title_x=0.5,\n",
        "        xaxis_title='Product Category',\n",
        "        yaxis_title='Price (R$)',\n",
        "        xaxis_tickangle=45\n",
        "    )\n",
        "    \n",
        "    fig_box.show()\n",
        "    \n",
        "    # Interactive 3D scatter plot\n",
        "    if all(var in category_data.columns for var in ['price', 'freight_value', 'delivery_days']):\n",
        "        fig_3d = px.scatter_3d(\n",
        "            category_data,\n",
        "            x='price',\n",
        "            y='freight_value', \n",
        "            z='delivery_days',\n",
        "            color='category_clean',\n",
        "            title='3D Relationship: Price, Freight, and Delivery Time',\n",
        "            width=900,\n",
        "            height=700\n",
        "        )\n",
        "        \n",
        "        fig_3d.update_layout(\n",
        "            title_font_size=16,\n",
        "            title_x=0.5\n",
        "        )\n",
        "        \n",
        "        fig_3d.show()\n",
        "\n",
        "print(\"\\nâœ… Interactive visualizations created!\")\n",
        "print(\"   ðŸ’¡ These plots allow you to:\")\n",
        "print(\"     â€¢ Hover for detailed information\")\n",
        "     â€¢ Zoom and pan for closer inspection\")\n",
        "print(\"     â€¢ Select/deselect categories in legends\")\n",
        "print(\"     â€¢ Rotate 3D plots for different perspectives\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "business_hypothesis_testing"
      },
      "source": [
        "## 6. Business Hypothesis Testing\n",
        "\n",
        "Let's use statistical tests to validate common business assumptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hypothesis_tests"
      },
      "outputs": [],
      "source": [
        "# Business Hypothesis Testing\n",
        "print(\"ðŸ§ª Business Hypothesis Testing\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "def business_hypothesis_test(data, test_name, description, test_function):\n",
        "    \"\"\"\n",
        "    Wrapper for conducting and interpreting business hypothesis tests\n",
        "    \"\"\"\n",
        "    print(f\"\\nðŸ“Š {test_name}\")\n",
        "    print(f\"   Question: {description}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    try:\n",
        "        result = test_function(data)\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"   Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Test 1: Do expensive products have longer delivery times?\n",
        "def test_price_delivery_relationship(data):\n",
        "    # Clean data\n",
        "    clean_data = data[['price', 'delivery_days']].dropna()\n",
        "    \n",
        "    if len(clean_data) < 100:\n",
        "        print(\"   Insufficient data for analysis\")\n",
        "        return\n",
        "    \n",
        "    # Divide into high and low price groups\n",
        "    price_median = clean_data['price'].median()\n",
        "    high_price = clean_data[clean_data['price'] > price_median]['delivery_days']\n",
        "    low_price = clean_data[clean_data['price'] <= price_median]['delivery_days']\n",
        "    \n",
        "    # Mann-Whitney U test (non-parametric)\n",
        "    statistic, p_value = mannwhitneyu(high_price, low_price, alternative='greater')\n",
        "    \n",
        "    print(f\"   High-price group (n={len(high_price)}): {high_price.mean():.1f} Â± {high_price.std():.1f} days\")\n",
        "    print(f\"   Low-price group (n={len(low_price)}): {low_price.mean():.1f} Â± {low_price.std():.1f} days\")\n",
        "    print(f\"   Mann-Whitney U statistic: {statistic:.0f}\")\n",
        "    print(f\"   p-value: {p_value:.4f}\")\n",
        "    \n",
        "    alpha = 0.05\n",
        "    if p_value < alpha:\n",
        "        print(f\"   âœ… Significant difference: Expensive products DO have longer delivery times\")\n",
        "    else:\n",
        "        print(f\"   âŒ No significant difference: Price doesn't significantly affect delivery time\")\n",
        "    \n",
        "    return {'statistic': statistic, 'p_value': p_value, 'significant': p_value < alpha}\n",
        "\n",
        "# Test 2: Do different product categories have different average ratings?\n",
        "def test_category_rating_differences(data):\n",
        "    if 'category_clean' not in data.columns or 'review_score' not in data.columns:\n",
        "        print(\"   Required columns not available\")\n",
        "        return\n",
        "    \n",
        "    # Clean data and get top categories\n",
        "    clean_data = data[['category_clean', 'review_score']].dropna()\n",
        "    \n",
        "    if len(clean_data) < 100:\n",
        "        print(\"   Insufficient data for analysis\")\n",
        "        return\n",
        "    \n",
        "    # Get top 5 categories by count\n",
        "    top_categories = clean_data['category_clean'].value_counts().head(5).index\n",
        "    category_data = clean_data[clean_data['category_clean'].isin(top_categories)]\n",
        "    \n",
        "    # Group ratings by category\n",
        "    category_groups = [group['review_score'].values for name, group in category_data.groupby('category_clean')]\n",
        "    category_names = [name for name, group in category_data.groupby('category_clean')]\n",
        "    \n",
        "    # Kruskal-Wallis test (non-parametric ANOVA)\n",
        "    statistic, p_value = kruskal(*category_groups)\n",
        "    \n",
        "    print(f\"   Categories analyzed: {', '.join(category_names)}\")\n",
        "    print(f\"   Average ratings by category:\")\n",
        "    for name, group in category_data.groupby('category_clean'):\n",
        "        avg_rating = group['review_score'].mean()\n",
        "        count = len(group)\n",
        "        print(f\"     {name}: {avg_rating:.2f} (n={count})\")\n",
        "    \n",
        "    print(f\"   Kruskal-Wallis H statistic: {statistic:.2f}\")\n",
        "    print(f\"   p-value: {p_value:.4f}\")\n",
        "    \n",
        "    alpha = 0.05\n",
        "    if p_value < alpha:\n",
        "        print(f\"   âœ… Significant difference: Product categories DO have different average ratings\")\n",
        "    else:\n",
        "        print(f\"   âŒ No significant difference: Product categories have similar average ratings\")\n",
        "    \n",
        "    return {'statistic': statistic, 'p_value': p_value, 'significant': p_value < alpha}\n",
        "\n",
        "# Test 3: Is there a relationship between freight cost and product weight?\n",
        "def test_freight_weight_correlation(data):\n",
        "    clean_data = data[['freight_value', 'product_weight_g']].dropna()\n",
        "    \n",
        "    if len(clean_data) < 50:\n",
        "        print(\"   Insufficient data for analysis\")\n",
        "        return\n",
        "    \n",
        "    # Calculate Spearman correlation (robust to outliers)\n",
        "    correlation, p_value = spearmanr(clean_data['freight_value'], clean_data['product_weight_g'])\n",
        "    \n",
        "    print(f\"   Sample size: {len(clean_data):,} observations\")\n",
        "    print(f\"   Spearman correlation: {correlation:.4f}\")\n",
        "    print(f\"   p-value: {p_value:.4f}\")\n",
        "    \n",
        "    alpha = 0.05\n",
        "    if p_value < alpha:\n",
        "        strength = \"weak\" if abs(correlation) < 0.3 else \"moderate\" if abs(correlation) < 0.7 else \"strong\"\n",
        "        direction = \"positive\" if correlation > 0 else \"negative\"\n",
        "        print(f\"   âœ… Significant {strength} {direction} correlation between freight cost and weight\")\n",
        "    else:\n",
        "        print(f\"   âŒ No significant correlation between freight cost and weight\")\n",
        "    \n",
        "    return {'correlation': correlation, 'p_value': p_value, 'significant': p_value < alpha}\n",
        "\n",
        "# Run hypothesis tests\n",
        "hypothesis_results = {}\n",
        "\n",
        "# Test 1\n",
        "hypothesis_results['price_delivery'] = business_hypothesis_test(\n",
        "    df,\n",
        "    \"Price vs Delivery Time Analysis\",\n",
        "    \"Do expensive products take longer to deliver?\",\n",
        "    test_price_delivery_relationship\n",
        ")\n",
        "\n",
        "# Test 2  \n",
        "hypothesis_results['category_ratings'] = business_hypothesis_test(\n",
        "    df,\n",
        "    \"Category Rating Differences\",\n",
        "    \"Do different product categories have different customer satisfaction levels?\",\n",
        "    test_category_rating_differences\n",
        ")\n",
        "\n",
        "# Test 3\n",
        "hypothesis_results['freight_weight'] = business_hypothesis_test(\n",
        "    df,\n",
        "    \"Freight Cost vs Product Weight\",\n",
        "    \"Is freight cost related to product weight?\",\n",
        "    test_freight_weight_correlation\n",
        ")\n",
        "\n",
        "# Summary of hypothesis test results\n",
        "print(f\"\\n\\nðŸ“‹ HYPOTHESIS TESTING SUMMARY\")\n",
        "print(f\"=\" * 40)\n",
        "\n",
        "significant_tests = []\n",
        "for test_name, result in hypothesis_results.items():\n",
        "    if result and result.get('significant', False):\n",
        "        significant_tests.append(test_name)\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Significant findings ({len(significant_tests)}/{len(hypothesis_results)}):\")\n",
        "for test in significant_tests:\n",
        "    print(f\"   âœ… {test.replace('_', ' ').title()}\")\n",
        "\n",
        "if len(significant_tests) < len(hypothesis_results):\n",
        "    non_significant = [test for test in hypothesis_results.keys() if test not in significant_tests]\n",
        "    print(f\"\\nâŒ Non-significant findings:\")\n",
        "    for test in non_significant:\n",
        "        print(f\"   â€¢ {test.replace('_', ' ').title()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comprehensive_summary"
      },
      "source": [
        "## 7. Comprehensive EDA Summary and Business Recommendations\n",
        "\n",
        "Let's synthesize all our findings into actionable business insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_summary_dashboard"
      },
      "outputs": [],
      "source": [
        "# Comprehensive EDA Summary Dashboard\n",
        "print(\"ðŸ“Š COMPREHENSIVE EDA SUMMARY - OLIST E-COMMERCE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Generate comprehensive insights report\n",
        "def generate_comprehensive_insights(data, correlation_results, hypothesis_results):\n",
        "    \"\"\"\n",
        "    Generate comprehensive business insights from EDA\n",
        "    \"\"\"\n",
        "    insights = {\n",
        "        'data_quality': {},\n",
        "        'distributions': {},\n",
        "        'correlations': {},\n",
        "        'business_validation': {},\n",
        "        'recommendations': []\n",
        "    }\n",
        "    \n",
        "    # Data Quality Assessment\n",
        "    total_records = len(data)\n",
        "    complete_records = len(data.dropna())\n",
        "    completeness_rate = (complete_records / total_records) * 100\n",
        "    \n",
        "    insights['data_quality'] = {\n",
        "        'total_records': total_records,\n",
        "        'complete_records': complete_records,\n",
        "        'completeness_rate': completeness_rate\n",
        "    }\n",
        "    \n",
        "    # Distribution Insights\n",
        "    price_skewness = data['price'].skew()\n",
        "    price_outlier_rate = len(detect_outliers(data['price'], 'iqr')) / len(data) * 100\n",
        "    \n",
        "    insights['distributions'] = {\n",
        "        'price_skewness': price_skewness,\n",
        "        'price_outlier_rate': price_outlier_rate\n",
        "    }\n",
        "    \n",
        "    # Correlation Insights\n",
        "    if correlation_results and 'Pearson' in correlation_results:\n",
        "        pearson_matrix = correlation_results['Pearson']\n",
        "        # Find strongest absolute correlation (excluding self-correlations)\n",
        "        mask = np.triu(np.ones_like(pearson_matrix, dtype=bool), k=1)\n",
        "        masked_corr = pearson_matrix.where(mask)\n",
        "        \n",
        "        max_corr_idx = np.unravel_index(np.nanargmax(np.abs(masked_corr.values)), masked_corr.shape)\n",
        "        strongest_pair = (pearson_matrix.index[max_corr_idx[0]], pearson_matrix.columns[max_corr_idx[1]])\n",
        "        strongest_corr = masked_corr.iloc[max_corr_idx[0], max_corr_idx[1]]\n",
        "        \n",
        "        insights['correlations'] = {\n",
        "            'strongest_pair': strongest_pair,\n",
        "            'strongest_correlation': strongest_corr\n",
        "        }\n",
        "    \n",
        "    # Business Validation Insights\n",
        "    significant_hypotheses = []\n",
        "    if hypothesis_results:\n",
        "        for test_name, result in hypothesis_results.items():\n",
        "            if result and result.get('significant', False):\n",
        "                significant_hypotheses.append(test_name)\n",
        "    \n",
        "    insights['business_validation'] = {\n",
        "        'significant_hypotheses': significant_hypotheses,\n",
        "        'validation_rate': len(significant_hypotheses) / len(hypothesis_results) * 100 if hypothesis_results else 0\n",
        "    }\n",
        "    \n",
        "    return insights\n",
        "\n",
        "# Generate insights\n",
        "comprehensive_insights = generate_comprehensive_insights(df, correlation_results, hypothesis_results)\n",
        "\n",
        "# Display comprehensive summary\n",
        "print(f\"\\nðŸ” DATA QUALITY ASSESSMENT:\")\n",
        "print(f\"   â€¢ Total records analyzed: {comprehensive_insights['data_quality']['total_records']:,}\")\n",
        "print(f\"   â€¢ Complete records: {comprehensive_insights['data_quality']['complete_records']:,}\")\n",
        "print(f\"   â€¢ Data completeness: {comprehensive_insights['data_quality']['completeness_rate']:.1f}%\")\n",
        "\n",
        "print(f\"\\nðŸ“ˆ DISTRIBUTION CHARACTERISTICS:\")\n",
        "print(f\"   â€¢ Price distribution skewness: {comprehensive_insights['distributions']['price_skewness']:.2f}\")\n",
        "if comprehensive_insights['distributions']['price_skewness'] > 1:\n",
        "    print(f\"     â†’ Highly right-skewed: Most products are low-priced with few expensive items\")\n",
        "elif comprehensive_insights['distributions']['price_skewness'] > 0.5:\n",
        "    print(f\"     â†’ Moderately right-skewed: Some concentration in lower price ranges\")\n",
        "else:\n",
        "    print(f\"     â†’ Approximately symmetric distribution\")\n",
        "\n",
        "print(f\"   â€¢ Price outlier rate: {comprehensive_insights['distributions']['price_outlier_rate']:.1f}%\")\n",
        "\n",
        "if 'correlations' in comprehensive_insights and comprehensive_insights['correlations']:\n",
        "    print(f\"\\nðŸ”— CORRELATION INSIGHTS:\")\n",
        "    strongest_pair = comprehensive_insights['correlations']['strongest_pair']\n",
        "    strongest_corr = comprehensive_insights['correlations']['strongest_correlation']\n",
        "    print(f\"   â€¢ Strongest relationship: {strongest_pair[0]} â†” {strongest_pair[1]}\")\n",
        "    print(f\"   â€¢ Correlation strength: {strongest_corr:.3f}\")\n",
        "\n",
        "print(f\"\\nðŸ§ª BUSINESS HYPOTHESIS VALIDATION:\")\n",
        "validation_rate = comprehensive_insights['business_validation']['validation_rate']\n",
        "significant_tests = comprehensive_insights['business_validation']['significant_hypotheses']\n",
        "print(f\"   â€¢ Hypotheses validated: {len(significant_tests)}/{len(hypothesis_results)} ({validation_rate:.0f}%)\")\n",
        "\n",
        "for test in significant_tests:\n",
        "    print(f\"   âœ… {test.replace('_', ' ').title()}\")\n",
        "\n",
        "# Strategic Business Recommendations\n",
        "print(f\"\\n\\nðŸŽ¯ STRATEGIC BUSINESS RECOMMENDATIONS\")\n",
        "print(f\"=\" * 50)\n",
        "\n",
        "print(f\"\\nðŸ’° PRICING STRATEGY:\")\n",
        "if comprehensive_insights['distributions']['price_skewness'] > 1:\n",
        "    print(f\"   1. Consider premium product line expansion (few high-value items currently)\")\n",
        "    print(f\"   2. Implement dynamic pricing for popular low-cost categories\")\n",
        "    print(f\"   3. Bundle low-priced items to increase average order value\")\n",
        "\n",
        "print(f\"\\nðŸ“¦ OPERATIONAL OPTIMIZATION:\")\n",
        "if 'freight_weight' in significant_tests:\n",
        "    print(f\"   1. Optimize shipping costs based on weight-freight correlation\")\n",
        "    print(f\"   2. Negotiate better rates for heavy items with logistics partners\")\n",
        "\n",
        "if 'price_delivery' in significant_tests:\n",
        "    print(f\"   3. Expedite delivery for high-value orders to maintain satisfaction\")\n",
        "    print(f\"   4. Set customer expectations for delivery times based on price tiers\")\n",
        "\n",
        "print(f\"\\nðŸ“Š DATA & ANALYTICS:\")\n",
        "if comprehensive_insights['data_quality']['completeness_rate'] < 95:\n",
        "    print(f\"   1. Improve data collection processes (current completeness: {comprehensive_insights['data_quality']['completeness_rate']:.1f}%)\")\n",
        "\n",
        "print(f\"   2. Implement real-time outlier detection for pricing anomalies\")\n",
        "print(f\"   3. Monitor correlation patterns for early trend detection\")\n",
        "print(f\"   4. Automate EDA reporting for regular business insights\")\n",
        "\n",
        "print(f\"\\nðŸŽ¯ CUSTOMER EXPERIENCE:\")\n",
        "if 'category_ratings' in significant_tests:\n",
        "    print(f\"   1. Focus improvement efforts on lower-rated categories\")\n",
        "    print(f\"   2. Replicate success factors from high-rated categories\")\n",
        "\n",
        "print(f\"   3. Personalize recommendations based on correlation patterns\")\n",
        "print(f\"   4. Optimize product mix based on statistical distribution insights\")\n",
        "\n",
        "# Final summary metrics\n",
        "print(f\"\\n\\nðŸ“‹ EDA COMPLETION SUMMARY\")\n",
        "print(f\"=\" * 40)\n",
        "print(f\"âœ… Distribution Analysis: {len(key_variables)} variables analyzed\")\n",
        "print(f\"âœ… Correlation Analysis: {len(available_vars)} variables cross-analyzed\")\n",
        "print(f\"âœ… Hypothesis Testing: {len(hypothesis_results)} business questions validated\")\n",
        "print(f\"âœ… Statistical Tests: Multiple normality and significance tests performed\")\n",
        "print(f\"âœ… Interactive Visualizations: Created for deeper exploration\")\n",
        "print(f\"âœ… Business Recommendations: Strategic insights delivered\")\n",
        "\n",
        "print(f\"\\nðŸŽ‰ Advanced EDA Analysis Complete!\")\n",
        "print(f\"   Ready for predictive modeling and advanced analytics.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "final_summary_part3"
      },
      "source": [
        "## Summary and Conclusions - Part 3\n",
        "\n",
        "### What We've Accomplished\n",
        "\n",
        "1. **âœ… Advanced Distribution Analysis**: Comprehensive normality testing and distribution fitting\n",
        "2. **âœ… Data Transformation Techniques**: Systematic comparison of transformation methods\n",
        "3. **âœ… Sophisticated Correlation Analysis**: Multiple correlation methods and partial correlations\n",
        "4. **âœ… Statistical Hypothesis Testing**: Validation of business assumptions with rigorous tests\n",
        "5. **âœ… Interactive Visualizations**: Dynamic plots for deeper data exploration\n",
        "6. **âœ… Comprehensive Business Insights**: Strategic recommendations based on statistical evidence\n",
        "\n",
        "### Key Advanced Techniques Mastered\n",
        "\n",
        "**Statistical Testing:**\n",
        "- Multiple normality tests (Shapiro-Wilk, Jarque-Bera, Anderson-Darling, etc.)\n",
        "- Non-parametric hypothesis tests (Mann-Whitney U, Kruskal-Wallis)\n",
        "- Correlation significance testing with confidence intervals\n",
        "\n",
        "**Advanced Analysis:**\n",
        "- Distribution fitting with AIC model selection\n",
        "- Partial correlation analysis\n",
        "- Data transformation optimization\n",
        "- Mutual information for non-linear relationships\n",
        "\n",
        "**Business Application:**\n",
        "- Statistical validation of business hypotheses\n",
        "- Evidence-based strategic recommendations\n",
        "- Risk assessment through outlier analysis\n",
        "- Performance optimization insights\n",
        "\n",
        "### Complete EDA Framework\n",
        "\n",
        "**Part 1:** Structured approach and initial exploration\n",
        "**Part 2:** Descriptive statistics and summary insights\n",
        "**Part 3:** Advanced distributions and correlation analysis\n",
        "\n",
        "This comprehensive framework provides a systematic approach to exploratory data analysis that can be applied to any business dataset.\n",
        "\n",
        "### Next Steps\n",
        "- Apply these techniques to your specific business problems\n",
        "- Use the automated EDA functions for rapid analysis\n",
        "- Combine insights for predictive modeling and machine learning\n",
        "- Build regular reporting systems using these methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "final_exercises"
      },
      "source": [
        "## ðŸŽ¯ Final Practice Exercises - Advanced EDA\n",
        "\n",
        "Master the advanced techniques:\n",
        "\n",
        "1. **Distribution Mastery**: Choose a business metric and perform complete distribution analysis including fitting, testing, and transformation\n",
        "\n",
        "2. **Correlation Investigation**: Investigate a business relationship using multiple correlation methods and partial correlation\n",
        "\n",
        "3. **Hypothesis Testing**: Formulate and test your own business hypothesis using appropriate statistical tests\n",
        "\n",
        "4. **Interactive Dashboard**: Create an interactive dashboard combining multiple advanced visualization techniques\n",
        "\n",
        "5. **EDA Automation**: Enhance the automated EDA function with advanced statistical tests and business insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_exercise_space"
      },
      "outputs": [],
      "source": [
        "# Final Exercise Space - Advanced EDA Techniques\n",
        "# Use this space to practice the advanced methods learned in Part 3\n",
        "\n",
        "# Exercise 1: Complete Distribution Analysis\n",
        "# Choose a business metric and perform comprehensive analysis\n",
        "\n",
        "# Exercise 2: Advanced Correlation Investigation  \n",
        "# Explore a business relationship with multiple methods\n",
        "\n",
        "# Exercise 3: Custom Hypothesis Testing\n",
        "# Formulate and test your own business question\n",
        "\n",
        "# Exercise 4: Interactive Dashboard Creation\n",
        "# Combine multiple visualization techniques\n",
        "\n",
        "# Exercise 5: Enhanced EDA Automation\n",
        "# Improve the automated EDA function with advanced features"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}